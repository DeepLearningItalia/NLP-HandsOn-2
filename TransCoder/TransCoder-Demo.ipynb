{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3a41d54-8a09-4f8f-91ad-50e83e391fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for calogero: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt install libclang-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824db867-ab40-4d18-ba3c-636f99a32baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1249ac-bbfe-45a9-83a4-31f6d5991b20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : TeamZMART\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: ['cpp_sa', 'python_sa', 'java_sa']\n",
      "                                     amp: 2\n",
      "                                     attention_dropout: 0\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 10\n",
      "                                     bptt: 256\n",
      "                                     bt_sample_temperature: 0\n",
      "                                     bt_src_langs: ['python_sa', 'cpp_sa', 'java_sa', 'cpp_sa', 'python_sa', 'java_sa']\n",
      "                                     bt_steps: [('python_sa', 'cpp_sa', 'python_sa'), ('cpp_sa', 'python_sa', 'cpp_sa'), ('java_sa', 'cpp_sa', 'java_sa'), ('cpp_sa', 'java_sa', 'cpp_sa'), ('python_sa', 'java_sa', 'python_sa'), ('java_sa', 'python_sa', 'java_sa')]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python XLM/train.py --n_heads 8 --bt_steps 'python_sa-cpp_sa-python_sa,cpp_sa-python_sa-cpp_sa,java_sa-cpp_sa-java_sa,cpp_sa-java_sa-cpp_sa,python_sa-java_sa-python_sa,java_sa-python_sa-java_sa' --max_vocab '-1' --word_blank '0.1' --n_layers 6 --generate_hypothesis true --max_len 512 --bptt 256 --fp16 True --share_inout_emb true --tokens_per_batch 6000 --has_sentences_ids true --eval_bleu true --split_data false --data_path './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test' --eval_computation true --batch_size 32 --reload_model 'model_2.pth,model_2.pth' --amp 2 --max_batch_size 128 --ae_steps 'cpp_sa,python_sa,java_sa' --emb_dim 1024 --eval_only True --beam_size 10 --retry_mistmatching_types 1 --dump_path '/tmp/' --exp_name=eval_final_model_wc_30 --lgs 'cpp_sa-java_sa-python_sa' --encoder_only=False --exp_id \"q5axwa7qpx\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0\n",
      "                                     dump_path: /tmp/eval_final_model_wc_30/q5axwa7qpx\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     emb_dim_decoder: 1024\n",
      "                                     emb_dim_encoder: 1024\n",
      "                                     encoder_only: False\n",
      "                                     epoch_size: 100000\n",
      "                                     eval_bleu: True\n",
      "                                     eval_bleu_test_only: False\n",
      "                                     eval_computation: True\n",
      "                                     eval_only: True\n",
      "                                     eval_temperature: None\n",
      "                                     exp_id: q5axwa7qpx\n",
      "                                     exp_name: eval_final_model_wc_30\n",
      "                                     fp16: True\n",
      "                                     gelu_activation: False\n",
      "                                     gen_tpb_multiplier: 1\n",
      "                                     generate_hypothesis: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     has_sentences_ids: True\n",
      "                                     id2lang: {0: 'cpp_sa', 1: 'java_sa', 2: 'python_sa'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lang2id: {'cpp_sa': 0, 'java_sa': 1, 'python_sa': 2}\n",
      "                                     langs: ['cpp_sa', 'java_sa', 'python_sa']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: cpp_sa-java_sa-python_sa\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 128\n",
      "                                     max_epoch: 100000\n",
      "                                     max_len: 512\n",
      "                                     max_vocab: -1\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: []\n",
      "                                     mono_dataset: {'cpp_sa': {'train': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/train.cpp_sa.pth', 'valid': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa.pth', 'test': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa.pth'}, 'java_sa': {'train': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/train.java_sa.pth', 'valid': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.java_sa.pth', 'test': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.java_sa.pth'}, 'python_sa': {'train': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/train.python_sa.pth', 'valid': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.python_sa.pth', 'test': './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.python_sa.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 3\n",
      "                                     n_layers: 6\n",
      "                                     n_layers_decoder: 6\n",
      "                                     n_layers_encoder: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_share_dec: 0\n",
      "                                     node_id: 0\n",
      "                                     number_samples: 1\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('cpp_sa', 'java_sa'): {'valid': ('./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-java_sa.cpp_sa.pth', './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-java_sa.java_sa.pth'), 'test': ('./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-java_sa.cpp_sa.pth', './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-java_sa.java_sa.pth')}, ('cpp_sa', 'python_sa'): {'valid': ('./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-python_sa.cpp_sa.pth', './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-python_sa.python_sa.pth'), 'test': ('./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-python_sa.cpp_sa.pth', './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-python_sa.python_sa.pth')}, ('java_sa', 'python_sa'): {'valid': ('./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.java_sa-python_sa.java_sa.pth', './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.java_sa-python_sa.python_sa.pth'), 'test': ('./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.java_sa-python_sa.java_sa.pth', './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.java_sa-python_sa.python_sa.pth')}}\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: model_2.pth,model_2.pth\n",
      "                                     retry_mistmatching_types: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     separate_decoders: False\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     split_data_accross_gpu: local\n",
      "                                     stopping_criterion: \n",
      "                                     tokens_per_batch: 6000\n",
      "                                     use_lang_emb: True\n",
      "                                     validation_metrics: \n",
      "                                     word_blank: 0.1\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - The experiment will be stored in /tmp/eval_final_model_wc_30/q5axwa7qpx\n",
      "                                     \n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Running command: python XLM/train.py --n_heads 8 --bt_steps 'python_sa-cpp_sa-python_sa,cpp_sa-python_sa-cpp_sa,java_sa-cpp_sa-java_sa,cpp_sa-java_sa-cpp_sa,python_sa-java_sa-python_sa,java_sa-python_sa-java_sa' --max_vocab '-1' --word_blank '0.1' --n_layers 6 --generate_hypothesis true --max_len 512 --bptt 256 --fp16 True --share_inout_emb true --tokens_per_batch 6000 --has_sentences_ids true --eval_bleu true --split_data false --data_path './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test' --eval_computation true --batch_size 32 --reload_model 'model_2.pth,model_2.pth' --amp 2 --max_batch_size 128 --ae_steps 'cpp_sa,python_sa,java_sa' --emb_dim 1024 --eval_only True --beam_size 10 --retry_mistmatching_types 1 --dump_path '/tmp/' --exp_name=eval_final_model_wc_30 --lgs 'cpp_sa-java_sa-python_sa' --encoder_only=False\n",
      "\n",
      "WARNING - 07/27/21 11:13:25 - 0:00:00 - Signal handler installed.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - ============ Monolingual data (cpp_sa)\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 545764 words (63961 unique) in 2541 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Removed 0 empty sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 820038 words (63961 unique) in 2852 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Removed 0 empty sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - ============ Monolingual data (java_sa)\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.java_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 125908 words (63961 unique) in 912 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Removed 0 empty sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.java_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 88101 words (63961 unique) in 646 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Removed 0 empty sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - ============ Monolingual data (python_sa)\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.python_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 666641 words (63961 unique) in 2634 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Removed 0 empty sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.python_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 568798 words (63961 unique) in 2664 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Removed 0 empty sentences.\n",
      "\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - ============ Parallel data (cpp_sa-java_sa)\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-java_sa.cpp_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 61937 words (63961 unique) in 470 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-java_sa.java_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:00 - 66397 words (63961 unique) in 470 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-java_sa.cpp_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - 127240 words (63961 unique) in 948 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-java_sa.java_sa.pth ...\n",
      "INFO - 07/27/21 11:13:25 - 0:00:01 - 137340 words (63961 unique) in 948 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 8 too long sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - ============ Parallel data (cpp_sa-python_sa)\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-python_sa.cpp_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 61937 words (63961 unique) in 470 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.cpp_sa-python_sa.python_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 63023 words (63961 unique) in 470 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-python_sa.cpp_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 127240 words (63961 unique) in 948 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.cpp_sa-python_sa.python_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 130820 words (63961 unique) in 948 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 6 too long sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - ============ Parallel data (java_sa-python_sa)\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.java_sa-python_sa.java_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 66397 words (63961 unique) in 470 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/valid.java_sa-python_sa.python_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 63023 words (63961 unique) in 470 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.java_sa-python_sa.java_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 137340 words (63961 unique) in 948 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Loading data from ./data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test/test.java_sa-python_sa.python_sa.pth ...\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - 130820 words (63961 unique) in 948 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Removed 8 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - ============ Data summary\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Monolingual data   - valid -       cpp_sa:      2541\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Monolingual data   -  test -       cpp_sa:      2852\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Monolingual data   - valid -      java_sa:       912\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Monolingual data   -  test -      java_sa:       646\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Monolingual data   - valid -    python_sa:      2634\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Monolingual data   -  test -    python_sa:      2664\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Parallel data      - valid - cpp_sa-java_sa:       469\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Parallel data      -  test - cpp_sa-java_sa:       940\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Parallel data      - valid - cpp_sa-python_sa:       469\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Parallel data      -  test - cpp_sa-python_sa:       942\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Parallel data      - valid - java_sa-python_sa:       469\n",
      "INFO - 07/27/21 11:13:26 - 0:00:01 - Parallel data      -  test - java_sa-python_sa:       940\n",
      "\n",
      "INFO - 07/27/21 11:13:29 - 0:00:04 - Reloading encoder from model_2.pth ...\n",
      "INFO - 07/27/21 11:13:31 - 0:00:06 - Reloading decoders from model_2.pth ...\n",
      "INFO - 07/27/21 11:13:32 - 0:00:07 - Number of parameters (encoder): 142191065\n",
      "INFO - 07/27/21 11:13:32 - 0:00:07 - Number of parameters (decoders): 167393753\n",
      "INFO - 07/27/21 11:13:32 - 0:00:07 - Number of decoders: 1\n",
      "INFO - 07/27/21 11:13:32 - 0:00:07 - Found 264 parameters in model.\n",
      "INFO - 07/27/21 11:13:32 - 0:00:07 - Optimizers: model\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "/home/calogero/miniconda3/envs/nlp-hands-on-2/lib/python3.7/site-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
      "  warnings.warn(\"An input tensor was not cuda.\")\n",
      "/home/calogero/miniconda3/envs/nlp-hands-on-2/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam9.txt\n",
      "INFO - 07/27/21 11:18:15 - 0:04:50 - Computation res valid python_sa java_sa : {\"error\": 224, \"failure\": 0, \"identical_gold\": 10, \"script_not_found\": 235, \"success\": 10, \"timeout\": 0, \"total\": 469, \"total_evaluated\": 234}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:18:15 - 0:04:50 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:18:15 - 0:04:50 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.valid_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.python_sa-java_sa.valid.txt : -1.000000\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam9.txt\n",
      "INFO - 07/27/21 11:25:46 - 0:12:21 - Computation res valid cpp_sa python_sa : {\"error\": 70, \"failure\": 22, \"identical_gold\": 19, \"script_not_found\": 232, \"success\": 143, \"timeout\": 2, \"total\": 469, \"total_evaluated\": 237}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:25:46 - 0:12:21 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:25:46 - 0:12:21 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-python_sa.valid_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.cpp_sa-python_sa.valid.txt : -1.000000\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam9.txt\n",
      "INFO - 07/27/21 11:29:22 - 0:15:57 - Computation res valid cpp_sa java_sa : {\"error\": 166, \"failure\": 0, \"identical_gold\": 68, \"script_not_found\": 235, \"success\": 68, \"timeout\": 0, \"total\": 469, \"total_evaluated\": 234}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:29:22 - 0:15:57 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:29:22 - 0:15:57 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.cpp_sa-java_sa.valid_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.cpp_sa-java_sa.valid.txt : -1.000000\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam9.txt\n",
      "INFO - 07/27/21 11:33:34 - 0:20:09 - Computation res valid java_sa cpp_sa : {\"error\": 23, \"failure\": 8, \"identical_gold\": 52, \"script_not_found\": 238, \"success\": 200, \"timeout\": 0, \"total\": 469, \"total_evaluated\": 231}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:33:34 - 0:20:09 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:33:34 - 0:20:09 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-cpp_sa.valid_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.java_sa-cpp_sa.valid.txt : -1.000000\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam9.txt\n",
      "INFO - 07/27/21 11:42:12 - 0:28:47 - Computation res valid java_sa python_sa : {\"error\": 61, \"failure\": 25, \"identical_gold\": 8, \"script_not_found\": 232, \"success\": 150, \"timeout\": 1, \"total\": 469, \"total_evaluated\": 237}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:42:12 - 0:28:47 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:42:12 - 0:28:47 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.java_sa-python_sa.valid_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.java_sa-python_sa.valid.txt : -1.000000\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam9.txt\n",
      "INFO - 07/27/21 11:48:34 - 0:35:09 - Computation res valid python_sa cpp_sa : {\"error\": 81, \"failure\": 40, \"identical_gold\": 12, \"script_not_found\": 238, \"success\": 109, \"timeout\": 1, \"total\": 469, \"total_evaluated\": 231}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:48:34 - 0:35:09 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:48:34 - 0:35:09 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-cpp_sa.valid_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.python_sa-cpp_sa.valid.txt : -1.000000\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam0.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam1.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam2.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam3.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam4.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam5.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam6.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam7.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam8.txt\n",
      "outputing hypotheses in /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam9.txt\n",
      "INFO - 07/27/21 11:57:19 - 0:43:55 - Computation res test python_sa java_sa : {\"error\": 448, \"failure\": 0, \"identical_gold\": 33, \"script_not_found\": 459, \"success\": 33, \"timeout\": 0, \"total\": 940, \"total_evaluated\": 481}\n",
      "/bin/sh: 1: /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/multi-bleu.perl: Permission denied\n",
      "WARNING - 07/27/21 11:57:20 - 0:43:55 - Impossible to parse BLEU score! \"\"\n",
      "INFO - 07/27/21 11:57:20 - 0:43:55 - BLEU /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/hyp0.python_sa-java_sa.test_beam0.txt /tmp/eval_final_model_wc_30/q5axwa7qpx/hypotheses/ref.python_sa-java_sa.test.txt : -1.000000\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"XLM/train.py\", line 341, in <module>\n",
      "    main(params)\n",
      "  File \"XLM/train.py\", line 262, in main\n",
      "    scores = evaluator.run_all_evals(trainer)\n",
      "  File \"/home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/evaluator.py\", line 229, in run_all_evals\n",
      "    lang2, eval_bleu, eval_computation)\n",
      "  File \"/home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/evaluation/evaluator.py\", line 493, in evaluate_mt\n",
      "    max_len=len_v\n",
      "  File \"/home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/XLM/src/model/transformer.py\", line 670, in generate_beam\n",
      "    next_scores[sent_id].max().item())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python XLM/train.py \\\n",
    "--n_heads 8 \\\n",
    "--bt_steps 'python_sa-cpp_sa-python_sa,cpp_sa-python_sa-cpp_sa,java_sa-cpp_sa-java_sa,cpp_sa-java_sa-cpp_sa,python_sa-java_sa-python_sa,java_sa-python_sa-java_sa' \\\n",
    "--max_vocab '-1' \\\n",
    "--word_blank '0.1' \\\n",
    "--n_layers 6  \\\n",
    "--generate_hypothesis true \\\n",
    "--max_len 512 \\\n",
    "--bptt 256 \\\n",
    "--fp16 True \\\n",
    "--share_inout_emb true \\\n",
    "--tokens_per_batch 6000 \\\n",
    "--has_sentences_ids true \\\n",
    "--eval_bleu true  \\\n",
    "--split_data false \\\n",
    "--data_path './data/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test'  \\\n",
    "--eval_computation true \\\n",
    "--batch_size 32 \\\n",
    "--reload_model 'model_2.pth,model_2.pth' \\\n",
    "--amp 2 \\\n",
    "--max_batch_size 128 \\\n",
    "--ae_steps 'cpp_sa,python_sa,java_sa' \\\n",
    "--emb_dim 1024 \\\n",
    "--eval_only True \\\n",
    "--beam_size 10 \\\n",
    "--retry_mistmatching_types 1 \\\n",
    "--dump_path '/tmp/' \\\n",
    "--exp_name='eval_final_model_wc_30' \\\n",
    "--lgs 'cpp_sa-java_sa-python_sa' \\\n",
    "--encoder_only=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "297d4789-5e1e-40ef-aa9f-c048702ab7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "python_example_list = open(\"./data/test_dataset/python/python.0.json\").readlines()\n",
    "\n",
    "for python_example in python_example_list:\n",
    "    python_example = json.loads(python_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59b601b-c12a-447c-a881-01c810723c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['repo_name', 'ref', 'path', 'content'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527b0370-4037-407d-9c50-63c1eb3de919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patrickstocklin/chattR'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_example[\"repo_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb53dda-2d9b-4403-9696-7282c9a5a44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refs/heads/master'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_example[\"ref\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4acce3df-2e50-44fc-b4c0-cad2021dd6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lib/python2.7/site-packages/pip/index.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_example[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0951c99-8d55-452d-a30f-7cab9bb6f9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"\"\"Routines related to PyPI, indexes\"\"\"\\n'\n",
      " 'from __future__ import absolute_import\\n'\n",
      " '\\n'\n",
      " 'import logging\\n'\n",
      " 'import cgi\\n'\n",
      " 'from collections import namedtuple\\n'\n",
      " 'import itertools\\n'\n",
      " 'import sys\\n'\n",
      " 'import os\\n'\n",
      " 'import re\\n'\n",
      " 'import mimetypes\\n'\n",
      " 'import posixpath\\n'\n",
      " 'import warnings\\n'\n",
      " '\\n'\n",
      " 'from pip._vendor.six.moves.urllib import parse as urllib_parse\\n'\n",
      " 'from pip._vendor.six.moves.urllib import request as urllib_request\\n'\n",
      " '\\n'\n",
      " 'from pip.compat import ipaddress\\n'\n",
      " 'from pip.utils import (\\n'\n",
      " '    Inf, cached_property, normalize_name, splitext, normalize_path,\\n'\n",
      " '    ARCHIVE_EXTENSIONS, SUPPORTED_EXTENSIONS)\\n'\n",
      " 'from pip.utils.deprecation import RemovedInPip8Warning\\n'\n",
      " 'from pip.utils.logging import indent_log\\n'\n",
      " 'from pip.exceptions import (\\n'\n",
      " '    DistributionNotFound, BestVersionAlreadyInstalled, '\n",
      " 'InvalidWheelFilename,\\n'\n",
      " '    UnsupportedWheel,\\n'\n",
      " ')\\n'\n",
      " 'from pip.download import HAS_TLS, url_to_path, path_to_url\\n'\n",
      " 'from pip.models import PyPI\\n'\n",
      " 'from pip.wheel import Wheel, wheel_ext\\n'\n",
      " 'from pip.pep425tags import supported_tags, supported_tags_noarch, '\n",
      " 'get_platform\\n'\n",
      " 'from pip._vendor import html5lib, requests, pkg_resources, six\\n'\n",
      " 'from pip._vendor.packaging.version import parse as parse_version\\n'\n",
      " 'from pip._vendor.requests.exceptions import SSLError\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"__all__ = ['FormatControl', 'fmt_ctl_handle_mutual_exclude', \"\n",
      " \"'PackageFinder']\\n\"\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"# Taken from Chrome's list of secure origins (See: http://bit.ly/1qrySKC)\\n\"\n",
      " 'SECURE_ORIGINS = [\\n'\n",
      " '    # protocol, hostname, port\\n'\n",
      " '    (\"https\", \"*\", \"*\"),\\n'\n",
      " '    (\"*\", \"localhost\", \"*\"),\\n'\n",
      " '    (\"*\", \"127.0.0.0/8\", \"*\"),\\n'\n",
      " '    (\"*\", \"::1/128\", \"*\"),\\n'\n",
      " '    (\"file\", \"*\", None),\\n'\n",
      " ']\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'logger = logging.getLogger(__name__)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class InstallationCandidate(object):\\n'\n",
      " '\\n'\n",
      " '    def __init__(self, project, version, location):\\n'\n",
      " '        self.project = project\\n'\n",
      " '        self.version = parse_version(version)\\n'\n",
      " '        self.location = location\\n'\n",
      " '        self._key = (self.project, self.version, self.location)\\n'\n",
      " '\\n'\n",
      " '    def __repr__(self):\\n'\n",
      " '        return \"<InstallationCandidate({0!r}, {1!r}, {2!r})>\".format(\\n'\n",
      " '            self.project, self.version, self.location,\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    def __hash__(self):\\n'\n",
      " '        return hash(self._key)\\n'\n",
      " '\\n'\n",
      " '    def __lt__(self, other):\\n'\n",
      " '        return self._compare(other, lambda s, o: s < o)\\n'\n",
      " '\\n'\n",
      " '    def __le__(self, other):\\n'\n",
      " '        return self._compare(other, lambda s, o: s <= o)\\n'\n",
      " '\\n'\n",
      " '    def __eq__(self, other):\\n'\n",
      " '        return self._compare(other, lambda s, o: s == o)\\n'\n",
      " '\\n'\n",
      " '    def __ge__(self, other):\\n'\n",
      " '        return self._compare(other, lambda s, o: s >= o)\\n'\n",
      " '\\n'\n",
      " '    def __gt__(self, other):\\n'\n",
      " '        return self._compare(other, lambda s, o: s > o)\\n'\n",
      " '\\n'\n",
      " '    def __ne__(self, other):\\n'\n",
      " '        return self._compare(other, lambda s, o: s != o)\\n'\n",
      " '\\n'\n",
      " '    def _compare(self, other, method):\\n'\n",
      " '        if not isinstance(other, InstallationCandidate):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '\\n'\n",
      " '        return method(self._key, other._key)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class PackageFinder(object):\\n'\n",
      " '    \"\"\"This finds packages.\\n'\n",
      " '\\n'\n",
      " \"    This is meant to match easy_install's technique for looking for\\n\"\n",
      " '    packages, by reading pages and looking for appropriate links.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    def __init__(self, find_links, index_urls,\\n'\n",
      " '                 allow_external=(), allow_unverified=(),\\n'\n",
      " '                 allow_all_external=False, allow_all_prereleases=False,\\n'\n",
      " '                 trusted_hosts=None, process_dependency_links=False,\\n'\n",
      " '                 session=None, format_control=None):\\n'\n",
      " '        \"\"\"Create a PackageFinder.\\n'\n",
      " '\\n'\n",
      " '        :param format_control: A FormatControl object or None. Used to '\n",
      " 'control\\n'\n",
      " '            the selection of source packages / binary packages when '\n",
      " 'consulting\\n'\n",
      " '            the index and links.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        if session is None:\\n'\n",
      " '            raise TypeError(\\n'\n",
      " '                \"PackageFinder() missing 1 required keyword argument: \"\\n'\n",
      " '                \"\\'session\\'\"\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        # Build find_links. If an argument starts with ~, it may be\\n'\n",
      " '        # a local file relative to a home directory. So try normalizing\\n'\n",
      " '        # it and if it exists, use the normalized version.\\n'\n",
      " '        # This is deliberately conservative - it might be fine just to\\n'\n",
      " '        # blindly normalize anything starting with a ~...\\n'\n",
      " '        self.find_links = []\\n'\n",
      " '        for link in find_links:\\n'\n",
      " \"            if link.startswith('~'):\\n\"\n",
      " '                new_link = normalize_path(link)\\n'\n",
      " '                if os.path.exists(new_link):\\n'\n",
      " '                    link = new_link\\n'\n",
      " '            self.find_links.append(link)\\n'\n",
      " '\\n'\n",
      " '        self.index_urls = index_urls\\n'\n",
      " '        self.dependency_links = []\\n'\n",
      " '\\n'\n",
      " '        # These are boring links that have already been logged somehow:\\n'\n",
      " '        self.logged_links = set()\\n'\n",
      " '\\n'\n",
      " '        self.format_control = format_control or FormatControl(set(), set())\\n'\n",
      " '\\n'\n",
      " '        # Do we allow (safe and verifiable) externally hosted files?\\n'\n",
      " '        self.allow_external = set(normalize_name(n) for n in '\n",
      " 'allow_external)\\n'\n",
      " '\\n'\n",
      " '        # Which names are allowed to install insecure and unverifiable '\n",
      " 'files?\\n'\n",
      " '        self.allow_unverified = set(\\n'\n",
      " '            normalize_name(n) for n in allow_unverified\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '        # Anything that is allowed unverified is also allowed external\\n'\n",
      " '        self.allow_external |= self.allow_unverified\\n'\n",
      " '\\n'\n",
      " '        # Do we allow all (safe and verifiable) externally hosted files?\\n'\n",
      " '        self.allow_all_external = allow_all_external\\n'\n",
      " '\\n'\n",
      " \"        # Domains that we won't emit warnings for when not using HTTPS\\n\"\n",
      " '        self.secure_origins = [\\n'\n",
      " '            (\"*\", host, \"*\")\\n'\n",
      " '            for host in (trusted_hosts if trusted_hosts else [])\\n'\n",
      " '        ]\\n'\n",
      " '\\n'\n",
      " '        # Stores if we ignored any external links so that we can instruct\\n'\n",
      " '        #   end users how to install them if no distributions are available\\n'\n",
      " '        self.need_warn_external = False\\n'\n",
      " '\\n'\n",
      " '        # Stores if we ignored any unsafe links so that we can instruct\\n'\n",
      " '        #   end users how to install them if no distributions are available\\n'\n",
      " '        self.need_warn_unverified = False\\n'\n",
      " '\\n'\n",
      " '        # Do we want to allow _all_ pre-releases?\\n'\n",
      " '        self.allow_all_prereleases = allow_all_prereleases\\n'\n",
      " '\\n'\n",
      " '        # Do we process dependency links?\\n'\n",
      " '        self.process_dependency_links = process_dependency_links\\n'\n",
      " '\\n'\n",
      " \"        # The Session we'll use to make requests\\n\"\n",
      " '        self.session = session\\n'\n",
      " '\\n'\n",
      " \"        # If we don't have TLS enabled, then WARN if anyplace we're looking\\n\"\n",
      " '        # relies on TLS.\\n'\n",
      " '        if not HAS_TLS:\\n'\n",
      " '            for link in itertools.chain(self.index_urls, self.find_links):\\n'\n",
      " '                parsed = urllib_parse.urlparse(link)\\n'\n",
      " '                if parsed.scheme == \"https\":\\n'\n",
      " '                    logger.warning(\\n'\n",
      " '                        \"pip is configured with locations that require \"\\n'\n",
      " '                        \"TLS/SSL, however the ssl module in Python is not \"\\n'\n",
      " '                        \"available.\"\\n'\n",
      " '                    )\\n'\n",
      " '                    break\\n'\n",
      " '\\n'\n",
      " '    def add_dependency_links(self, links):\\n'\n",
      " \"        # # FIXME: this shouldn't be global list this, it should only\\n\"\n",
      " '        # # apply to requirements of the package that specifies the\\n'\n",
      " '        # # dependency_links value\\n'\n",
      " '        # # FIXME: also, we should track comes_from (i.e., use Link)\\n'\n",
      " '        if self.process_dependency_links:\\n'\n",
      " '            warnings.warn(\\n'\n",
      " '                \"Dependency Links processing has been deprecated and will be '\n",
      " '\"\\n'\n",
      " '                \"removed in a future release.\",\\n'\n",
      " '                RemovedInPip8Warning,\\n'\n",
      " '            )\\n'\n",
      " '            self.dependency_links.extend(links)\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _sort_locations(locations, expand_dir=False):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Sort locations into \"files\" (archives) and \"urls\", and return\\n'\n",
      " '        a pair of lists (files,urls)\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        files = []\\n'\n",
      " '        urls = []\\n'\n",
      " '\\n'\n",
      " '        # puts the url for the given file path into the appropriate list\\n'\n",
      " '        def sort_path(path):\\n'\n",
      " '            url = path_to_url(path)\\n'\n",
      " \"            if mimetypes.guess_type(url, strict=False)[0] == 'text/html':\\n\"\n",
      " '                urls.append(url)\\n'\n",
      " '            else:\\n'\n",
      " '                files.append(url)\\n'\n",
      " '\\n'\n",
      " '        for url in locations:\\n'\n",
      " '\\n'\n",
      " '            is_local_path = os.path.exists(url)\\n'\n",
      " \"            is_file_url = url.startswith('file:')\\n\"\n",
      " '\\n'\n",
      " '            if is_local_path or is_file_url:\\n'\n",
      " '                if is_local_path:\\n'\n",
      " '                    path = url\\n'\n",
      " '                else:\\n'\n",
      " '                    path = url_to_path(url)\\n'\n",
      " '                if os.path.isdir(path):\\n'\n",
      " '                    if expand_dir:\\n'\n",
      " '                        path = os.path.realpath(path)\\n'\n",
      " '                        for item in os.listdir(path):\\n'\n",
      " '                            sort_path(os.path.join(path, item))\\n'\n",
      " '                    elif is_file_url:\\n'\n",
      " '                        urls.append(url)\\n'\n",
      " '                elif os.path.isfile(path):\\n'\n",
      " '                    sort_path(path)\\n'\n",
      " '            else:\\n'\n",
      " '                urls.append(url)\\n'\n",
      " '\\n'\n",
      " '        return files, urls\\n'\n",
      " '\\n'\n",
      " '    def _candidate_sort_key(self, candidate):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Function used to generate link sort key for link tuples.\\n'\n",
      " '        The greater the return value, the more preferred it is.\\n'\n",
      " '        If not finding wheels, then sorted by version only.\\n'\n",
      " '        If finding wheels, then the sort order is by version, then:\\n'\n",
      " '          1. existing installs\\n'\n",
      " '          2. wheels ordered via Wheel.support_index_min()\\n'\n",
      " '          3. source archives\\n'\n",
      " '        Note: it was considered to embed this logic into the Link\\n'\n",
      " '              comparison operators, but then different sdist links\\n'\n",
      " '              with the same version, would have to be considered equal\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        support_num = len(supported_tags)\\n'\n",
      " '        if candidate.location == INSTALLED_VERSION:\\n'\n",
      " '            pri = 1\\n'\n",
      " '        elif candidate.location.is_wheel:\\n'\n",
      " '            # can raise InvalidWheelFilename\\n'\n",
      " '            wheel = Wheel(candidate.location.filename)\\n'\n",
      " '            if not wheel.supported():\\n'\n",
      " '                raise UnsupportedWheel(\\n'\n",
      " '                    \"%s is not a supported wheel for this platform. It \"\\n'\n",
      " '                    \"can\\'t be sorted.\" % wheel.filename\\n'\n",
      " '                )\\n'\n",
      " '            pri = -(wheel.support_index_min())\\n'\n",
      " '        else:  # sdist\\n'\n",
      " '            pri = -(support_num)\\n'\n",
      " '        return (candidate.version, pri)\\n'\n",
      " '\\n'\n",
      " '    def _sort_versions(self, applicable_versions):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Bring the latest version (and wheels) to the front, but maintain '\n",
      " 'the\\n'\n",
      " '        existing ordering as secondary. See the docstring for '\n",
      " '`_link_sort_key`\\n'\n",
      " '        for details. This function is isolated for easier unit testing.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        return sorted(\\n'\n",
      " '            applicable_versions,\\n'\n",
      " '            key=self._candidate_sort_key,\\n'\n",
      " '            reverse=True\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    def _validate_secure_origin(self, logger, location):\\n'\n",
      " '        # Determine if this url used a secure transport mechanism\\n'\n",
      " '        parsed = urllib_parse.urlparse(str(location))\\n'\n",
      " '        origin = (parsed.scheme, parsed.hostname, parsed.port)\\n'\n",
      " '\\n'\n",
      " '        # Determine if our origin is a secure origin by looking through our\\n'\n",
      " '        # hardcoded list of secure origins, as well as any additional ones\\n'\n",
      " '        # configured on this PackageFinder instance.\\n'\n",
      " '        for secure_origin in (SECURE_ORIGINS + self.secure_origins):\\n'\n",
      " '            # Check to see if the protocol matches\\n'\n",
      " '            if origin[0] != secure_origin[0] and secure_origin[0] != \"*\":\\n'\n",
      " '                continue\\n'\n",
      " '\\n'\n",
      " '            try:\\n'\n",
      " '                # We need to do this decode dance to ensure that we have a\\n'\n",
      " '                # unicode object, even on Python 2.x.\\n'\n",
      " '                addr = ipaddress.ip_address(\\n'\n",
      " '                    origin[1]\\n'\n",
      " '                    if (\\n'\n",
      " '                        isinstance(origin[1], six.text_type) or\\n'\n",
      " '                        origin[1] is None\\n'\n",
      " '                    )\\n'\n",
      " '                    else origin[1].decode(\"utf8\")\\n'\n",
      " '                )\\n'\n",
      " '                network = ipaddress.ip_network(\\n'\n",
      " '                    secure_origin[1]\\n'\n",
      " '                    if isinstance(secure_origin[1], six.text_type)\\n'\n",
      " '                    else secure_origin[1].decode(\"utf8\")\\n'\n",
      " '                )\\n'\n",
      " '            except ValueError:\\n'\n",
      " \"                # We don't have both a valid address or a valid network, so\\n\"\n",
      " \"                # we'll check this origin against hostnames.\\n\"\n",
      " '                if origin[1] != secure_origin[1] and secure_origin[1] != '\n",
      " '\"*\":\\n'\n",
      " '                    continue\\n'\n",
      " '            else:\\n'\n",
      " '                # We have a valid address and network, so see if the '\n",
      " 'address\\n'\n",
      " '                # is contained within the network.\\n'\n",
      " '                if addr not in network:\\n'\n",
      " '                    continue\\n'\n",
      " '\\n'\n",
      " '            # Check to see if the port patches\\n'\n",
      " '            if (origin[2] != secure_origin[2] and\\n'\n",
      " '                    secure_origin[2] != \"*\" and\\n'\n",
      " '                    secure_origin[2] is not None):\\n'\n",
      " '                continue\\n'\n",
      " '\\n'\n",
      " \"            # If we've gotten here, then this origin matches the current\\n\"\n",
      " '            # secure origin and we should return True\\n'\n",
      " '            return True\\n'\n",
      " '\\n'\n",
      " \"        # If we've gotten to this point, then the origin isn't secure and \"\n",
      " 'we\\n'\n",
      " '        # will not accept it as a valid location to search. We will however\\n'\n",
      " '        # log a warning that we are ignoring it.\\n'\n",
      " '        logger.warning(\\n'\n",
      " '            \"The repository located at %s is not a trusted or secure host '\n",
      " 'and \"\\n'\n",
      " '            \"is being ignored. If this repository is available via HTTPS it '\n",
      " '\"\\n'\n",
      " '            \"is recommended to use HTTPS instead, otherwise you may silence '\n",
      " '\"\\n'\n",
      " '            \"this warning and allow it anyways with \\'--trusted-host '\n",
      " '%s\\'.\",\\n'\n",
      " '            parsed.hostname,\\n'\n",
      " '            parsed.hostname,\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '        return False\\n'\n",
      " '\\n'\n",
      " '    def _get_index_urls_locations(self, project_name):\\n'\n",
      " '        \"\"\"Returns the locations found via self.index_urls\\n'\n",
      " '\\n'\n",
      " '        Checks the url_name on the main (first in the list) index and\\n'\n",
      " '        use this url_name to produce all locations\\n'\n",
      " '        \"\"\"\\n'\n",
      " '\\n'\n",
      " '        def mkurl_pypi_url(url):\\n'\n",
      " '            loc = posixpath.join(url, project_url_name)\\n'\n",
      " '            # For maximum compatibility with easy_install, ensure the path\\n'\n",
      " \"            # ends in a trailing slash.  Although this isn't in the spec\\n\"\n",
      " '            # (and PyPI can handle it without the slash) some other index\\n'\n",
      " \"            # implementations might break if they relied on easy_install's\\n\"\n",
      " '            # behavior.\\n'\n",
      " \"            if not loc.endswith('/'):\\n\"\n",
      " \"                loc = loc + '/'\\n\"\n",
      " '            return loc\\n'\n",
      " '\\n'\n",
      " '        project_url_name = urllib_parse.quote(project_name.lower())\\n'\n",
      " '\\n'\n",
      " '        if self.index_urls:\\n'\n",
      " '            # Check that we have the url_name correctly spelled:\\n'\n",
      " '\\n'\n",
      " '            # Only check main index if index URL is given\\n'\n",
      " '            main_index_url = Link(\\n'\n",
      " '                mkurl_pypi_url(self.index_urls[0]),\\n'\n",
      " '                trusted=True,\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '            page = self._get_page(main_index_url)\\n'\n",
      " '            if page is None and PyPI.netloc not in str(main_index_url):\\n'\n",
      " '                warnings.warn(\\n'\n",
      " '                    \"Failed to find %r at %s. It is suggested to upgrade \"\\n'\n",
      " '                    \"your index to support normalized names as the name in '\n",
      " '\"\\n'\n",
      " '                    \"/simple/{name}.\" % (project_name, main_index_url),\\n'\n",
      " '                    RemovedInPip8Warning,\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '                project_url_name = self._find_url_name(\\n'\n",
      " '                    Link(self.index_urls[0], trusted=True),\\n'\n",
      " '                    project_url_name,\\n'\n",
      " '                ) or project_url_name\\n'\n",
      " '\\n'\n",
      " '        if project_url_name is not None:\\n'\n",
      " '            return [mkurl_pypi_url(url) for url in self.index_urls]\\n'\n",
      " '        return []\\n'\n",
      " '\\n'\n",
      " '    def _find_all_versions(self, project_name):\\n'\n",
      " '        \"\"\"Find all available versions for project_name\\n'\n",
      " '\\n'\n",
      " '        This checks index_urls, find_links and dependency_links\\n'\n",
      " '        All versions found are returned\\n'\n",
      " '\\n'\n",
      " '        See _link_package_versions for details on which files are accepted\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        index_locations = self._get_index_urls_locations(project_name)\\n'\n",
      " '        index_file_loc, index_url_loc = '\n",
      " 'self._sort_locations(index_locations)\\n'\n",
      " '        fl_file_loc, fl_url_loc = self._sort_locations(\\n'\n",
      " '            self.find_links, expand_dir=True)\\n'\n",
      " '        dep_file_loc, dep_url_loc = '\n",
      " 'self._sort_locations(self.dependency_links)\\n'\n",
      " '\\n'\n",
      " '        file_locations = (\\n'\n",
      " '            Link(url) for url in itertools.chain(\\n'\n",
      " '                index_file_loc, fl_file_loc, dep_file_loc)\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '        # We trust every url that the user has given us whether it was '\n",
      " 'given\\n'\n",
      " '        #   via --index-url or --find-links\\n'\n",
      " '        # We explicitly do not trust links that came from dependency_links\\n'\n",
      " '        # We want to filter out any thing which does not have a secure '\n",
      " 'origin.\\n'\n",
      " '        url_locations = [\\n'\n",
      " '            link for link in itertools.chain(\\n'\n",
      " '                (Link(url, trusted=True) for url in index_url_loc),\\n'\n",
      " '                (Link(url, trusted=True) for url in fl_url_loc),\\n'\n",
      " '                (Link(url) for url in dep_url_loc),\\n'\n",
      " '            )\\n'\n",
      " '            if self._validate_secure_origin(logger, link)\\n'\n",
      " '        ]\\n'\n",
      " '\\n'\n",
      " \"        logger.debug('%d location(s) to search for versions of %s:',\\n\"\n",
      " '                     len(url_locations), project_name)\\n'\n",
      " '\\n'\n",
      " '        for location in url_locations:\\n'\n",
      " \"            logger.debug('* %s', location)\\n\"\n",
      " '\\n'\n",
      " '        canonical_name = pkg_resources.safe_name(project_name).lower()\\n'\n",
      " '        formats = fmt_ctl_formats(self.format_control, canonical_name)\\n'\n",
      " '        search = Search(project_name.lower(), canonical_name, formats)\\n'\n",
      " '        find_links_versions = self._package_versions(\\n'\n",
      " '            # We trust every directly linked archive in find_links\\n'\n",
      " \"            (Link(url, '-f', trusted=True) for url in self.find_links),\\n\"\n",
      " '            search\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '        page_versions = []\\n'\n",
      " '        for page in self._get_pages(url_locations, project_name):\\n'\n",
      " \"            logger.debug('Analyzing links from page %s', page.url)\\n\"\n",
      " '            with indent_log():\\n'\n",
      " '                page_versions.extend(\\n'\n",
      " '                    self._package_versions(page.links, search)\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '        dependency_versions = self._package_versions(\\n'\n",
      " '            (Link(url) for url in self.dependency_links), search\\n'\n",
      " '        )\\n'\n",
      " '        if dependency_versions:\\n'\n",
      " '            logger.debug(\\n'\n",
      " \"                'dependency_links found: %s',\\n\"\n",
      " \"                ', '.join([\\n\"\n",
      " '                    version.location.url for version in dependency_versions\\n'\n",
      " '                ])\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        file_versions = self._package_versions(file_locations, search)\\n'\n",
      " '        if file_versions:\\n'\n",
      " '            file_versions.sort(reverse=True)\\n'\n",
      " '            logger.debug(\\n'\n",
      " \"                'Local files found: %s',\\n\"\n",
      " \"                ', '.join([\\n\"\n",
      " '                    url_to_path(candidate.location.url)\\n'\n",
      " '                    for candidate in file_versions\\n'\n",
      " '                ])\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        # This is an intentional priority ordering\\n'\n",
      " '        return (\\n'\n",
      " '            file_versions + find_links_versions + page_versions +\\n'\n",
      " '            dependency_versions\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    def find_requirement(self, req, upgrade):\\n'\n",
      " '        \"\"\"Try to find an InstallationCandidate for req\\n'\n",
      " '\\n'\n",
      " '        Expects req, an InstallRequirement and upgrade, a boolean\\n'\n",
      " '        Returns an InstallationCandidate or None\\n'\n",
      " '        May raise DistributionNotFound or BestVersionAlreadyInstalled\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        all_versions = self._find_all_versions(req.name)\\n'\n",
      " '\\n'\n",
      " \"        # Filter out anything which doesn't match our specifier\\n\"\n",
      " '        _versions = set(\\n'\n",
      " '            req.specifier.filter(\\n'\n",
      " '                # We turn the version object into a str here because '\n",
      " 'otherwise\\n'\n",
      " \"                # when we're debundled but setuptools isn't, Python will \"\n",
      " 'see\\n'\n",
      " '                # packaging.version.Version and\\n'\n",
      " '                # pkg_resources._vendor.packaging.version.Version as '\n",
      " 'different\\n'\n",
      " \"                # types. This way we'll use a str as a common data \"\n",
      " 'interchange\\n'\n",
      " '                # format. If we stop using the pkg_resources provided '\n",
      " 'specifier\\n'\n",
      " '                # and start using our own, we can drop the cast to str().\\n'\n",
      " '                [str(x.version) for x in all_versions],\\n'\n",
      " '                prereleases=(\\n'\n",
      " '                    self.allow_all_prereleases\\n'\n",
      " '                    if self.allow_all_prereleases else None\\n'\n",
      " '                ),\\n'\n",
      " '            )\\n'\n",
      " '        )\\n'\n",
      " '        applicable_versions = [\\n'\n",
      " '            # Again, converting to str to deal with debundling.\\n'\n",
      " '            x for x in all_versions if str(x.version) in _versions\\n'\n",
      " '        ]\\n'\n",
      " '\\n'\n",
      " '        if req.satisfied_by is not None:\\n'\n",
      " '            # Finally add our existing versions to the front of our '\n",
      " 'versions.\\n'\n",
      " '            applicable_versions.insert(\\n'\n",
      " '                0,\\n'\n",
      " '                InstallationCandidate(\\n'\n",
      " '                    req.name,\\n'\n",
      " '                    req.satisfied_by.version,\\n'\n",
      " '                    INSTALLED_VERSION,\\n'\n",
      " '                )\\n'\n",
      " '            )\\n'\n",
      " '            existing_applicable = True\\n'\n",
      " '        else:\\n'\n",
      " '            existing_applicable = False\\n'\n",
      " '\\n'\n",
      " '        applicable_versions = self._sort_versions(applicable_versions)\\n'\n",
      " '\\n'\n",
      " '        if not upgrade and existing_applicable:\\n'\n",
      " '            if applicable_versions[0].location is INSTALLED_VERSION:\\n'\n",
      " '                logger.debug(\\n'\n",
      " \"                    'Existing installed version (%s) is most up-to-date and \"\n",
      " \"'\\n\"\n",
      " \"                    'satisfies requirement',\\n\"\n",
      " '                    req.satisfied_by.version,\\n'\n",
      " '                )\\n'\n",
      " '            else:\\n'\n",
      " '                logger.debug(\\n'\n",
      " \"                    'Existing installed version (%s) satisfies requirement \"\n",
      " \"'\\n\"\n",
      " \"                    '(most up-to-date version is %s)',\\n\"\n",
      " '                    req.satisfied_by.version,\\n'\n",
      " '                    applicable_versions[0][2],\\n'\n",
      " '                )\\n'\n",
      " '            return None\\n'\n",
      " '\\n'\n",
      " '        if not applicable_versions:\\n'\n",
      " '            logger.critical(\\n'\n",
      " \"                'Could not find a version that satisfies the requirement %s \"\n",
      " \"'\\n\"\n",
      " \"                '(from versions: %s)',\\n\"\n",
      " '                req,\\n'\n",
      " \"                ', '.join(\\n\"\n",
      " '                    sorted(\\n'\n",
      " '                        set(str(i.version) for i in all_versions),\\n'\n",
      " '                        key=parse_version,\\n'\n",
      " '                    )\\n'\n",
      " '                )\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '            if self.need_warn_external:\\n'\n",
      " '                logger.warning(\\n'\n",
      " '                    \"Some externally hosted files were ignored as access to '\n",
      " '\"\\n'\n",
      " '                    \"them may be unreliable (use --allow-external %s to \"\\n'\n",
      " '                    \"allow).\",\\n'\n",
      " '                    req.name,\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '            if self.need_warn_unverified:\\n'\n",
      " '                logger.warning(\\n'\n",
      " '                    \"Some insecure and unverifiable files were ignored\"\\n'\n",
      " '                    \" (use --allow-unverified %s to allow).\",\\n'\n",
      " '                    req.name,\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '            raise DistributionNotFound(\\n'\n",
      " \"                'No matching distribution found for %s' % req\\n\"\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        if applicable_versions[0].location is INSTALLED_VERSION:\\n'\n",
      " '            # We have an existing version, and its the best version\\n'\n",
      " '            logger.debug(\\n'\n",
      " \"                'Installed version (%s) is most up-to-date (past versions: \"\n",
      " \"'\\n\"\n",
      " \"                '%s)',\\n\"\n",
      " '                req.satisfied_by.version,\\n'\n",
      " \"                ', '.join(str(i.version) for i in applicable_versions[1:]) \"\n",
      " 'or\\n'\n",
      " '                \"none\",\\n'\n",
      " '            )\\n'\n",
      " '            raise BestVersionAlreadyInstalled\\n'\n",
      " '\\n'\n",
      " '        if len(applicable_versions) > 1:\\n'\n",
      " '            logger.debug(\\n'\n",
      " \"                'Using version %s (newest of versions: %s)',\\n\"\n",
      " '                applicable_versions[0].version,\\n'\n",
      " \"                ', '.join(str(i.version) for i in applicable_versions)\\n\"\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        selected_version = applicable_versions[0].location\\n'\n",
      " '\\n'\n",
      " '        if (selected_version.verifiable is not None and not\\n'\n",
      " '                selected_version.verifiable):\\n'\n",
      " '            logger.warning(\\n'\n",
      " '                \"%s is potentially insecure and unverifiable.\", req.name,\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        return selected_version\\n'\n",
      " '\\n'\n",
      " '    def _find_url_name(self, index_url, url_name):\\n'\n",
      " '        \"\"\"\\n'\n",
      " \"        Finds the true URL name of a package, when the given name isn't \"\n",
      " 'quite\\n'\n",
      " '        correct.\\n'\n",
      " '        This is usually used to implement case-insensitivity.\\n'\n",
      " '        \"\"\"\\n'\n",
      " \"        if not index_url.url.endswith('/'):\\n\"\n",
      " '            # Vaguely part of the PyPI API... weird but true.\\n'\n",
      " '            # FIXME: bad to modify this?\\n'\n",
      " \"            index_url.url += '/'\\n\"\n",
      " '        page = self._get_page(index_url)\\n'\n",
      " '        if page is None:\\n'\n",
      " \"            logger.critical('Cannot fetch index base URL %s', index_url)\\n\"\n",
      " '            return\\n'\n",
      " '        norm_name = normalize_name(url_name)\\n'\n",
      " '        for link in page.links:\\n'\n",
      " \"            base = posixpath.basename(link.path.rstrip('/'))\\n\"\n",
      " '            if norm_name == normalize_name(base):\\n'\n",
      " '                logger.debug(\\n'\n",
      " \"                    'Real name of requirement %s is %s', url_name, base,\\n\"\n",
      " '                )\\n'\n",
      " '                return base\\n'\n",
      " '        return None\\n'\n",
      " '\\n'\n",
      " '    def _get_pages(self, locations, project_name):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Yields (page, page_url) from the given locations, skipping\\n'\n",
      " '        locations that have errors, and adding download/homepage links\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        all_locations = list(locations)\\n'\n",
      " '        seen = set()\\n'\n",
      " '        normalized = normalize_name(project_name)\\n'\n",
      " '\\n'\n",
      " '        while all_locations:\\n'\n",
      " '            location = all_locations.pop(0)\\n'\n",
      " '            if location in seen:\\n'\n",
      " '                continue\\n'\n",
      " '            seen.add(location)\\n'\n",
      " '\\n'\n",
      " '            page = self._get_page(location)\\n'\n",
      " '            if page is None:\\n'\n",
      " '                continue\\n'\n",
      " '\\n'\n",
      " '            yield page\\n'\n",
      " '\\n'\n",
      " '            for link in page.rel_links():\\n'\n",
      " '\\n'\n",
      " '                if (normalized not in self.allow_external and not\\n'\n",
      " '                        self.allow_all_external):\\n'\n",
      " '                    self.need_warn_external = True\\n'\n",
      " '                    logger.debug(\\n'\n",
      " '                        \"Not searching %s for files because external \"\\n'\n",
      " '                        \"urls are disallowed.\",\\n'\n",
      " '                        link,\\n'\n",
      " '                    )\\n'\n",
      " '                    continue\\n'\n",
      " '\\n'\n",
      " '                if (link.trusted is not None and not\\n'\n",
      " '                        link.trusted and\\n'\n",
      " '                        normalized not in self.allow_unverified):\\n'\n",
      " '                    logger.debug(\\n'\n",
      " '                        \"Not searching %s for urls, it is an \"\\n'\n",
      " '                        \"untrusted link and cannot produce safe or \"\\n'\n",
      " '                        \"verifiable files.\",\\n'\n",
      " '                        link,\\n'\n",
      " '                    )\\n'\n",
      " '                    self.need_warn_unverified = True\\n'\n",
      " '                    continue\\n'\n",
      " '\\n'\n",
      " '                all_locations.append(link)\\n'\n",
      " '\\n'\n",
      " \"    _py_version_re = re.compile(r'-py([123]\\\\.?[0-9]?)$')\\n\"\n",
      " '\\n'\n",
      " '    def _sort_links(self, links):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Returns elements of links in order, non-egg links first, egg links\\n'\n",
      " '        second, while eliminating duplicates\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        eggs, no_eggs = [], []\\n'\n",
      " '        seen = set()\\n'\n",
      " '        for link in links:\\n'\n",
      " '            if link not in seen:\\n'\n",
      " '                seen.add(link)\\n'\n",
      " '                if link.egg_fragment:\\n'\n",
      " '                    eggs.append(link)\\n'\n",
      " '                else:\\n'\n",
      " '                    no_eggs.append(link)\\n'\n",
      " '        return no_eggs + eggs\\n'\n",
      " '\\n'\n",
      " '    def _package_versions(self, links, search):\\n'\n",
      " '        result = []\\n'\n",
      " '        for link in self._sort_links(links):\\n'\n",
      " '            v = self._link_package_versions(link, search)\\n'\n",
      " '            if v is not None:\\n'\n",
      " '                result.append(v)\\n'\n",
      " '        return result\\n'\n",
      " '\\n'\n",
      " '    def _log_skipped_link(self, link, reason):\\n'\n",
      " '        if link not in self.logged_links:\\n'\n",
      " \"            logger.debug('Skipping link %s; %s', link, reason)\\n\"\n",
      " '            self.logged_links.add(link)\\n'\n",
      " '\\n'\n",
      " '    def _link_package_versions(self, link, search):\\n'\n",
      " '        \"\"\"Return an InstallationCandidate or None\"\"\"\\n'\n",
      " '        platform = get_platform()\\n'\n",
      " '\\n'\n",
      " '        version = None\\n'\n",
      " '        if link.egg_fragment:\\n'\n",
      " '            egg_info = link.egg_fragment\\n'\n",
      " '            ext = link.ext\\n'\n",
      " '        else:\\n'\n",
      " '            egg_info, ext = link.splitext()\\n'\n",
      " '            if not ext:\\n'\n",
      " \"                self._log_skipped_link(link, 'not a file')\\n\"\n",
      " '                return\\n'\n",
      " '            if ext not in SUPPORTED_EXTENSIONS:\\n'\n",
      " '                self._log_skipped_link(\\n'\n",
      " \"                    link, 'unsupported archive format: %s' % ext)\\n\"\n",
      " '                return\\n'\n",
      " '            if \"binary\" not in search.formats and ext == wheel_ext:\\n'\n",
      " '                self._log_skipped_link(\\n'\n",
      " \"                    link, 'No binaries permitted for %s' % search.supplied)\\n\"\n",
      " '                return\\n'\n",
      " '            if \"macosx10\" in link.path and ext == \\'.zip\\':\\n'\n",
      " \"                self._log_skipped_link(link, 'macosx10 one')\\n\"\n",
      " '                return\\n'\n",
      " '            if ext == wheel_ext:\\n'\n",
      " '                try:\\n'\n",
      " '                    wheel = Wheel(link.filename)\\n'\n",
      " '                except InvalidWheelFilename:\\n'\n",
      " \"                    self._log_skipped_link(link, 'invalid wheel filename')\\n\"\n",
      " '                    return\\n'\n",
      " '                if (pkg_resources.safe_name(wheel.name).lower() !=\\n'\n",
      " '                        search.canonical):\\n'\n",
      " '                    self._log_skipped_link(\\n'\n",
      " \"                        link, 'wrong project name (not %s)' % \"\n",
      " 'search.supplied)\\n'\n",
      " '                    return\\n'\n",
      " '                if not wheel.supported():\\n'\n",
      " '                    self._log_skipped_link(\\n'\n",
      " \"                        link, 'it is not compatible with this Python')\\n\"\n",
      " '                    return\\n'\n",
      " '                # This is a dirty hack to prevent installing Binary Wheels '\n",
      " 'from\\n'\n",
      " '                # PyPI unless it is a Windows or Mac Binary Wheel. This is\\n'\n",
      " '                # paired with a change to PyPI disabling uploads for the\\n'\n",
      " '                # same. Once we have a mechanism for enabling support for\\n'\n",
      " '                # binary wheels on linux that deals with the inherent '\n",
      " 'problems\\n'\n",
      " '                # of binary distribution this can be removed.\\n'\n",
      " '                comes_from = getattr(link, \"comes_from\", None)\\n'\n",
      " '                if (\\n'\n",
      " '                        (\\n'\n",
      " \"                            not platform.startswith('win') and not\\n\"\n",
      " \"                            platform.startswith('macosx') and not\\n\"\n",
      " \"                            platform == 'cli'\\n\"\n",
      " '                        ) and\\n'\n",
      " '                        comes_from is not None and\\n'\n",
      " '                        urllib_parse.urlparse(\\n'\n",
      " '                            comes_from.url\\n'\n",
      " '                        ).netloc.endswith(PyPI.netloc)):\\n'\n",
      " '                    if not wheel.supported(tags=supported_tags_noarch):\\n'\n",
      " '                        self._log_skipped_link(\\n'\n",
      " '                            link,\\n'\n",
      " '                            \"it is a pypi-hosted binary \"\\n'\n",
      " '                            \"Wheel on an unsupported platform\",\\n'\n",
      " '                        )\\n'\n",
      " '                        return\\n'\n",
      " '                version = wheel.version\\n'\n",
      " '\\n'\n",
      " '        # This should be up by the search.ok_binary check, but see issue '\n",
      " '2700.\\n'\n",
      " '        if \"source\" not in search.formats and ext != wheel_ext:\\n'\n",
      " '            self._log_skipped_link(\\n'\n",
      " \"                link, 'No sources permitted for %s' % search.supplied)\\n\"\n",
      " '            return\\n'\n",
      " '\\n'\n",
      " '        if not version:\\n'\n",
      " '            version = egg_info_matches(egg_info, search.supplied, link)\\n'\n",
      " '        if version is None:\\n'\n",
      " '            self._log_skipped_link(\\n'\n",
      " \"                link, 'wrong project name (not %s)' % search.supplied)\\n\"\n",
      " '            return\\n'\n",
      " '\\n'\n",
      " '        if (link.internal is not None and not\\n'\n",
      " '                link.internal and not\\n'\n",
      " '                normalize_name(search.supplied).lower()\\n'\n",
      " '                in self.allow_external and not\\n'\n",
      " '                self.allow_all_external):\\n'\n",
      " '            # We have a link that we are sure is external, so we should '\n",
      " 'skip\\n'\n",
      " '            #   it unless we are allowing externals\\n'\n",
      " \"            self._log_skipped_link(link, 'it is externally hosted')\\n\"\n",
      " '            self.need_warn_external = True\\n'\n",
      " '            return\\n'\n",
      " '\\n'\n",
      " '        if (link.verifiable is not None and not\\n'\n",
      " '                link.verifiable and not\\n'\n",
      " '                (normalize_name(search.supplied).lower()\\n'\n",
      " '                    in self.allow_unverified)):\\n'\n",
      " '            # We have a link that we are sure we cannot verify its '\n",
      " 'integrity,\\n'\n",
      " '            #   so we should skip it unless we are allowing unsafe installs\\n'\n",
      " '            #   for this requirement.\\n'\n",
      " '            self._log_skipped_link(\\n'\n",
      " \"                link, 'it is an insecure and unverifiable file')\\n\"\n",
      " '            self.need_warn_unverified = True\\n'\n",
      " '            return\\n'\n",
      " '\\n'\n",
      " '        match = self._py_version_re.search(version)\\n'\n",
      " '        if match:\\n'\n",
      " '            version = version[:match.start()]\\n'\n",
      " '            py_version = match.group(1)\\n'\n",
      " '            if py_version != sys.version[:3]:\\n'\n",
      " '                self._log_skipped_link(\\n'\n",
      " \"                    link, 'Python version is incorrect')\\n\"\n",
      " '                return\\n'\n",
      " \"        logger.debug('Found link %s, version: %s', link, version)\\n\"\n",
      " '\\n'\n",
      " '        return InstallationCandidate(search.supplied, version, link)\\n'\n",
      " '\\n'\n",
      " '    def _get_page(self, link):\\n'\n",
      " '        return HTMLPage.get_page(link, session=self.session)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'def egg_info_matches(\\n'\n",
      " '        egg_info, search_name, link,\\n'\n",
      " \"        _egg_info_re=re.compile(r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)', re.I)):\\n\"\n",
      " '    \"\"\"Pull the version part out of a string.\\n'\n",
      " '\\n'\n",
      " '    :param egg_info: The string to parse. E.g. foo-2.1\\n'\n",
      " '    :param search_name: The name of the package this belongs to. None to\\n'\n",
      " '        infer the name. Note that this cannot unambiguously parse strings\\n'\n",
      " '        like foo-2-2 which might be foo, 2-2 or foo-2, 2.\\n'\n",
      " '    :param link: The link the string came from, for logging on failure.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '    match = _egg_info_re.search(egg_info)\\n'\n",
      " '    if not match:\\n'\n",
      " \"        logger.debug('Could not parse version from link: %s', link)\\n\"\n",
      " '        return None\\n'\n",
      " '    if search_name is None:\\n'\n",
      " '        full_match = match.group(0)\\n'\n",
      " \"        return full_match[full_match.index('-'):]\\n\"\n",
      " '    name = match.group(0).lower()\\n'\n",
      " '    # To match the \"safe\" name that pkg_resources creates:\\n'\n",
      " \"    name = name.replace('_', '-')\\n\"\n",
      " '    # project name and version must be separated by a dash\\n'\n",
      " '    look_for = search_name.lower() + \"-\"\\n'\n",
      " '    if name.startswith(look_for):\\n'\n",
      " '        return match.group(0)[len(look_for):]\\n'\n",
      " '    else:\\n'\n",
      " '        return None\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class HTMLPage(object):\\n'\n",
      " '    \"\"\"Represents one page, along with its URL\"\"\"\\n'\n",
      " '\\n'\n",
      " '    def __init__(self, content, url, headers=None, trusted=None):\\n'\n",
      " '        # Determine if we have any encoding information in our headers\\n'\n",
      " '        encoding = None\\n'\n",
      " '        if headers and \"Content-Type\" in headers:\\n'\n",
      " '            content_type, params = '\n",
      " 'cgi.parse_header(headers[\"Content-Type\"])\\n'\n",
      " '\\n'\n",
      " '            if \"charset\" in params:\\n'\n",
      " \"                encoding = params['charset']\\n\"\n",
      " '\\n'\n",
      " '        self.content = content\\n'\n",
      " '        self.parsed = html5lib.parse(\\n'\n",
      " '            self.content,\\n'\n",
      " '            encoding=encoding,\\n'\n",
      " '            namespaceHTMLElements=False,\\n'\n",
      " '        )\\n'\n",
      " '        self.url = url\\n'\n",
      " '        self.headers = headers\\n'\n",
      " '        self.trusted = trusted\\n'\n",
      " '\\n'\n",
      " '    def __str__(self):\\n'\n",
      " '        return self.url\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def get_page(cls, link, skip_archives=True, session=None):\\n'\n",
      " '        if session is None:\\n'\n",
      " '            raise TypeError(\\n'\n",
      " '                \"get_page() missing 1 required keyword argument: '\n",
      " '\\'session\\'\"\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        url = link.url\\n'\n",
      " \"        url = url.split('#', 1)[0]\\n\"\n",
      " '\\n'\n",
      " '        # Check for VCS schemes that do not support lookup as web pages.\\n'\n",
      " '        from pip.vcs import VcsSupport\\n'\n",
      " '        for scheme in VcsSupport.schemes:\\n'\n",
      " \"            if url.lower().startswith(scheme) and url[len(scheme)] in '+:':\\n\"\n",
      " \"                logger.debug('Cannot look at %s URL %s', scheme, link)\\n\"\n",
      " '                return None\\n'\n",
      " '\\n'\n",
      " '        try:\\n'\n",
      " '            if skip_archives:\\n'\n",
      " '                filename = link.filename\\n'\n",
      " '                for bad_ext in ARCHIVE_EXTENSIONS:\\n'\n",
      " '                    if filename.endswith(bad_ext):\\n'\n",
      " '                        content_type = cls._get_content_type(\\n'\n",
      " '                            url, session=session,\\n'\n",
      " '                        )\\n'\n",
      " \"                        if content_type.lower().startswith('text/html'):\\n\"\n",
      " '                            break\\n'\n",
      " '                        else:\\n'\n",
      " '                            logger.debug(\\n'\n",
      " \"                                'Skipping page %s because of Content-Type: \"\n",
      " \"%s',\\n\"\n",
      " '                                link,\\n'\n",
      " '                                content_type,\\n'\n",
      " '                            )\\n'\n",
      " '                            return\\n'\n",
      " '\\n'\n",
      " \"            logger.debug('Getting page %s', url)\\n\"\n",
      " '\\n'\n",
      " '            # Tack index.html onto file:// URLs that point to directories\\n'\n",
      " '            (scheme, netloc, path, params, query, fragment) = \\\\\\n'\n",
      " '                urllib_parse.urlparse(url)\\n'\n",
      " \"            if (scheme == 'file' and\\n\"\n",
      " '                    os.path.isdir(urllib_request.url2pathname(path))):\\n'\n",
      " \"                # add trailing slash if not present so urljoin doesn't trim\\n\"\n",
      " '                # final segment\\n'\n",
      " \"                if not url.endswith('/'):\\n\"\n",
      " \"                    url += '/'\\n\"\n",
      " \"                url = urllib_parse.urljoin(url, 'index.html')\\n\"\n",
      " \"                logger.debug(' file: URL is directory, getting %s', url)\\n\"\n",
      " '\\n'\n",
      " '            resp = session.get(\\n'\n",
      " '                url,\\n'\n",
      " '                headers={\\n'\n",
      " '                    \"Accept\": \"text/html\",\\n'\n",
      " '                    \"Cache-Control\": \"max-age=600\",\\n'\n",
      " '                },\\n'\n",
      " '            )\\n'\n",
      " '            resp.raise_for_status()\\n'\n",
      " '\\n'\n",
      " '            # The check for archives above only works if the url ends with\\n'\n",
      " '            #   something that looks like an archive. However that is not a\\n'\n",
      " '            #   requirement of an url. Unless we issue a HEAD request on '\n",
      " 'every\\n'\n",
      " '            #   url we cannot know ahead of time for sure if something is '\n",
      " 'HTML\\n'\n",
      " \"            #   or not. However we can check after we've downloaded it.\\n\"\n",
      " \"            content_type = resp.headers.get('Content-Type', 'unknown')\\n\"\n",
      " '            if not content_type.lower().startswith(\"text/html\"):\\n'\n",
      " '                logger.debug(\\n'\n",
      " \"                    'Skipping page %s because of Content-Type: %s',\\n\"\n",
      " '                    link,\\n'\n",
      " '                    content_type,\\n'\n",
      " '                )\\n'\n",
      " '                return\\n'\n",
      " '\\n'\n",
      " '            inst = cls(\\n'\n",
      " '                resp.content, resp.url, resp.headers,\\n'\n",
      " '                trusted=link.trusted,\\n'\n",
      " '            )\\n'\n",
      " '        except requests.HTTPError as exc:\\n'\n",
      " '            level = 2 if exc.response.status_code == 404 else 1\\n'\n",
      " '            cls._handle_fail(link, exc, url, level=level)\\n'\n",
      " '        except requests.ConnectionError as exc:\\n'\n",
      " '            cls._handle_fail(link, \"connection error: %s\" % exc, url)\\n'\n",
      " '        except requests.Timeout:\\n'\n",
      " '            cls._handle_fail(link, \"timed out\", url)\\n'\n",
      " '        except SSLError as exc:\\n'\n",
      " '            reason = (\"There was a problem confirming the ssl certificate: '\n",
      " '\"\\n'\n",
      " '                      \"%s\" % exc)\\n'\n",
      " '            cls._handle_fail(link, reason, url, level=2, meth=logger.info)\\n'\n",
      " '        else:\\n'\n",
      " '            return inst\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _handle_fail(link, reason, url, level=1, meth=None):\\n'\n",
      " '        if meth is None:\\n'\n",
      " '            meth = logger.debug\\n'\n",
      " '\\n'\n",
      " '        meth(\"Could not fetch URL %s: %s - skipping\", link, reason)\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _get_content_type(url, session):\\n'\n",
      " '        \"\"\"Get the Content-Type of the given url, using a HEAD request\"\"\"\\n'\n",
      " '        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)\\n'\n",
      " \"        if scheme not in ('http', 'https'):\\n\"\n",
      " '            # FIXME: some warning or something?\\n'\n",
      " '            # assertion error?\\n'\n",
      " \"            return ''\\n\"\n",
      " '\\n'\n",
      " '        resp = session.head(url, allow_redirects=True)\\n'\n",
      " '        resp.raise_for_status()\\n'\n",
      " '\\n'\n",
      " '        return resp.headers.get(\"Content-Type\", \"\")\\n'\n",
      " '\\n'\n",
      " '    @cached_property\\n'\n",
      " '    def api_version(self):\\n'\n",
      " '        metas = [\\n'\n",
      " '            x for x in self.parsed.findall(\".//meta\")\\n'\n",
      " '            if x.get(\"name\", \"\").lower() == \"api-version\"\\n'\n",
      " '        ]\\n'\n",
      " '        if metas:\\n'\n",
      " '            try:\\n'\n",
      " '                return int(metas[0].get(\"value\", None))\\n'\n",
      " '            except (TypeError, ValueError):\\n'\n",
      " '                pass\\n'\n",
      " '\\n'\n",
      " '        return None\\n'\n",
      " '\\n'\n",
      " '    @cached_property\\n'\n",
      " '    def base_url(self):\\n'\n",
      " '        bases = [\\n'\n",
      " '            x for x in self.parsed.findall(\".//base\")\\n'\n",
      " '            if x.get(\"href\") is not None\\n'\n",
      " '        ]\\n'\n",
      " '        if bases and bases[0].get(\"href\"):\\n'\n",
      " '            return bases[0].get(\"href\")\\n'\n",
      " '        else:\\n'\n",
      " '            return self.url\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def links(self):\\n'\n",
      " '        \"\"\"Yields all links in the page\"\"\"\\n'\n",
      " '        for anchor in self.parsed.findall(\".//a\"):\\n'\n",
      " '            if anchor.get(\"href\"):\\n'\n",
      " '                href = anchor.get(\"href\")\\n'\n",
      " '                url = self.clean_link(\\n'\n",
      " '                    urllib_parse.urljoin(self.base_url, href)\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '                # Determine if this link is internal. If that distinction\\n'\n",
      " \"                #   doesn't make sense in this context, then we don't make\\n\"\n",
      " '                #   any distinction.\\n'\n",
      " '                internal = None\\n'\n",
      " '                if self.api_version and self.api_version >= 2:\\n'\n",
      " '                    # Only api_versions >= 2 have a distinction between\\n'\n",
      " '                    #   external and internal links\\n'\n",
      " '                    internal = bool(\\n'\n",
      " '                        anchor.get(\"rel\") and\\n'\n",
      " '                        \"internal\" in anchor.get(\"rel\").split()\\n'\n",
      " '                    )\\n'\n",
      " '\\n'\n",
      " '                yield Link(url, self, internal=internal)\\n'\n",
      " '\\n'\n",
      " \"    def rel_links(self, rels=('homepage', 'download')):\\n\"\n",
      " '        \"\"\"Yields all links with the given relations\"\"\"\\n'\n",
      " '        rels = set(rels)\\n'\n",
      " '\\n'\n",
      " '        for anchor in self.parsed.findall(\".//a\"):\\n'\n",
      " '            if anchor.get(\"rel\") and anchor.get(\"href\"):\\n'\n",
      " '                found_rels = set(anchor.get(\"rel\").split())\\n'\n",
      " '                # Determine the intersection between what rels were found '\n",
      " 'and\\n'\n",
      " '                #   what rels were being looked for\\n'\n",
      " '                if found_rels & rels:\\n'\n",
      " '                    href = anchor.get(\"href\")\\n'\n",
      " '                    url = self.clean_link(\\n'\n",
      " '                        urllib_parse.urljoin(self.base_url, href)\\n'\n",
      " '                    )\\n'\n",
      " '                    yield Link(url, self, trusted=False)\\n'\n",
      " '\\n'\n",
      " \"    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\\\\\\\|-]', re.I)\\n\"\n",
      " '\\n'\n",
      " '    def clean_link(self, url):\\n'\n",
      " '        \"\"\"Makes sure a link is fully encoded.  That is, if a \\' \\' shows up '\n",
      " 'in\\n'\n",
      " '        the link, it will be rewritten to %20 (while not over-quoting\\n'\n",
      " '        % or other characters).\"\"\"\\n'\n",
      " '        return self._clean_re.sub(\\n'\n",
      " \"            lambda match: '%%%2x' % ord(match.group(0)), url)\\n\"\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class Link(object):\\n'\n",
      " '\\n'\n",
      " '    def __init__(self, url, comes_from=None, internal=None, trusted=None):\\n'\n",
      " '\\n'\n",
      " '        # url can be a UNC windows share\\n'\n",
      " \"        if url != Inf and url.startswith('\\\\\\\\\\\\\\\\'):\\n\"\n",
      " '            url = path_to_url(url)\\n'\n",
      " '\\n'\n",
      " '        self.url = url\\n'\n",
      " '        self.comes_from = comes_from\\n'\n",
      " '        self.internal = internal\\n'\n",
      " '        self.trusted = trusted\\n'\n",
      " '\\n'\n",
      " '    def __str__(self):\\n'\n",
      " '        if self.comes_from:\\n'\n",
      " \"            return '%s (from %s)' % (self.url, self.comes_from)\\n\"\n",
      " '        else:\\n'\n",
      " '            return str(self.url)\\n'\n",
      " '\\n'\n",
      " '    def __repr__(self):\\n'\n",
      " \"        return '<Link %s>' % self\\n\"\n",
      " '\\n'\n",
      " '    def __eq__(self, other):\\n'\n",
      " '        if not isinstance(other, Link):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '        return self.url == other.url\\n'\n",
      " '\\n'\n",
      " '    def __ne__(self, other):\\n'\n",
      " '        if not isinstance(other, Link):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '        return self.url != other.url\\n'\n",
      " '\\n'\n",
      " '    def __lt__(self, other):\\n'\n",
      " '        if not isinstance(other, Link):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '        return self.url < other.url\\n'\n",
      " '\\n'\n",
      " '    def __le__(self, other):\\n'\n",
      " '        if not isinstance(other, Link):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '        return self.url <= other.url\\n'\n",
      " '\\n'\n",
      " '    def __gt__(self, other):\\n'\n",
      " '        if not isinstance(other, Link):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '        return self.url > other.url\\n'\n",
      " '\\n'\n",
      " '    def __ge__(self, other):\\n'\n",
      " '        if not isinstance(other, Link):\\n'\n",
      " '            return NotImplemented\\n'\n",
      " '        return self.url >= other.url\\n'\n",
      " '\\n'\n",
      " '    def __hash__(self):\\n'\n",
      " '        return hash(self.url)\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def filename(self):\\n'\n",
      " '        _, netloc, path, _, _ = urllib_parse.urlsplit(self.url)\\n'\n",
      " \"        name = posixpath.basename(path.rstrip('/')) or netloc\\n\"\n",
      " '        name = urllib_parse.unquote(name)\\n'\n",
      " \"        assert name, ('URL %r produced no filename' % self.url)\\n\"\n",
      " '        return name\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def scheme(self):\\n'\n",
      " '        return urllib_parse.urlsplit(self.url)[0]\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def netloc(self):\\n'\n",
      " '        return urllib_parse.urlsplit(self.url)[1]\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def path(self):\\n'\n",
      " '        return urllib_parse.unquote(urllib_parse.urlsplit(self.url)[2])\\n'\n",
      " '\\n'\n",
      " '    def splitext(self):\\n'\n",
      " \"        return splitext(posixpath.basename(self.path.rstrip('/')))\\n\"\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def ext(self):\\n'\n",
      " '        return self.splitext()[1]\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def url_without_fragment(self):\\n'\n",
      " '        scheme, netloc, path, query, fragment = '\n",
      " 'urllib_parse.urlsplit(self.url)\\n'\n",
      " '        return urllib_parse.urlunsplit((scheme, netloc, path, query, None))\\n'\n",
      " '\\n'\n",
      " \"    _egg_fragment_re = re.compile(r'#egg=([^&]*)')\\n\"\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def egg_fragment(self):\\n'\n",
      " '        match = self._egg_fragment_re.search(self.url)\\n'\n",
      " '        if not match:\\n'\n",
      " '            return None\\n'\n",
      " '        return match.group(1)\\n'\n",
      " '\\n'\n",
      " '    _hash_re = re.compile(\\n'\n",
      " \"        r'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)'\\n\"\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def hash(self):\\n'\n",
      " '        match = self._hash_re.search(self.url)\\n'\n",
      " '        if match:\\n'\n",
      " '            return match.group(2)\\n'\n",
      " '        return None\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def hash_name(self):\\n'\n",
      " '        match = self._hash_re.search(self.url)\\n'\n",
      " '        if match:\\n'\n",
      " '            return match.group(1)\\n'\n",
      " '        return None\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def show_url(self):\\n'\n",
      " \"        return posixpath.basename(self.url.split('#', 1)[0].split('?', \"\n",
      " '1)[0])\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def verifiable(self):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Returns True if this link can be verified after download, False if '\n",
      " 'it\\n'\n",
      " '        cannot, and None if we cannot determine.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        trusted = self.trusted or getattr(self.comes_from, \"trusted\", None)\\n'\n",
      " '        if trusted is not None and trusted:\\n'\n",
      " '            # This link came from a trusted source. It *may* be verifiable '\n",
      " 'but\\n'\n",
      " '            #   first we need to see if this page is operating under the '\n",
      " 'new\\n'\n",
      " '            #   API version.\\n'\n",
      " '            try:\\n'\n",
      " '                api_version = getattr(self.comes_from, \"api_version\", None)\\n'\n",
      " '                api_version = int(api_version)\\n'\n",
      " '            except (ValueError, TypeError):\\n'\n",
      " '                api_version = None\\n'\n",
      " '\\n'\n",
      " '            if api_version is None or api_version <= 1:\\n'\n",
      " '                # This link is either trusted, or it came from a trusted,\\n'\n",
      " '                #   however it is not operating under the API version 2 so\\n'\n",
      " \"                #   we can't make any claims about if it's safe or not\\n\"\n",
      " '                return\\n'\n",
      " '\\n'\n",
      " '            if self.hash:\\n'\n",
      " '                # This link came from a trusted source and it has a hash, so '\n",
      " 'we\\n'\n",
      " '                #   can consider it safe.\\n'\n",
      " '                return True\\n'\n",
      " '            else:\\n'\n",
      " '                # This link came from a trusted source, using the new API\\n'\n",
      " '                #   version, and it does not have a hash. It is NOT '\n",
      " 'verifiable\\n'\n",
      " '                return False\\n'\n",
      " '        elif trusted is not None:\\n'\n",
      " '            # This link came from an untrusted source and we cannot trust '\n",
      " 'it\\n'\n",
      " '            return False\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def is_wheel(self):\\n'\n",
      " '        return self.ext == wheel_ext\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def is_artifact(self):\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Determines if this points to an actual artifact (e.g. a tarball) or '\n",
      " 'if\\n'\n",
      " '        it points to an \"abstract\" thing like a path or a VCS location.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        from pip.vcs import vcs\\n'\n",
      " '\\n'\n",
      " '        if self.scheme in vcs.all_schemes:\\n'\n",
      " '            return False\\n'\n",
      " '\\n'\n",
      " '        return True\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '# An object to represent the \"link\" for the installed version of a '\n",
      " 'requirement.\\n'\n",
      " '# Using Inf as the url makes it sort higher.\\n'\n",
      " 'INSTALLED_VERSION = Link(Inf)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"FormatControl = namedtuple('FormatControl', 'no_binary only_binary')\\n\"\n",
      " '\"\"\"This object has two fields, no_binary and only_binary.\\n'\n",
      " '\\n'\n",
      " \"If a field is falsy, it isn't set. If it is {':all:'}, it should match all\\n\"\n",
      " 'packages except those listed in the other field. Only one field can be set\\n'\n",
      " \"to {':all:'} at a time. The rest of the time exact package name matches\\n\"\n",
      " 'are listed, with any given package only showing up in one field at a time.\\n'\n",
      " '\"\"\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'def fmt_ctl_handle_mutual_exclude(value, target, other):\\n'\n",
      " \"    new = value.split(',')\\n\"\n",
      " \"    while ':all:' in new:\\n\"\n",
      " '        other.clear()\\n'\n",
      " '        target.clear()\\n'\n",
      " \"        target.add(':all:')\\n\"\n",
      " \"        del new[:new.index(':all:') + 1]\\n\"\n",
      " \"        if ':none:' not in new:\\n\"\n",
      " '            # Without a none, we want to discard everything as :all: covers '\n",
      " 'it\\n'\n",
      " '            return\\n'\n",
      " '    for name in new:\\n'\n",
      " \"        if name == ':none:':\\n\"\n",
      " '            target.clear()\\n'\n",
      " '            continue\\n'\n",
      " '        name = pkg_resources.safe_name(name).lower()\\n'\n",
      " '        other.discard(name)\\n'\n",
      " '        target.add(name)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'def fmt_ctl_formats(fmt_ctl, canonical_name):\\n'\n",
      " '    result = set([\"binary\", \"source\"])\\n'\n",
      " '    if canonical_name in fmt_ctl.only_binary:\\n'\n",
      " \"        result.discard('source')\\n\"\n",
      " '    elif canonical_name in fmt_ctl.no_binary:\\n'\n",
      " \"        result.discard('binary')\\n\"\n",
      " \"    elif ':all:' in fmt_ctl.only_binary:\\n\"\n",
      " \"        result.discard('source')\\n\"\n",
      " \"    elif ':all:' in fmt_ctl.no_binary:\\n\"\n",
      " \"        result.discard('binary')\\n\"\n",
      " '    return frozenset(result)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'def fmt_ctl_no_binary(fmt_ctl):\\n'\n",
      " '    fmt_ctl_handle_mutual_exclude(\\n'\n",
      " \"        ':all:', fmt_ctl.no_binary, fmt_ctl.only_binary)\\n\"\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'def fmt_ctl_no_use_wheel(fmt_ctl):\\n'\n",
      " '    fmt_ctl_no_binary(fmt_ctl)\\n'\n",
      " '    warnings.warn(\\n'\n",
      " \"        '--no-use-wheel is deprecated and will be removed in the future. '\\n\"\n",
      " \"        ' Please use --no-binary :all: instead.', DeprecationWarning,\\n\"\n",
      " '        stacklevel=2)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"Search = namedtuple('Search', 'supplied canonical formats')\\n\"\n",
      " '\"\"\"Capture key aspects of a search.\\n'\n",
      " '\\n'\n",
      " ':attribute supplied: The user supplied package.\\n'\n",
      " ':attribute canonical: The canonical package name.\\n'\n",
      " ':attribute formats: The formats allowed for this package. Should be a set\\n'\n",
      " \"    with 'binary' or 'source' or both in it.\\n\"\n",
      " '\"\"\"\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(python_example[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7460d44-4169-4a31-a8fe-4edd52be408a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m preprocessing.preprocess \\\n",
    "./data/test_dataset \\\n",
    "--lang1 java \\\n",
    "--lang2 python \\\n",
    "--lang3 cpp \\\n",
    "--keep_comments True \\\n",
    "--bpe_train_size 0 \\\n",
    "--test_size 10 \\\n",
    "--local True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b610c6-4b28-4ce2-b29e-34ab185729cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_code = \"\"\"\n",
    "\n",
    "// C++ program to demonstrate\n",
    "// example of sum() function.\n",
    "  \n",
    "#include <bits/stdc++.h>\n",
    "using namespace std;\n",
    "  \n",
    "int main()\n",
    "{\n",
    "  \n",
    "    // Initializing valarray\n",
    "    valarray<int> varr = { 15, 10, 30, 33, 40 };\n",
    "  \n",
    "    // Displaying sum of valarray\n",
    "    cout << \"The sum of valarray is = \"\n",
    "         << varr.sum() << endl;\n",
    "  \n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "open(\"input_code.cpp\", \"w\").write(input_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8405e8f3-7372-49ef-9ba3-335668b41fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "@ include ( \"bits/stdc+.h\" ) public static void main ( String [ ] args ) {\n",
      "  Valr varr = new Valr ( 15 , 10 , 30 , 33 , 40 ) ;\n",
      "  System . out . println ( \"The sum of valr is = \" + varr . sum ( ) ) ;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang cpp --tgt_lang java --model_path model_2.pth < input_code.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4ed3cda-96ee-4e94-84f0-c27e8fb87970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "# include < bits / stdc + + . h > using namespace std;\n",
      "valarray < int > varr = {\n",
      "  15, 10, 30, 33, 40 };\n",
      "  cout << \"The sum of valarray is = \" << varr . sum ( ) << endl;\n",
      "  return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang cpp --tgt_lang cpp --model_path model_2.pth < input_code.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6b9632-3fcc-4155-9f62-e440ab227d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "@ SuppressWarnings ( \"unused\" ) public static void main ( String [ ] args ) {\n",
      "  Valr varr = new Valr ( 15 , 10 , 30 , 33 , 40 ) ;\n",
      "  System . out . println ( \"The sum of valr is = \" + varr . sum ( ) ) ;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang cpp --tgt_lang java --model_path model_1.pth < input_code.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215ceed1-246e-45c3-9e87-463dc6f068c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "def test_valarray ( ) :\n",
      "    varr = valarray ( [ 15 , 10 , 30 , 33 , 40 ] )\n",
      "    assert varr.sum ( ) == 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang cpp --tgt_lang python --model_path model_2.pth < input_code.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770eee5a-2e89-45b1-b49f-e524ca7948e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "def test_sum ( ) :\n",
      "    varr = valarray ( [ 15 , 10 , 30 , 33 , 40 ] )\n",
      "    assert varr.sum ( ) == 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang cpp --tgt_lang python --model_path model_1.pth < input_code.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd99df12-f5c9-417f-95e3-8cf5f9800e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_python = \"\"\"\n",
    "# Program to display the Fibonacci sequence up to n-th term\n",
    "\n",
    "nterms = int(input(\"How many terms? \"))\n",
    "\n",
    "# first two terms\n",
    "n1, n2 = 0, 1\n",
    "count = 0\n",
    "\n",
    "# check if the number of terms is valid\n",
    "if nterms <= 0:\n",
    "   print(\"Please enter a positive integer\")\n",
    "# if there is only one term, return n1\n",
    "elif nterms == 1:\n",
    "   print(\"Fibonacci sequence upto\",nterms,\":\")\n",
    "   print(n1)\n",
    "# generate fibonacci sequence\n",
    "else:\n",
    "   print(\"Fibonacci sequence:\")\n",
    "   while count < nterms:\n",
    "       print(n1)\n",
    "       nth = n1 + n2\n",
    "       # update values\n",
    "       n1 = n2\n",
    "       n2 = nth\n",
    "       count += 1\n",
    "\"\"\"\n",
    "open(\"input_python.py\", \"w\").write(input_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6b464f-d7ef-4a82-9b38-82b183489f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "int main ( ) {\n",
      "  int nterms;\n",
      "  int n1 = 0, n2 = 1;\n",
      "  int count = 0;\n",
      "  if ( nterms <= 0 ) cout << \"Please enter a positive integer\" << endl;\n",
      "  else if ( nterms == 1 ) {\n",
      "    cout << \"Fibonacci sequence upto\" << nterms << \":\" << endl;\n",
      "    cout << n1 << endl;\n",
      "  }\n",
      "  else {\n",
      "    cout << \"Fibonacci sequence:\" << endl;\n",
      "    while ( count < nterms ) {\n",
      "      cout << n1 << endl;\n",
      "      int nth = n1 + n2;\n",
      "      n1 = n2;\n",
      "      n2 = nth;\n",
      "      count ++;\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang python --tgt_lang cpp --model_path model_2.pth < input_python.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610ae980-ed52-47cc-8869-3905c2ddf23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_neural_network = \"\"\"\n",
    "#Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#dataset import\n",
    "dataset = pd.read_csv(data/train.csv) #You need to change #directory accordingly\n",
    "dataset.head(10) #Return 10 rows of data\n",
    "\n",
    "#Changing pandas dataframe to numpy array\n",
    "X = dataset.iloc[:,:20].values\n",
    "y = dataset.iloc[:,20:21].values\n",
    "\n",
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()\n",
    "\n",
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=20, activation=relu))\n",
    "model.add(Dense(12, activation=relu))\n",
    "model.add(Dense(4, activation=softmax))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "#Converting predictions to label\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "#Converting one hot encoded test label to label\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(pred,test)\n",
    "print('Accuracy is:', a*100)\n",
    "\"\"\"\n",
    "\n",
    "open(\"input_neural_network.py\", \"w\").write(input_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3e4fc0a-374f-45dc-9d1b-1065cb42e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "// Copyright (c) 2015 Roshan <thisisroshansmail@gmail.com>\n",
      "// Distributed under the Boost Software License, Version 1.0\n",
      "// See accompanying file LICENSE_1_0.txt or copy at\n",
      "// http://www.boost.org/LICENSE_1_0.txt\n",
      "// See http://boostorg.github.com/compute for more information.\n",
      "int main ( int argc, char * argv [ ] ) {\n",
      "  using namespace boost :: compute;\n",
      "  using namespace boost :: chrono;\n",
      "  dataset dataset = dataset :: read_csv (  data / train . csv  );\n",
      "  dataset . head ( 10 );\n",
      "  auto X = dataset . iloc [ 0 ] . first;\n",
      "  auto y = dataset . iloc [ 0 ] . second;\n",
      "  using namespace boost :: compute;\n",
      "  using namespace boost :: chrono;\n",
      "  auto sc = sc :: sc;\n",
      "  X = sc :: fit_transform ( X );\n",
      "  using namespace boost :: compute;\n",
      "  using namespace boost :: chrono;\n",
      "  auto ohe = sc :: ohe;\n",
      "  y = ohe ( y ) . first;\n",
      "  boost :: shared_ptr < boost :: shared_ptr < int > > ptr = ptr ( );\n",
      "  boost :: shared_ptr < int > test = ptr ( ) . second;\n",
      "  boost :: shared_ptr < int > a = ptr ( );\n",
      "  a = a ( );\n",
      "  std :: vector < int > history;\n",
      "  history . reserve ( 100 );\n",
      "  for ( int i = 0;\n",
      "  i < history . size ( );\n",
      "  ++ i ) {\n",
      "    history . push_back ( boost :: get < int > ( i ) );\n",
      "  }\n",
      "  std :: vector < int > pred;\n",
      "  for ( int i = 0;\n",
      "  i < history . size ( );\n",
      "  ++ i ) {\n",
      "    pred . push_back ( history [ i ] );\n",
      "  }\n",
      "  std :: vector < int > test;\n",
      "  for ( int i = 0;\n",
      "  i < history . size ( );\n",
      "  ++ i ) {\n",
      "    test . push_back ( history [ i ] );\n",
      "  }\n",
      "  using namespace boost :: compute;\n",
      "  double a = accuracy_score ( pred, test );\n",
      "  std :: cout << \"Accuracy is:\" << a * 100 << std :: endl;\n",
      "  return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang python --tgt_lang cpp --model_path model_2.pth < input_neural_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8379bfa7-2782-4ef5-acb8-cdf999395409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from /home/calogero/Documents/DeepLearningItalia/nlp-hands-on-2/TransCoder/data/BPE_with_comments_codes ...\n",
      "Read 50000 codes from the codes file.\n",
      "====================\n",
      "public static void main ( String [ ] args ) {\n",
      "  final SparkSession spark = SparkSession . builder ( ) . appName ( \"JavaNaiveBayesExample\" ) . getOrCreate ( ) ;\n",
      "  final JavaSparkContext jsc = new JavaSparkContext ( spark . sparkContext ( ) ) ;\n",
      "  Dataset < Row > dataset = spark . read ( ) . textFile (  data / train . txt   ) ;\n",
      "  dataset . head ( 10 ) ;\n",
      "  Dataset < Row > dataset = dataset . iloc [ 0 ] . select ( \" age \" ) . from ( \" data \" ) ;\n",
      "  final JavaRDD < Row > sc = dataset . map ( new Features2Dataset ( ) ) ;\n",
      "  final JavaRDD < Row > rdd = sc . parallelize ( Arrays . asList ( new Row ( 16 , 20 , \" Hello SPACETOKEN World ! \" ) ) ) ;\n",
      "  final JavaRDD < Row > ohe = rdd . map ( new Features2Dataset ( ) ) ;\n",
      "  final JavaRDD < Row > y = ohe . map ( new Features2Dataset ( ) ) . collect ( ) ;\n",
      "  final JavaRDD < Row > javaRDD = javaRDD . map ( new Features2Dataset ( ) ) ;\n",
      "  final JavaRDD < Row > result = javaRDD . filter ( new Predicate < Row > ( ) {\n",
      "    @ Override public boolean apply ( Row row ) {\n",
      "      return row . equals ( y . get ( 0 ) ) ;\n",
      "    }\n",
      "  }\n",
      "  ) ;\n",
      "  final int history = result . count ( ) ;\n",
      "  final int len = result . count ( ) ;\n",
      "  final List < Integer > pred = Lists . newArrayList ( ) ;\n",
      "  for ( int i = 0 ;\n",
      "  i < len ;\n",
      "  i ++ ) {\n",
      "    pred . add ( Integer . valueOf ( i ) ) ;\n",
      "  }\n",
      "  final List < Integer > test = Lists . newArrayList ( ) ;\n",
      "  for ( int i = 0 ;\n",
      "  i < len ;\n",
      "  i ++ ) {\n",
      "    test . add ( Integer . valueOf ( i ) ) ;\n",
      "  }\n",
      "  final double a = result . mean ( ) ;\n",
      "  System . out . println ( \" Accuracy SPACETOKEN is : \" + a * 100 ) ;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate.py --src_lang python --tgt_lang java --model_path model_2.pth < input_neural_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343436dd-7afd-40ef-845a-5f56aa5421a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
