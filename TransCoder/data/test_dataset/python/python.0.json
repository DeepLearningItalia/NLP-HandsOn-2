{"repo_name":"coronary/RandomEpisode","ref":"refs/heads/master","path":"depends/Lib/encodings/cp1006.py","content":"\"\"\" Python Character Mapping Codec cp1006 generated from 'MAPPINGS/VENDORS/MISC/CP1006.TXT' with gencodec.py.\n\n\"\"\"#\"\n\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    def encode(self,input,errors='strict'):\n        return codecs.charmap_encode(input,errors,encoding_table)\n\n    def decode(self,input,errors='strict'):\n        return codecs.charmap_decode(input,errors,decoding_table)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='cp1006',\n        encode=Codec().encode,\n        decode=Codec().decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n\n\n### Decoding Table\n\ndecoding_table = (\n    '\\x00'     #  0x00 -\u003e NULL\n    '\\x01'     #  0x01 -\u003e START OF HEADING\n    '\\x02'     #  0x02 -\u003e START OF TEXT\n    '\\x03'     #  0x03 -\u003e END OF TEXT\n    '\\x04'     #  0x04 -\u003e END OF TRANSMISSION\n    '\\x05'     #  0x05 -\u003e ENQUIRY\n    '\\x06'     #  0x06 -\u003e ACKNOWLEDGE\n    '\\x07'     #  0x07 -\u003e BELL\n    '\\x08'     #  0x08 -\u003e BACKSPACE\n    '\\t'       #  0x09 -\u003e HORIZONTAL TABULATION\n    '\\n'       #  0x0A -\u003e LINE FEED\n    '\\x0b'     #  0x0B -\u003e VERTICAL TABULATION\n    '\\x0c'     #  0x0C -\u003e FORM FEED\n    '\\r'       #  0x0D -\u003e CARRIAGE RETURN\n    '\\x0e'     #  0x0E -\u003e SHIFT OUT\n    '\\x0f'     #  0x0F -\u003e SHIFT IN\n    '\\x10'     #  0x10 -\u003e DATA LINK ESCAPE\n    '\\x11'     #  0x11 -\u003e DEVICE CONTROL ONE\n    '\\x12'     #  0x12 -\u003e DEVICE CONTROL TWO\n    '\\x13'     #  0x13 -\u003e DEVICE CONTROL THREE\n    '\\x14'     #  0x14 -\u003e DEVICE CONTROL FOUR\n    '\\x15'     #  0x15 -\u003e NEGATIVE ACKNOWLEDGE\n    '\\x16'     #  0x16 -\u003e SYNCHRONOUS IDLE\n    '\\x17'     #  0x17 -\u003e END OF TRANSMISSION BLOCK\n    '\\x18'     #  0x18 -\u003e CANCEL\n    '\\x19'     #  0x19 -\u003e END OF MEDIUM\n    '\\x1a'     #  0x1A -\u003e SUBSTITUTE\n    '\\x1b'     #  0x1B -\u003e ESCAPE\n    '\\x1c'     #  0x1C -\u003e FILE SEPARATOR\n    '\\x1d'     #  0x1D -\u003e GROUP SEPARATOR\n    '\\x1e'     #  0x1E -\u003e RECORD SEPARATOR\n    '\\x1f'     #  0x1F -\u003e UNIT SEPARATOR\n    ' '        #  0x20 -\u003e SPACE\n    '!'        #  0x21 -\u003e EXCLAMATION MARK\n    '\"'        #  0x22 -\u003e QUOTATION MARK\n    '#'        #  0x23 -\u003e NUMBER SIGN\n    '$'        #  0x24 -\u003e DOLLAR SIGN\n    '%'        #  0x25 -\u003e PERCENT SIGN\n    '\u0026'        #  0x26 -\u003e AMPERSAND\n    \"'\"        #  0x27 -\u003e APOSTROPHE\n    '('        #  0x28 -\u003e LEFT PARENTHESIS\n    ')'        #  0x29 -\u003e RIGHT PARENTHESIS\n    '*'        #  0x2A -\u003e ASTERISK\n    '+'        #  0x2B -\u003e PLUS SIGN\n    ','        #  0x2C -\u003e COMMA\n    '-'        #  0x2D -\u003e HYPHEN-MINUS\n    '.'        #  0x2E -\u003e FULL STOP\n    '/'        #  0x2F -\u003e SOLIDUS\n    '0'        #  0x30 -\u003e DIGIT ZERO\n    '1'        #  0x31 -\u003e DIGIT ONE\n    '2'        #  0x32 -\u003e DIGIT TWO\n    '3'        #  0x33 -\u003e DIGIT THREE\n    '4'        #  0x34 -\u003e DIGIT FOUR\n    '5'        #  0x35 -\u003e DIGIT FIVE\n    '6'        #  0x36 -\u003e DIGIT SIX\n    '7'        #  0x37 -\u003e DIGIT SEVEN\n    '8'        #  0x38 -\u003e DIGIT EIGHT\n    '9'        #  0x39 -\u003e DIGIT NINE\n    ':'        #  0x3A -\u003e COLON\n    ';'        #  0x3B -\u003e SEMICOLON\n    '\u003c'        #  0x3C -\u003e LESS-THAN SIGN\n    '='        #  0x3D -\u003e EQUALS SIGN\n    '\u003e'        #  0x3E -\u003e GREATER-THAN SIGN\n    '?'        #  0x3F -\u003e QUESTION MARK\n    '@'        #  0x40 -\u003e COMMERCIAL AT\n    'A'        #  0x41 -\u003e LATIN CAPITAL LETTER A\n    'B'        #  0x42 -\u003e LATIN CAPITAL LETTER B\n    'C'        #  0x43 -\u003e LATIN CAPITAL LETTER C\n    'D'        #  0x44 -\u003e LATIN CAPITAL LETTER D\n    'E'        #  0x45 -\u003e LATIN CAPITAL LETTER E\n    'F'        #  0x46 -\u003e LATIN CAPITAL LETTER F\n    'G'        #  0x47 -\u003e LATIN CAPITAL LETTER G\n    'H'        #  0x48 -\u003e LATIN CAPITAL LETTER H\n    'I'        #  0x49 -\u003e LATIN CAPITAL LETTER I\n    'J'        #  0x4A -\u003e LATIN CAPITAL LETTER J\n    'K'        #  0x4B -\u003e LATIN CAPITAL LETTER K\n    'L'        #  0x4C -\u003e LATIN CAPITAL LETTER L\n    'M'        #  0x4D -\u003e LATIN CAPITAL LETTER M\n    'N'        #  0x4E -\u003e LATIN CAPITAL LETTER N\n    'O'        #  0x4F -\u003e LATIN CAPITAL LETTER O\n    'P'        #  0x50 -\u003e LATIN CAPITAL LETTER P\n    'Q'        #  0x51 -\u003e LATIN CAPITAL LETTER Q\n    'R'        #  0x52 -\u003e LATIN CAPITAL LETTER R\n    'S'        #  0x53 -\u003e LATIN CAPITAL LETTER S\n    'T'        #  0x54 -\u003e LATIN CAPITAL LETTER T\n    'U'        #  0x55 -\u003e LATIN CAPITAL LETTER U\n    'V'        #  0x56 -\u003e LATIN CAPITAL LETTER V\n    'W'        #  0x57 -\u003e LATIN CAPITAL LETTER W\n    'X'        #  0x58 -\u003e LATIN CAPITAL LETTER X\n    'Y'        #  0x59 -\u003e LATIN CAPITAL LETTER Y\n    'Z'        #  0x5A -\u003e LATIN CAPITAL LETTER Z\n    '['        #  0x5B -\u003e LEFT SQUARE BRACKET\n    '\\\\'       #  0x5C -\u003e REVERSE SOLIDUS\n    ']'        #  0x5D -\u003e RIGHT SQUARE BRACKET\n    '^'        #  0x5E -\u003e CIRCUMFLEX ACCENT\n    '_'        #  0x5F -\u003e LOW LINE\n    '`'        #  0x60 -\u003e GRAVE ACCENT\n    'a'        #  0x61 -\u003e LATIN SMALL LETTER A\n    'b'        #  0x62 -\u003e LATIN SMALL LETTER B\n    'c'        #  0x63 -\u003e LATIN SMALL LETTER C\n    'd'        #  0x64 -\u003e LATIN SMALL LETTER D\n    'e'        #  0x65 -\u003e LATIN SMALL LETTER E\n    'f'        #  0x66 -\u003e LATIN SMALL LETTER F\n    'g'        #  0x67 -\u003e LATIN SMALL LETTER G\n    'h'        #  0x68 -\u003e LATIN SMALL LETTER H\n    'i'        #  0x69 -\u003e LATIN SMALL LETTER I\n    'j'        #  0x6A -\u003e LATIN SMALL LETTER J\n    'k'        #  0x6B -\u003e LATIN SMALL LETTER K\n    'l'        #  0x6C -\u003e LATIN SMALL LETTER L\n    'm'        #  0x6D -\u003e LATIN SMALL LETTER M\n    'n'        #  0x6E -\u003e LATIN SMALL LETTER N\n    'o'        #  0x6F -\u003e LATIN SMALL LETTER O\n    'p'        #  0x70 -\u003e LATIN SMALL LETTER P\n    'q'        #  0x71 -\u003e LATIN SMALL LETTER Q\n    'r'        #  0x72 -\u003e LATIN SMALL LETTER R\n    's'        #  0x73 -\u003e LATIN SMALL LETTER S\n    't'        #  0x74 -\u003e LATIN SMALL LETTER T\n    'u'        #  0x75 -\u003e LATIN SMALL LETTER U\n    'v'        #  0x76 -\u003e LATIN SMALL LETTER V\n    'w'        #  0x77 -\u003e LATIN SMALL LETTER W\n    'x'        #  0x78 -\u003e LATIN SMALL LETTER X\n    'y'        #  0x79 -\u003e LATIN SMALL LETTER Y\n    'z'        #  0x7A -\u003e LATIN SMALL LETTER Z\n    '{'        #  0x7B -\u003e LEFT CURLY BRACKET\n    '|'        #  0x7C -\u003e VERTICAL LINE\n    '}'        #  0x7D -\u003e RIGHT CURLY BRACKET\n    '~'        #  0x7E -\u003e TILDE\n    '\\x7f'     #  0x7F -\u003e DELETE\n    '\\x80'     #  0x80 -\u003e \u003ccontrol\u003e\n    '\\x81'     #  0x81 -\u003e \u003ccontrol\u003e\n    '\\x82'     #  0x82 -\u003e \u003ccontrol\u003e\n    '\\x83'     #  0x83 -\u003e \u003ccontrol\u003e\n    '\\x84'     #  0x84 -\u003e \u003ccontrol\u003e\n    '\\x85'     #  0x85 -\u003e \u003ccontrol\u003e\n    '\\x86'     #  0x86 -\u003e \u003ccontrol\u003e\n    '\\x87'     #  0x87 -\u003e \u003ccontrol\u003e\n    '\\x88'     #  0x88 -\u003e \u003ccontrol\u003e\n    '\\x89'     #  0x89 -\u003e \u003ccontrol\u003e\n    '\\x8a'     #  0x8A -\u003e \u003ccontrol\u003e\n    '\\x8b'     #  0x8B -\u003e \u003ccontrol\u003e\n    '\\x8c'     #  0x8C -\u003e \u003ccontrol\u003e\n    '\\x8d'     #  0x8D -\u003e \u003ccontrol\u003e\n    '\\x8e'     #  0x8E -\u003e \u003ccontrol\u003e\n    '\\x8f'     #  0x8F -\u003e \u003ccontrol\u003e\n    '\\x90'     #  0x90 -\u003e \u003ccontrol\u003e\n    '\\x91'     #  0x91 -\u003e \u003ccontrol\u003e\n    '\\x92'     #  0x92 -\u003e \u003ccontrol\u003e\n    '\\x93'     #  0x93 -\u003e \u003ccontrol\u003e\n    '\\x94'     #  0x94 -\u003e \u003ccontrol\u003e\n    '\\x95'     #  0x95 -\u003e \u003ccontrol\u003e\n    '\\x96'     #  0x96 -\u003e \u003ccontrol\u003e\n    '\\x97'     #  0x97 -\u003e \u003ccontrol\u003e\n    '\\x98'     #  0x98 -\u003e \u003ccontrol\u003e\n    '\\x99'     #  0x99 -\u003e \u003ccontrol\u003e\n    '\\x9a'     #  0x9A -\u003e \u003ccontrol\u003e\n    '\\x9b'     #  0x9B -\u003e \u003ccontrol\u003e\n    '\\x9c'     #  0x9C -\u003e \u003ccontrol\u003e\n    '\\x9d'     #  0x9D -\u003e \u003ccontrol\u003e\n    '\\x9e'     #  0x9E -\u003e \u003ccontrol\u003e\n    '\\x9f'     #  0x9F -\u003e \u003ccontrol\u003e\n    '\\xa0'     #  0xA0 -\u003e NO-BREAK SPACE\n    '\\u06f0'   #  0xA1 -\u003e EXTENDED ARABIC-INDIC DIGIT ZERO\n    '\\u06f1'   #  0xA2 -\u003e EXTENDED ARABIC-INDIC DIGIT ONE\n    '\\u06f2'   #  0xA3 -\u003e EXTENDED ARABIC-INDIC DIGIT TWO\n    '\\u06f3'   #  0xA4 -\u003e EXTENDED ARABIC-INDIC DIGIT THREE\n    '\\u06f4'   #  0xA5 -\u003e EXTENDED ARABIC-INDIC DIGIT FOUR\n    '\\u06f5'   #  0xA6 -\u003e EXTENDED ARABIC-INDIC DIGIT FIVE\n    '\\u06f6'   #  0xA7 -\u003e EXTENDED ARABIC-INDIC DIGIT SIX\n    '\\u06f7'   #  0xA8 -\u003e EXTENDED ARABIC-INDIC DIGIT SEVEN\n    '\\u06f8'   #  0xA9 -\u003e EXTENDED ARABIC-INDIC DIGIT EIGHT\n    '\\u06f9'   #  0xAA -\u003e EXTENDED ARABIC-INDIC DIGIT NINE\n    '\\u060c'   #  0xAB -\u003e ARABIC COMMA\n    '\\u061b'   #  0xAC -\u003e ARABIC SEMICOLON\n    '\\xad'     #  0xAD -\u003e SOFT HYPHEN\n    '\\u061f'   #  0xAE -\u003e ARABIC QUESTION MARK\n    '\\ufe81'   #  0xAF -\u003e ARABIC LETTER ALEF WITH MADDA ABOVE ISOLATED FORM\n    '\\ufe8d'   #  0xB0 -\u003e ARABIC LETTER ALEF ISOLATED FORM\n    '\\ufe8e'   #  0xB1 -\u003e ARABIC LETTER ALEF FINAL FORM\n    '\\ufe8e'   #  0xB2 -\u003e ARABIC LETTER ALEF FINAL FORM\n    '\\ufe8f'   #  0xB3 -\u003e ARABIC LETTER BEH ISOLATED FORM\n    '\\ufe91'   #  0xB4 -\u003e ARABIC LETTER BEH INITIAL FORM\n    '\\ufb56'   #  0xB5 -\u003e ARABIC LETTER PEH ISOLATED FORM\n    '\\ufb58'   #  0xB6 -\u003e ARABIC LETTER PEH INITIAL FORM\n    '\\ufe93'   #  0xB7 -\u003e ARABIC LETTER TEH MARBUTA ISOLATED FORM\n    '\\ufe95'   #  0xB8 -\u003e ARABIC LETTER TEH ISOLATED FORM\n    '\\ufe97'   #  0xB9 -\u003e ARABIC LETTER TEH INITIAL FORM\n    '\\ufb66'   #  0xBA -\u003e ARABIC LETTER TTEH ISOLATED FORM\n    '\\ufb68'   #  0xBB -\u003e ARABIC LETTER TTEH INITIAL FORM\n    '\\ufe99'   #  0xBC -\u003e ARABIC LETTER THEH ISOLATED FORM\n    '\\ufe9b'   #  0xBD -\u003e ARABIC LETTER THEH INITIAL FORM\n    '\\ufe9d'   #  0xBE -\u003e ARABIC LETTER JEEM ISOLATED FORM\n    '\\ufe9f'   #  0xBF -\u003e ARABIC LETTER JEEM INITIAL FORM\n    '\\ufb7a'   #  0xC0 -\u003e ARABIC LETTER TCHEH ISOLATED FORM\n    '\\ufb7c'   #  0xC1 -\u003e ARABIC LETTER TCHEH INITIAL FORM\n    '\\ufea1'   #  0xC2 -\u003e ARABIC LETTER HAH ISOLATED FORM\n    '\\ufea3'   #  0xC3 -\u003e ARABIC LETTER HAH INITIAL FORM\n    '\\ufea5'   #  0xC4 -\u003e ARABIC LETTER KHAH ISOLATED FORM\n    '\\ufea7'   #  0xC5 -\u003e ARABIC LETTER KHAH INITIAL FORM\n    '\\ufea9'   #  0xC6 -\u003e ARABIC LETTER DAL ISOLATED FORM\n    '\\ufb84'   #  0xC7 -\u003e ARABIC LETTER DAHAL ISOLATED FORMN\n    '\\ufeab'   #  0xC8 -\u003e ARABIC LETTER THAL ISOLATED FORM\n    '\\ufead'   #  0xC9 -\u003e ARABIC LETTER REH ISOLATED FORM\n    '\\ufb8c'   #  0xCA -\u003e ARABIC LETTER RREH ISOLATED FORM\n    '\\ufeaf'   #  0xCB -\u003e ARABIC LETTER ZAIN ISOLATED FORM\n    '\\ufb8a'   #  0xCC -\u003e ARABIC LETTER JEH ISOLATED FORM\n    '\\ufeb1'   #  0xCD -\u003e ARABIC LETTER SEEN ISOLATED FORM\n    '\\ufeb3'   #  0xCE -\u003e ARABIC LETTER SEEN INITIAL FORM\n    '\\ufeb5'   #  0xCF -\u003e ARABIC LETTER SHEEN ISOLATED FORM\n    '\\ufeb7'   #  0xD0 -\u003e ARABIC LETTER SHEEN INITIAL FORM\n    '\\ufeb9'   #  0xD1 -\u003e ARABIC LETTER SAD ISOLATED FORM\n    '\\ufebb'   #  0xD2 -\u003e ARABIC LETTER SAD INITIAL FORM\n    '\\ufebd'   #  0xD3 -\u003e ARABIC LETTER DAD ISOLATED FORM\n    '\\ufebf'   #  0xD4 -\u003e ARABIC LETTER DAD INITIAL FORM\n    '\\ufec1'   #  0xD5 -\u003e ARABIC LETTER TAH ISOLATED FORM\n    '\\ufec5'   #  0xD6 -\u003e ARABIC LETTER ZAH ISOLATED FORM\n    '\\ufec9'   #  0xD7 -\u003e ARABIC LETTER AIN ISOLATED FORM\n    '\\ufeca'   #  0xD8 -\u003e ARABIC LETTER AIN FINAL FORM\n    '\\ufecb'   #  0xD9 -\u003e ARABIC LETTER AIN INITIAL FORM\n    '\\ufecc'   #  0xDA -\u003e ARABIC LETTER AIN MEDIAL FORM\n    '\\ufecd'   #  0xDB -\u003e ARABIC LETTER GHAIN ISOLATED FORM\n    '\\ufece'   #  0xDC -\u003e ARABIC LETTER GHAIN FINAL FORM\n    '\\ufecf'   #  0xDD -\u003e ARABIC LETTER GHAIN INITIAL FORM\n    '\\ufed0'   #  0xDE -\u003e ARABIC LETTER GHAIN MEDIAL FORM\n    '\\ufed1'   #  0xDF -\u003e ARABIC LETTER FEH ISOLATED FORM\n    '\\ufed3'   #  0xE0 -\u003e ARABIC LETTER FEH INITIAL FORM\n    '\\ufed5'   #  0xE1 -\u003e ARABIC LETTER QAF ISOLATED FORM\n    '\\ufed7'   #  0xE2 -\u003e ARABIC LETTER QAF INITIAL FORM\n    '\\ufed9'   #  0xE3 -\u003e ARABIC LETTER KAF ISOLATED FORM\n    '\\ufedb'   #  0xE4 -\u003e ARABIC LETTER KAF INITIAL FORM\n    '\\ufb92'   #  0xE5 -\u003e ARABIC LETTER GAF ISOLATED FORM\n    '\\ufb94'   #  0xE6 -\u003e ARABIC LETTER GAF INITIAL FORM\n    '\\ufedd'   #  0xE7 -\u003e ARABIC LETTER LAM ISOLATED FORM\n    '\\ufedf'   #  0xE8 -\u003e ARABIC LETTER LAM INITIAL FORM\n    '\\ufee0'   #  0xE9 -\u003e ARABIC LETTER LAM MEDIAL FORM\n    '\\ufee1'   #  0xEA -\u003e ARABIC LETTER MEEM ISOLATED FORM\n    '\\ufee3'   #  0xEB -\u003e ARABIC LETTER MEEM INITIAL FORM\n    '\\ufb9e'   #  0xEC -\u003e ARABIC LETTER NOON GHUNNA ISOLATED FORM\n    '\\ufee5'   #  0xED -\u003e ARABIC LETTER NOON ISOLATED FORM\n    '\\ufee7'   #  0xEE -\u003e ARABIC LETTER NOON INITIAL FORM\n    '\\ufe85'   #  0xEF -\u003e ARABIC LETTER WAW WITH HAMZA ABOVE ISOLATED FORM\n    '\\ufeed'   #  0xF0 -\u003e ARABIC LETTER WAW ISOLATED FORM\n    '\\ufba6'   #  0xF1 -\u003e ARABIC LETTER HEH GOAL ISOLATED FORM\n    '\\ufba8'   #  0xF2 -\u003e ARABIC LETTER HEH GOAL INITIAL FORM\n    '\\ufba9'   #  0xF3 -\u003e ARABIC LETTER HEH GOAL MEDIAL FORM\n    '\\ufbaa'   #  0xF4 -\u003e ARABIC LETTER HEH DOACHASHMEE ISOLATED FORM\n    '\\ufe80'   #  0xF5 -\u003e ARABIC LETTER HAMZA ISOLATED FORM\n    '\\ufe89'   #  0xF6 -\u003e ARABIC LETTER YEH WITH HAMZA ABOVE ISOLATED FORM\n    '\\ufe8a'   #  0xF7 -\u003e ARABIC LETTER YEH WITH HAMZA ABOVE FINAL FORM\n    '\\ufe8b'   #  0xF8 -\u003e ARABIC LETTER YEH WITH HAMZA ABOVE INITIAL FORM\n    '\\ufef1'   #  0xF9 -\u003e ARABIC LETTER YEH ISOLATED FORM\n    '\\ufef2'   #  0xFA -\u003e ARABIC LETTER YEH FINAL FORM\n    '\\ufef3'   #  0xFB -\u003e ARABIC LETTER YEH INITIAL FORM\n    '\\ufbb0'   #  0xFC -\u003e ARABIC LETTER YEH BARREE WITH HAMZA ABOVE ISOLATED FORM\n    '\\ufbae'   #  0xFD -\u003e ARABIC LETTER YEH BARREE ISOLATED FORM\n    '\\ufe7c'   #  0xFE -\u003e ARABIC SHADDA ISOLATED FORM\n    '\\ufe7d'   #  0xFF -\u003e ARABIC SHADDA MEDIAL FORM\n)\n\n### Encoding table\nencoding_table=codecs.charmap_build(decoding_table)\n"}
{"repo_name":"fernandog/Medusa","ref":"refs/heads/optimized","path":"ext/click/termui.py","content":"import os\nimport sys\nimport struct\n\nfrom ._compat import raw_input, text_type, string_types, \\\n     isatty, strip_ansi, get_winterm_size, DEFAULT_COLUMNS, WIN\nfrom .utils import echo\nfrom .exceptions import Abort, UsageError\nfrom .types import convert_type\nfrom .globals import resolve_color_default\n\n\n# The prompt functions to use.  The doc tools currently override these\n# functions to customize how they work.\nvisible_prompt_func = raw_input\n\n_ansi_colors = ('black', 'red', 'green', 'yellow', 'blue', 'magenta',\n                'cyan', 'white', 'reset')\n_ansi_reset_all = '\\033[0m'\n\n\ndef hidden_prompt_func(prompt):\n    import getpass\n    return getpass.getpass(prompt)\n\n\ndef _build_prompt(text, suffix, show_default=False, default=None):\n    prompt = text\n    if default is not None and show_default:\n        prompt = '%s [%s]' % (prompt, default)\n    return prompt + suffix\n\n\ndef prompt(text, default=None, hide_input=False,\n           confirmation_prompt=False, type=None,\n           value_proc=None, prompt_suffix=': ',\n           show_default=True, err=False):\n    \"\"\"Prompts a user for input.  This is a convenience function that can\n    be used to prompt a user for input later.\n\n    If the user aborts the input by sending a interrupt signal, this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 6.0\n       Added unicode support for cmd.exe on Windows.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the text to show for the prompt.\n    :param default: the default value to use if no input happens.  If this\n                    is not given it will prompt until it's aborted.\n    :param hide_input: if this is set to true then the input value will\n                       be hidden.\n    :param confirmation_prompt: asks for confirmation for the value.\n    :param type: the type to use to check the value against.\n    :param value_proc: if this parameter is provided it's a function that\n                       is invoked instead of the type conversion to\n                       convert a value.\n    :param prompt_suffix: a suffix that should be added to the prompt.\n    :param show_default: shows or hides the default value in the prompt.\n    :param err: if set to true the file defaults to ``stderr`` instead of\n                ``stdout``, the same as with echo.\n    \"\"\"\n    result = None\n\n    def prompt_func(text):\n        f = hide_input and hidden_prompt_func or visible_prompt_func\n        try:\n            # Write the prompt separately so that we get nice\n            # coloring through colorama on Windows\n            echo(text, nl=False, err=err)\n            return f('')\n        except (KeyboardInterrupt, EOFError):\n            # getpass doesn't print a newline if the user aborts input with ^C.\n            # Allegedly this behavior is inherited from getpass(3).\n            # A doc bug has been filed at https://bugs.python.org/issue24711\n            if hide_input:\n                echo(None, err=err)\n            raise Abort()\n\n    if value_proc is None:\n        value_proc = convert_type(type, default)\n\n    prompt = _build_prompt(text, prompt_suffix, show_default, default)\n\n    while 1:\n        while 1:\n            value = prompt_func(prompt)\n            if value:\n                break\n            # If a default is set and used, then the confirmation\n            # prompt is always skipped because that's the only thing\n            # that really makes sense.\n            elif default is not None:\n                return default\n        try:\n            result = value_proc(value)\n        except UsageError as e:\n            echo('Error: %s' % e.message, err=err)\n            continue\n        if not confirmation_prompt:\n            return result\n        while 1:\n            value2 = prompt_func('Repeat for confirmation: ')\n            if value2:\n                break\n        if value == value2:\n            return result\n        echo('Error: the two entered values do not match', err=err)\n\n\ndef confirm(text, default=False, abort=False, prompt_suffix=': ',\n            show_default=True, err=False):\n    \"\"\"Prompts for confirmation (yes/no question).\n\n    If the user aborts the input by sending a interrupt signal this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the question to ask.\n    :param default: the default for the prompt.\n    :param abort: if this is set to `True` a negative answer aborts the\n                  exception by raising :exc:`Abort`.\n    :param prompt_suffix: a suffix that should be added to the prompt.\n    :param show_default: shows or hides the default value in the prompt.\n    :param err: if set to true the file defaults to ``stderr`` instead of\n                ``stdout``, the same as with echo.\n    \"\"\"\n    prompt = _build_prompt(text, prompt_suffix, show_default,\n                           default and 'Y/n' or 'y/N')\n    while 1:\n        try:\n            # Write the prompt separately so that we get nice\n            # coloring through colorama on Windows\n            echo(prompt, nl=False, err=err)\n            value = visible_prompt_func('').lower().strip()\n        except (KeyboardInterrupt, EOFError):\n            raise Abort()\n        if value in ('y', 'yes'):\n            rv = True\n        elif value in ('n', 'no'):\n            rv = False\n        elif value == '':\n            rv = default\n        else:\n            echo('Error: invalid input', err=err)\n            continue\n        break\n    if abort and not rv:\n        raise Abort()\n    return rv\n\n\ndef get_terminal_size():\n    \"\"\"Returns the current size of the terminal as tuple in the form\n    ``(width, height)`` in columns and rows.\n    \"\"\"\n    # If shutil has get_terminal_size() (Python 3.3 and later) use that\n    if sys.version_info \u003e= (3, 3):\n        import shutil\n        shutil_get_terminal_size = getattr(shutil, 'get_terminal_size', None)\n        if shutil_get_terminal_size:\n            sz = shutil_get_terminal_size()\n            return sz.columns, sz.lines\n\n    if get_winterm_size is not None:\n        return get_winterm_size()\n\n    def ioctl_gwinsz(fd):\n        try:\n            import fcntl\n            import termios\n            cr = struct.unpack(\n                'hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))\n        except Exception:\n            return\n        return cr\n\n    cr = ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)\n    if not cr:\n        try:\n            fd = os.open(os.ctermid(), os.O_RDONLY)\n            try:\n                cr = ioctl_gwinsz(fd)\n            finally:\n                os.close(fd)\n        except Exception:\n            pass\n    if not cr or not cr[0] or not cr[1]:\n        cr = (os.environ.get('LINES', 25),\n              os.environ.get('COLUMNS', DEFAULT_COLUMNS))\n    return int(cr[1]), int(cr[0])\n\n\ndef echo_via_pager(text, color=None):\n    \"\"\"This function takes a text and shows it via an environment specific\n    pager on stdout.\n\n    .. versionchanged:: 3.0\n       Added the `color` flag.\n\n    :param text: the text to page.\n    :param color: controls if the pager supports ANSI colors or not.  The\n                  default is autodetection.\n    \"\"\"\n    color = resolve_color_default(color)\n    if not isinstance(text, string_types):\n        text = text_type(text)\n    from ._termui_impl import pager\n    return pager(text + '\\n', color)\n\n\ndef progressbar(iterable=None, length=None, label=None, show_eta=True,\n                show_percent=None, show_pos=False,\n                item_show_func=None, fill_char='#', empty_char='-',\n                bar_template='%(label)s  [%(bar)s]  %(info)s',\n                info_sep='  ', width=36, file=None, color=None):\n    \"\"\"This function creates an iterable context manager that can be used\n    to iterate over something while showing a progress bar.  It will\n    either iterate over the `iterable` or `length` items (that are counted\n    up).  While iteration happens, this function will print a rendered\n    progress bar to the given `file` (defaults to stdout) and will attempt\n    to calculate remaining time and more.  By default, this progress bar\n    will not be rendered if the file is not a terminal.\n\n    The context manager creates the progress bar.  When the context\n    manager is entered the progress bar is already displayed.  With every\n    iteration over the progress bar, the iterable passed to the bar is\n    advanced and the bar is updated.  When the context manager exits,\n    a newline is printed and the progress bar is finalized on screen.\n\n    No printing must happen or the progress bar will be unintentionally\n    destroyed.\n\n    Example usage::\n\n        with progressbar(items) as bar:\n            for item in bar:\n                do_something_with(item)\n\n    Alternatively, if no iterable is specified, one can manually update the\n    progress bar through the `update()` method instead of directly\n    iterating over the progress bar.  The update method accepts the number\n    of steps to increment the bar with::\n\n        with progressbar(length=chunks.total_bytes) as bar:\n            for chunk in chunks:\n                process_chunk(chunk)\n                bar.update(chunks.bytes)\n\n    .. versionadded:: 2.0\n\n    .. versionadded:: 4.0\n       Added the `color` parameter.  Added a `update` method to the\n       progressbar object.\n\n    :param iterable: an iterable to iterate over.  If not provided the length\n                     is required.\n    :param length: the number of items to iterate over.  By default the\n                   progressbar will attempt to ask the iterator about its\n                   length, which might or might not work.  If an iterable is\n                   also provided this parameter can be used to override the\n                   length.  If an iterable is not provided the progress bar\n                   will iterate over a range of that length.\n    :param label: the label to show next to the progress bar.\n    :param show_eta: enables or disables the estimated time display.  This is\n                     automatically disabled if the length cannot be\n                     determined.\n    :param show_percent: enables or disables the percentage display.  The\n                         default is `True` if the iterable has a length or\n                         `False` if not.\n    :param show_pos: enables or disables the absolute position display.  The\n                     default is `False`.\n    :param item_show_func: a function called with the current item which\n                           can return a string to show the current item\n                           next to the progress bar.  Note that the current\n                           item can be `None`!\n    :param fill_char: the character to use to show the filled part of the\n                      progress bar.\n    :param empty_char: the character to use to show the non-filled part of\n                       the progress bar.\n    :param bar_template: the format string to use as template for the bar.\n                         The parameters in it are ``label`` for the label,\n                         ``bar`` for the progress bar and ``info`` for the\n                         info section.\n    :param info_sep: the separator between multiple info items (eta etc.)\n    :param width: the width of the progress bar in characters, 0 means full\n                  terminal width\n    :param file: the file to write to.  If this is not a terminal then\n                 only the label is printed.\n    :param color: controls if the terminal supports ANSI colors or not.  The\n                  default is autodetection.  This is only needed if ANSI\n                  codes are included anywhere in the progress bar output\n                  which is not the case by default.\n    \"\"\"\n    from ._termui_impl import ProgressBar\n    color = resolve_color_default(color)\n    return ProgressBar(iterable=iterable, length=length, show_eta=show_eta,\n                       show_percent=show_percent, show_pos=show_pos,\n                       item_show_func=item_show_func, fill_char=fill_char,\n                       empty_char=empty_char, bar_template=bar_template,\n                       info_sep=info_sep, file=file, label=label,\n                       width=width, color=color)\n\n\ndef clear():\n    \"\"\"Clears the terminal screen.  This will have the effect of clearing\n    the whole visible space of the terminal and moving the cursor to the\n    top left.  This does not do anything if not connected to a terminal.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    if not isatty(sys.stdout):\n        return\n    # If we're on Windows and we don't have colorama available, then we\n    # clear the screen by shelling out.  Otherwise we can use an escape\n    # sequence.\n    if WIN:\n        os.system('cls')\n    else:\n        sys.stdout.write('\\033[2J\\033[1;1H')\n\n\ndef style(text, fg=None, bg=None, bold=None, dim=None, underline=None,\n          blink=None, reverse=None, reset=True):\n    \"\"\"Styles a text with ANSI styles and returns the new string.  By\n    default the styling is self contained which means that at the end\n    of the string a reset code is issued.  This can be prevented by\n    passing ``reset=False``.\n\n    Examples::\n\n        click.echo(click.style('Hello World!', fg='green'))\n        click.echo(click.style('ATTENTION!', blink=True))\n        click.echo(click.style('Some things', reverse=True, fg='cyan'))\n\n    Supported color names:\n\n    * ``black`` (might be a gray)\n    * ``red``\n    * ``green``\n    * ``yellow`` (might be an orange)\n    * ``blue``\n    * ``magenta``\n    * ``cyan``\n    * ``white`` (might be light gray)\n    * ``reset`` (reset the color code only)\n\n    .. versionadded:: 2.0\n\n    :param text: the string to style with ansi codes.\n    :param fg: if provided this will become the foreground color.\n    :param bg: if provided this will become the background color.\n    :param bold: if provided this will enable or disable bold mode.\n    :param dim: if provided this will enable or disable dim mode.  This is\n                badly supported.\n    :param underline: if provided this will enable or disable underline.\n    :param blink: if provided this will enable or disable blinking.\n    :param reverse: if provided this will enable or disable inverse\n                    rendering (foreground becomes background and the\n                    other way round).\n    :param reset: by default a reset-all code is added at the end of the\n                  string which means that styles do not carry over.  This\n                  can be disabled to compose styles.\n    \"\"\"\n    bits = []\n    if fg:\n        try:\n            bits.append('\\033[%dm' % (_ansi_colors.index(fg) + 30))\n        except ValueError:\n            raise TypeError('Unknown color %r' % fg)\n    if bg:\n        try:\n            bits.append('\\033[%dm' % (_ansi_colors.index(bg) + 40))\n        except ValueError:\n            raise TypeError('Unknown color %r' % bg)\n    if bold is not None:\n        bits.append('\\033[%dm' % (1 if bold else 22))\n    if dim is not None:\n        bits.append('\\033[%dm' % (2 if dim else 22))\n    if underline is not None:\n        bits.append('\\033[%dm' % (4 if underline else 24))\n    if blink is not None:\n        bits.append('\\033[%dm' % (5 if blink else 25))\n    if reverse is not None:\n        bits.append('\\033[%dm' % (7 if reverse else 27))\n    bits.append(text)\n    if reset:\n        bits.append(_ansi_reset_all)\n    return ''.join(bits)\n\n\ndef unstyle(text):\n    \"\"\"Removes ANSI styling information from a string.  Usually it's not\n    necessary to use this function as Click's echo function will\n    automatically remove styling if necessary.\n\n    .. versionadded:: 2.0\n\n    :param text: the text to remove style information from.\n    \"\"\"\n    return strip_ansi(text)\n\n\ndef secho(text, file=None, nl=True, err=False, color=None, **styles):\n    \"\"\"This function combines :func:`echo` and :func:`style` into one\n    call.  As such the following two calls are the same::\n\n        click.secho('Hello World!', fg='green')\n        click.echo(click.style('Hello World!', fg='green'))\n\n    All keyword arguments are forwarded to the underlying functions\n    depending on which one they go with.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    return echo(style(text, **styles), file=file, nl=nl, err=err, color=color)\n\n\ndef edit(text=None, editor=None, env=None, require_save=True,\n         extension='.txt', filename=None):\n    r\"\"\"Edits the given text in the defined editor.  If an editor is given\n    (should be the full path to the executable but the regular operating\n    system search path is used for finding the executable) it overrides\n    the detected editor.  Optionally, some environment variables can be\n    used.  If the editor is closed without changes, `None` is returned.  In\n    case a file is edited directly the return value is always `None` and\n    `require_save` and `extension` are ignored.\n\n    If the editor cannot be opened a :exc:`UsageError` is raised.\n\n    Note for Windows: to simplify cross-platform usage, the newlines are\n    automatically converted from POSIX to Windows and vice versa.  As such,\n    the message here will have ``\\n`` as newline markers.\n\n    :param text: the text to edit.\n    :param editor: optionally the editor to use.  Defaults to automatic\n                   detection.\n    :param env: environment variables to forward to the editor.\n    :param require_save: if this is true, then not saving in the editor\n                         will make the return value become `None`.\n    :param extension: the extension to tell the editor about.  This defaults\n                      to `.txt` but changing this might change syntax\n                      highlighting.\n    :param filename: if provided it will edit this file instead of the\n                     provided text contents.  It will not use a temporary\n                     file as an indirection in that case.\n    \"\"\"\n    from ._termui_impl import Editor\n    editor = Editor(editor=editor, env=env, require_save=require_save,\n                    extension=extension)\n    if filename is None:\n        return editor.edit(text)\n    editor.edit_file(filename)\n\n\ndef launch(url, wait=False, locate=False):\n    \"\"\"This function launches the given URL (or filename) in the default\n    viewer application for this file type.  If this is an executable, it\n    might launch the executable in a new session.  The return value is\n    the exit code of the launched application.  Usually, ``0`` indicates\n    success.\n\n    Examples::\n\n        click.launch('http://click.pocoo.org/')\n        click.launch('/my/downloaded/file', locate=True)\n\n    .. versionadded:: 2.0\n\n    :param url: URL or filename of the thing to launch.\n    :param wait: waits for the program to stop.\n    :param locate: if this is set to `True` then instead of launching the\n                   application associated with the URL it will attempt to\n                   launch a file manager with the file located.  This\n                   might have weird effects if the URL does not point to\n                   the filesystem.\n    \"\"\"\n    from ._termui_impl import open_url\n    return open_url(url, wait=wait, locate=locate)\n\n\n# If this is provided, getchar() calls into this instead.  This is used\n# for unittesting purposes.\n_getchar = None\n\n\ndef getchar(echo=False):\n    \"\"\"Fetches a single character from the terminal and returns it.  This\n    will always return a unicode character and under certain rare\n    circumstances this might return more than one character.  The\n    situations which more than one character is returned is when for\n    whatever reason multiple characters end up in the terminal buffer or\n    standard input was not actually a terminal.\n\n    Note that this will always read from the terminal, even if something\n    is piped into the standard input.\n\n    .. versionadded:: 2.0\n\n    :param echo: if set to `True`, the character read will also show up on\n                 the terminal.  The default is to not show it.\n    \"\"\"\n    f = _getchar\n    if f is None:\n        from ._termui_impl import getchar as f\n    return f(echo)\n\n\ndef pause(info='Press any key to continue ...', err=False):\n    \"\"\"This command stops execution and waits for the user to press any\n    key to continue.  This is similar to the Windows batch \"pause\"\n    command.  If the program is not run through a terminal, this command\n    will instead do nothing.\n\n    .. versionadded:: 2.0\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param info: the info string to print before pausing.\n    :param err: if set to message goes to ``stderr`` instead of\n                ``stdout``, the same as with echo.\n    \"\"\"\n    if not isatty(sys.stdin) or not isatty(sys.stdout):\n        return\n    try:\n        if info:\n            echo(info, nl=False, err=err)\n        try:\n            getchar()\n        except (KeyboardInterrupt, EOFError):\n            pass\n    finally:\n        if info:\n            echo(err=err)\n"}
{"repo_name":"robertglen/flask","ref":"refs/heads/master","path":"tests/test_instance_config.py","content":"# -*- coding: utf-8 -*-\n\"\"\"\n    tests.test_instance\n    ~~~~~~~~~~~~~~~~~~~\n\n    :copyright: (c) 2015 by the Flask Team, see AUTHORS for more details.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\nimport os\nimport sys\n\nimport pytest\nimport flask\nfrom flask._compat import PY2\n\n\ndef test_explicit_instance_paths(modules_tmpdir):\n    with pytest.raises(ValueError) as excinfo:\n        flask.Flask(__name__, instance_path='instance')\n    assert 'must be absolute' in str(excinfo.value)\n\n    app = flask.Flask(__name__, instance_path=str(modules_tmpdir))\n    assert app.instance_path == str(modules_tmpdir)\n\n\ndef test_main_module_paths(modules_tmpdir, purge_module):\n    app = modules_tmpdir.join('main_app.py')\n    app.write('import flask\\n\\napp = flask.Flask(\"__main__\")')\n    purge_module('main_app')\n\n    from main_app import app\n    here = os.path.abspath(os.getcwd())\n    assert app.instance_path == os.path.join(here, 'instance')\n\n\ndef test_uninstalled_module_paths(modules_tmpdir, purge_module):\n    app = modules_tmpdir.join('config_module_app.py').write(\n        'import os\\n'\n        'import flask\\n'\n        'here = os.path.abspath(os.path.dirname(__file__))\\n'\n        'app = flask.Flask(__name__)\\n'\n    )\n    purge_module('config_module_app')\n\n    from config_module_app import app\n    assert app.instance_path == str(modules_tmpdir.join('instance'))\n\n\ndef test_uninstalled_package_paths(modules_tmpdir, purge_module):\n    app = modules_tmpdir.mkdir('config_package_app')\n    init = app.join('__init__.py')\n    init.write(\n        'import os\\n'\n        'import flask\\n'\n        'here = os.path.abspath(os.path.dirname(__file__))\\n'\n        'app = flask.Flask(__name__)\\n'\n    )\n    purge_module('config_package_app')\n\n    from config_package_app import app\n    assert app.instance_path == str(modules_tmpdir.join('instance'))\n\n\ndef test_installed_module_paths(modules_tmpdir, modules_tmpdir_prefix,\n                                purge_module, site_packages, limit_loader):\n    site_packages.join('site_app.py').write(\n        'import flask\\n'\n        'app = flask.Flask(__name__)\\n'\n    )\n    purge_module('site_app')\n\n    from site_app import app\n    assert app.instance_path == \\\n        modules_tmpdir.join('var').join('site_app-instance')\n\n\ndef test_installed_package_paths(limit_loader, modules_tmpdir,\n                                 modules_tmpdir_prefix, purge_module,\n                                 monkeypatch):\n    installed_path = modules_tmpdir.mkdir('path')\n    monkeypatch.syspath_prepend(installed_path)\n\n    app = installed_path.mkdir('installed_package')\n    init = app.join('__init__.py')\n    init.write('import flask\\napp = flask.Flask(__name__)')\n    purge_module('installed_package')\n\n    from installed_package import app\n    assert app.instance_path == \\\n        modules_tmpdir.join('var').join('installed_package-instance')\n\n\ndef test_prefix_package_paths(limit_loader, modules_tmpdir,\n                              modules_tmpdir_prefix, purge_module,\n                              site_packages):\n    app = site_packages.mkdir('site_package')\n    init = app.join('__init__.py')\n    init.write('import flask\\napp = flask.Flask(__name__)')\n    purge_module('site_package')\n\n    import site_package\n    assert site_package.app.instance_path == \\\n        modules_tmpdir.join('var').join('site_package-instance')\n\n\ndef test_egg_installed_paths(install_egg, modules_tmpdir,\n                             modules_tmpdir_prefix):\n    modules_tmpdir.mkdir('site_egg').join('__init__.py').write(\n        'import flask\\n\\napp = flask.Flask(__name__)'\n    )\n    install_egg('site_egg')\n    try:\n        import site_egg\n        assert site_egg.app.instance_path == \\\n            str(modules_tmpdir.join('var/').join('site_egg-instance'))\n    finally:\n        if 'site_egg' in sys.modules:\n            del sys.modules['site_egg']\n\n\n@pytest.mark.skipif(not PY2, reason='This only works under Python 2.')\ndef test_meta_path_loader_without_is_package(request, modules_tmpdir):\n    app = modules_tmpdir.join('unimportable.py')\n    app.write('import flask\\napp = flask.Flask(__name__)')\n\n    class Loader(object):\n        def find_module(self, name, path=None):\n            return self\n\n    sys.meta_path.append(Loader())\n    request.addfinalizer(sys.meta_path.pop)\n\n    with pytest.raises(AttributeError):\n        import unimportable\n"}
{"repo_name":"thomasgilgenast/gilgistatus-nonrel","ref":"refs/heads/master","path":"django/db/backends/creation.py","content":"import sys\nimport time\n\nfrom django.conf import settings\nfrom django.utils.datastructures import DictWrapper\n\n# The prefix to put on the default database name when creating\n# the test database.\nTEST_DATABASE_PREFIX = 'test_'\n\nclass BaseDatabaseCreation(object):\n    \"\"\"\n    This class encapsulates all backend-specific differences that pertain to\n    database *creation*, such as the column types to use for particular Django\n    Fields, the SQL used to create and destroy tables, and the creation and\n    destruction of test databases.\n    \"\"\"\n    data_types = {}\n\n    def __init__(self, connection):\n        self.connection = connection\n\n    def _digest(self, *args):\n        \"\"\"\n        Generates a 32-bit digest of a set of arguments that can be used to\n        shorten identifying names.\n        \"\"\"\n        return '%x' % (abs(hash(args)) % 4294967296L)  # 2**32\n    \n    def db_type(self, field):\n        return self._db_type(field, field.get_internal_type())\n\n    def related_db_type(self, field):\n        return self._db_type(field, field.get_related_internal_type())\n\n    def _db_type(self, field, internal_type):\n        data = DictWrapper(field.__dict__, self.connection.ops.quote_name, \"qn_\")\n        try:\n            return self.connection.creation.data_types[internal_type] % data\n        except KeyError:\n            return None\n\n    def sql_create_model(self, model, style, known_models=set()):\n        \"\"\"\n        Returns the SQL required to create a single model, as a tuple of:\n            (list_of_sql, pending_references_dict)\n        \"\"\"\n        opts = model._meta\n        if not opts.managed or opts.proxy:\n            return [], {}\n        final_output = []\n        table_output = []\n        pending_references = {}\n        qn = self.connection.ops.quote_name\n        for f in opts.local_fields:\n            col_type = f.db_type(connection=self.connection)\n            tablespace = f.db_tablespace or opts.db_tablespace\n            if col_type is None:\n                # Skip ManyToManyFields, because they're not represented as\n                # database columns in this table.\n                continue\n            # Make the definition (e.g. 'foo VARCHAR(30)') for this field.\n            field_output = [style.SQL_FIELD(qn(f.column)),\n                style.SQL_COLTYPE(col_type)]\n            if not f.null:\n                field_output.append(style.SQL_KEYWORD('NOT NULL'))\n            if f.primary_key:\n                field_output.append(style.SQL_KEYWORD('PRIMARY KEY'))\n            elif f.unique:\n                field_output.append(style.SQL_KEYWORD('UNIQUE'))\n            if tablespace and f.unique:\n                # We must specify the index tablespace inline, because we\n                # won't be generating a CREATE INDEX statement for this field.\n                field_output.append(self.connection.ops.tablespace_sql(tablespace, inline=True))\n            if f.rel:\n                ref_output, pending = self.sql_for_inline_foreign_key_references(f, known_models, style)\n                if pending:\n                    pr = pending_references.setdefault(f.rel.to, []).append((model, f))\n                else:\n                    field_output.extend(ref_output)\n            table_output.append(' '.join(field_output))\n        for field_constraints in opts.unique_together:\n            table_output.append(style.SQL_KEYWORD('UNIQUE') + ' (%s)' % \\\n                \", \".join([style.SQL_FIELD(qn(opts.get_field(f).column)) for f in field_constraints]))\n\n        full_statement = [style.SQL_KEYWORD('CREATE TABLE') + ' ' + style.SQL_TABLE(qn(opts.db_table)) + ' (']\n        for i, line in enumerate(table_output): # Combine and add commas.\n            full_statement.append('    %s%s' % (line, i \u003c len(table_output)-1 and ',' or ''))\n        full_statement.append(')')\n        if opts.db_tablespace:\n            full_statement.append(self.connection.ops.tablespace_sql(opts.db_tablespace))\n        full_statement.append(';')\n        final_output.append('\\n'.join(full_statement))\n\n        if opts.has_auto_field:\n            # Add any extra SQL needed to support auto-incrementing primary keys.\n            auto_column = opts.auto_field.db_column or opts.auto_field.name\n            autoinc_sql = self.connection.ops.autoinc_sql(opts.db_table, auto_column)\n            if autoinc_sql:\n                for stmt in autoinc_sql:\n                    final_output.append(stmt)\n\n        return final_output, pending_references\n\n    def sql_for_inline_foreign_key_references(self, field, known_models, style):\n        \"Return the SQL snippet defining the foreign key reference for a field\"\n        qn = self.connection.ops.quote_name\n        if field.rel.to in known_models:\n            output = [style.SQL_KEYWORD('REFERENCES') + ' ' + \\\n                style.SQL_TABLE(qn(field.rel.to._meta.db_table)) + ' (' + \\\n                style.SQL_FIELD(qn(field.rel.to._meta.get_field(field.rel.field_name).column)) + ')' +\n                self.connection.ops.deferrable_sql()\n            ]\n            pending = False\n        else:\n            # We haven't yet created the table to which this field\n            # is related, so save it for later.\n            output = []\n            pending = True\n\n        return output, pending\n\n    def sql_for_pending_references(self, model, style, pending_references):\n        \"Returns any ALTER TABLE statements to add constraints after the fact.\"\n        from django.db.backends.util import truncate_name\n\n        if not model._meta.managed or model._meta.proxy:\n            return []\n        qn = self.connection.ops.quote_name\n        final_output = []\n        opts = model._meta\n        if model in pending_references:\n            for rel_class, f in pending_references[model]:\n                rel_opts = rel_class._meta\n                r_table = rel_opts.db_table\n                r_col = f.column\n                table = opts.db_table\n                col = opts.get_field(f.rel.field_name).column\n                # For MySQL, r_name must be unique in the first 64 characters.\n                # So we are careful with character usage here.\n                r_name = '%s_refs_%s_%s' % (r_col, col, self._digest(r_table, table))\n                final_output.append(style.SQL_KEYWORD('ALTER TABLE') + ' %s ADD CONSTRAINT %s FOREIGN KEY (%s) REFERENCES %s (%s)%s;' % \\\n                    (qn(r_table), qn(truncate_name(r_name, self.connection.ops.max_name_length())),\n                    qn(r_col), qn(table), qn(col),\n                    self.connection.ops.deferrable_sql()))\n            del pending_references[model]\n        return final_output\n\n    def sql_for_many_to_many(self, model, style):\n        \"Return the CREATE TABLE statments for all the many-to-many tables defined on a model\"\n        import warnings\n        warnings.warn(\n            'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated',\n            DeprecationWarning\n        )\n\n        output = []\n        for f in model._meta.local_many_to_many:\n            if model._meta.managed or f.rel.to._meta.managed:\n                output.extend(self.sql_for_many_to_many_field(model, f, style))\n        return output\n\n    def sql_for_many_to_many_field(self, model, f, style):\n        \"Return the CREATE TABLE statements for a single m2m field\"\n        import warnings\n        warnings.warn(\n            'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated',\n            DeprecationWarning\n        )\n\n        from django.db import models\n        from django.db.backends.util import truncate_name\n\n        output = []\n        if f.auto_created:\n            opts = model._meta\n            qn = self.connection.ops.quote_name\n            tablespace = f.db_tablespace or opts.db_tablespace\n            if tablespace:\n                sql = self.connection.ops.tablespace_sql(tablespace, inline=True)\n                if sql:\n                    tablespace_sql = ' ' + sql\n                else:\n                    tablespace_sql = ''\n            else:\n                tablespace_sql = ''\n            table_output = [style.SQL_KEYWORD('CREATE TABLE') + ' ' + \\\n                style.SQL_TABLE(qn(f.m2m_db_table())) + ' (']\n            table_output.append('    %s %s %s%s,' %\n                (style.SQL_FIELD(qn('id')),\n                style.SQL_COLTYPE(models.AutoField(primary_key=True).db_type(connection=self.connection)),\n                style.SQL_KEYWORD('NOT NULL PRIMARY KEY'),\n                tablespace_sql))\n\n            deferred = []\n            inline_output, deferred = self.sql_for_inline_many_to_many_references(model, f, style)\n            table_output.extend(inline_output)\n\n            table_output.append('    %s (%s, %s)%s' %\n                (style.SQL_KEYWORD('UNIQUE'),\n                style.SQL_FIELD(qn(f.m2m_column_name())),\n                style.SQL_FIELD(qn(f.m2m_reverse_name())),\n                tablespace_sql))\n            table_output.append(')')\n            if opts.db_tablespace:\n                # f.db_tablespace is only for indices, so ignore its value here.\n                table_output.append(self.connection.ops.tablespace_sql(opts.db_tablespace))\n            table_output.append(';')\n            output.append('\\n'.join(table_output))\n\n            for r_table, r_col, table, col in deferred:\n                r_name = '%s_refs_%s_%s' % (r_col, col, self._digest(r_table, table))\n                output.append(style.SQL_KEYWORD('ALTER TABLE') + ' %s ADD CONSTRAINT %s FOREIGN KEY (%s) REFERENCES %s (%s)%s;' %\n                (qn(r_table),\n                qn(truncate_name(r_name, self.connection.ops.max_name_length())),\n                qn(r_col), qn(table), qn(col),\n                self.connection.ops.deferrable_sql()))\n\n            # Add any extra SQL needed to support auto-incrementing PKs\n            autoinc_sql = self.connection.ops.autoinc_sql(f.m2m_db_table(), 'id')\n            if autoinc_sql:\n                for stmt in autoinc_sql:\n                    output.append(stmt)\n        return output\n\n    def sql_for_inline_many_to_many_references(self, model, field, style):\n        \"Create the references to other tables required by a many-to-many table\"\n        import warnings\n        warnings.warn(\n            'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated',\n            DeprecationWarning\n        )\n\n        from django.db import models\n        opts = model._meta\n        qn = self.connection.ops.quote_name\n\n        table_output = [\n            '    %s %s %s %s (%s)%s,' %\n                (style.SQL_FIELD(qn(field.m2m_column_name())),\n                style.SQL_COLTYPE(models.ForeignKey(model).db_type(connection=self.connection)),\n                style.SQL_KEYWORD('NOT NULL REFERENCES'),\n                style.SQL_TABLE(qn(opts.db_table)),\n                style.SQL_FIELD(qn(opts.pk.column)),\n                self.connection.ops.deferrable_sql()),\n            '    %s %s %s %s (%s)%s,' %\n                (style.SQL_FIELD(qn(field.m2m_reverse_name())),\n                style.SQL_COLTYPE(models.ForeignKey(field.rel.to).db_type(connection=self.connection)),\n                style.SQL_KEYWORD('NOT NULL REFERENCES'),\n                style.SQL_TABLE(qn(field.rel.to._meta.db_table)),\n                style.SQL_FIELD(qn(field.rel.to._meta.pk.column)),\n                self.connection.ops.deferrable_sql())\n        ]\n        deferred = []\n\n        return table_output, deferred\n\n    def sql_indexes_for_model(self, model, style):\n        \"Returns the CREATE INDEX SQL statements for a single model\"\n        if not model._meta.managed or model._meta.proxy:\n            return []\n        output = []\n        for f in model._meta.local_fields:\n            output.extend(self.sql_indexes_for_field(model, f, style))\n        return output\n\n    def sql_indexes_for_field(self, model, f, style):\n        \"Return the CREATE INDEX SQL statements for a single model field\"\n        from django.db.backends.util import truncate_name\n\n        if f.db_index and not f.unique:\n            qn = self.connection.ops.quote_name\n            tablespace = f.db_tablespace or model._meta.db_tablespace\n            if tablespace:\n                sql = self.connection.ops.tablespace_sql(tablespace)\n                if sql:\n                    tablespace_sql = ' ' + sql\n                else:\n                    tablespace_sql = ''\n            else:\n                tablespace_sql = ''\n            i_name = '%s_%s' % (model._meta.db_table, self._digest(f.column))\n            output = [style.SQL_KEYWORD('CREATE INDEX') + ' ' +\n                style.SQL_TABLE(qn(truncate_name(i_name, self.connection.ops.max_name_length()))) + ' ' +\n                style.SQL_KEYWORD('ON') + ' ' +\n                style.SQL_TABLE(qn(model._meta.db_table)) + ' ' +\n                \"(%s)\" % style.SQL_FIELD(qn(f.column)) +\n                \"%s;\" % tablespace_sql]\n        else:\n            output = []\n        return output\n\n    def sql_destroy_model(self, model, references_to_delete, style):\n        \"Return the DROP TABLE and restraint dropping statements for a single model\"\n        if not model._meta.managed or model._meta.proxy:\n            return []\n        # Drop the table now\n        qn = self.connection.ops.quote_name\n        output = ['%s %s;' % (style.SQL_KEYWORD('DROP TABLE'),\n                              style.SQL_TABLE(qn(model._meta.db_table)))]\n        if model in references_to_delete:\n            output.extend(self.sql_remove_table_constraints(model, references_to_delete, style))\n\n        if model._meta.has_auto_field:\n            ds = self.connection.ops.drop_sequence_sql(model._meta.db_table)\n            if ds:\n                output.append(ds)\n        return output\n\n    def sql_remove_table_constraints(self, model, references_to_delete, style):\n        from django.db.backends.util import truncate_name\n\n        if not model._meta.managed or model._meta.proxy:\n            return []\n        output = []\n        qn = self.connection.ops.quote_name\n        for rel_class, f in references_to_delete[model]:\n            table = rel_class._meta.db_table\n            col = f.column\n            r_table = model._meta.db_table\n            r_col = model._meta.get_field(f.rel.field_name).column\n            r_name = '%s_refs_%s_%s' % (col, r_col, self._digest(table, r_table))\n            output.append('%s %s %s %s;' % \\\n                (style.SQL_KEYWORD('ALTER TABLE'),\n                style.SQL_TABLE(qn(table)),\n                style.SQL_KEYWORD(self.connection.ops.drop_foreignkey_sql()),\n                style.SQL_FIELD(qn(truncate_name(r_name, self.connection.ops.max_name_length())))))\n        del references_to_delete[model]\n        return output\n\n    def sql_destroy_many_to_many(self, model, f, style):\n        \"Returns the DROP TABLE statements for a single m2m field\"\n        import warnings\n        warnings.warn(\n            'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated',\n            DeprecationWarning\n        )\n\n        qn = self.connection.ops.quote_name\n        output = []\n        if f.auto_created:\n            output.append(\"%s %s;\" % (style.SQL_KEYWORD('DROP TABLE'),\n                style.SQL_TABLE(qn(f.m2m_db_table()))))\n            ds = self.connection.ops.drop_sequence_sql(\"%s_%s\" % (model._meta.db_table, f.column))\n            if ds:\n                output.append(ds)\n        return output\n\n    def create_test_db(self, verbosity=1, autoclobber=False):\n        \"\"\"\n        Creates a test database, prompting the user for confirmation if the\n        database already exists. Returns the name of the test database created.\n        \"\"\"\n        # Don't import django.core.management if it isn't needed.\n        from django.core.management import call_command\n\n        test_database_name = self._get_test_db_name()\n\n        if verbosity \u003e= 1:\n            test_db_repr = ''\n            if verbosity \u003e= 2:\n                test_db_repr = \" ('%s')\" % test_database_name\n            print \"Creating test database for alias '%s'%s...\" % (self.connection.alias, test_db_repr)\n\n        self._create_test_db(verbosity, autoclobber)\n\n        self.connection.close()\n        self.connection.settings_dict[\"NAME\"] = test_database_name\n\n        # Confirm the feature set of the test database\n        self.connection.features.confirm()\n\n        # Report syncdb messages at one level lower than that requested.\n        # This ensures we don't get flooded with messages during testing\n        # (unless you really ask to be flooded)\n        call_command('syncdb',\n            verbosity=max(verbosity - 1, 0),\n            interactive=False,\n            database=self.connection.alias,\n            load_initial_data=False)\n\n        # We need to then do a flush to ensure that any data installed by\n        # custom SQL has been removed. The only test data should come from\n        # test fixtures, or autogenerated from post_syncdb triggers.\n        # This has the side effect of loading initial data (which was\n        # intentionally skipped in the syncdb).\n        call_command('flush',\n            verbosity=max(verbosity - 1, 0),\n            interactive=False,\n            database=self.connection.alias)\n\n        from django.core.cache import get_cache\n        from django.core.cache.backends.db import BaseDatabaseCache\n        for cache_alias in settings.CACHES:\n            cache = get_cache(cache_alias)\n            if isinstance(cache, BaseDatabaseCache):\n                from django.db import router\n                if router.allow_syncdb(self.connection.alias, cache.cache_model_class):\n                    call_command('createcachetable', cache._table, database=self.connection.alias)\n\n        # Get a cursor (even though we don't need one yet). This has\n        # the side effect of initializing the test database.\n        cursor = self.connection.cursor()\n\n        return test_database_name\n\n    def _get_test_db_name(self):\n        \"\"\"\n        Internal implementation - returns the name of the test DB that will be\n        created. Only useful when called from create_test_db() and\n        _create_test_db() and when no external munging is done with the 'NAME'\n        or 'TEST_NAME' settings.\n        \"\"\"\n        if self.connection.settings_dict['TEST_NAME']:\n            return self.connection.settings_dict['TEST_NAME']\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict['NAME']\n\n    def _create_test_db(self, verbosity, autoclobber):\n        \"Internal implementation - creates the test db tables.\"\n        suffix = self.sql_table_creation_suffix()\n\n        test_database_name = self._get_test_db_name()\n\n        qn = self.connection.ops.quote_name\n\n        # Create the test database and connect to it. We need to autocommit\n        # if the database supports it because PostgreSQL doesn't allow\n        # CREATE/DROP DATABASE statements within transactions.\n        cursor = self.connection.cursor()\n        self.set_autocommit()\n        try:\n            cursor.execute(\"CREATE DATABASE %s %s\" % (qn(test_database_name), suffix))\n        except Exception, e:\n            sys.stderr.write(\"Got an error creating the test database: %s\\n\" % e)\n            if not autoclobber:\n                confirm = raw_input(\"Type 'yes' if you would like to try deleting the test database '%s', or 'no' to cancel: \" % test_database_name)\n            if autoclobber or confirm == 'yes':\n                try:\n                    if verbosity \u003e= 1:\n                        print \"Destroying old test database '%s'...\" % self.connection.alias\n                    cursor.execute(\"DROP DATABASE %s\" % qn(test_database_name))\n                    cursor.execute(\"CREATE DATABASE %s %s\" % (qn(test_database_name), suffix))\n                except Exception, e:\n                    sys.stderr.write(\"Got an error recreating the test database: %s\\n\" % e)\n                    sys.exit(2)\n            else:\n                print \"Tests cancelled.\"\n                sys.exit(1)\n\n        return test_database_name\n\n    def destroy_test_db(self, old_database_name, verbosity=1):\n        \"\"\"\n        Destroy a test database, prompting the user for confirmation if the\n        database already exists. Returns the name of the test database created.\n        \"\"\"\n        self.connection.close()\n        test_database_name = self.connection.settings_dict['NAME']\n        if verbosity \u003e= 1:\n            test_db_repr = ''\n            if verbosity \u003e= 2:\n                test_db_repr = \" ('%s')\" % test_database_name\n            print \"Destroying test database for alias '%s'%s...\" % (self.connection.alias, test_db_repr)\n        self.connection.settings_dict['NAME'] = old_database_name\n\n        self._destroy_test_db(test_database_name, verbosity)\n\n    def _destroy_test_db(self, test_database_name, verbosity):\n        \"Internal implementation - remove the test db tables.\"\n        # Remove the test database to clean up after\n        # ourselves. Connect to the previous database (not the test database)\n        # to do so, because it's not allowed to delete a database while being\n        # connected to it.\n        cursor = self.connection.cursor()\n        self.set_autocommit()\n        time.sleep(1) # To avoid \"database is being accessed by other users\" errors.\n        cursor.execute(\"DROP DATABASE %s\" % self.connection.ops.quote_name(test_database_name))\n        self.connection.close()\n\n    def set_autocommit(self):\n        \"Make sure a connection is in autocommit mode.\"\n        if hasattr(self.connection.connection, \"autocommit\"):\n            if callable(self.connection.connection.autocommit):\n                self.connection.connection.autocommit(True)\n            else:\n                self.connection.connection.autocommit = True\n        elif hasattr(self.connection.connection, \"set_isolation_level\"):\n            self.connection.connection.set_isolation_level(0)\n\n    def sql_table_creation_suffix(self):\n        \"SQL to append to the end of the test table creation statements\"\n        return ''\n\n    def test_db_signature(self):\n        \"\"\"\n        Returns a tuple with elements of self.connection.settings_dict (a\n        DATABASES setting value) that uniquely identify a database\n        accordingly to the RDBMS particularities.\n        \"\"\"\n        settings_dict = self.connection.settings_dict\n        return (\n            settings_dict['HOST'],\n            settings_dict['PORT'],\n            settings_dict['ENGINE'],\n            settings_dict['NAME']\n        )\n"}
{"repo_name":"moijes12/oh-mainline","ref":"refs/heads/master","path":"vendor/packages/sphinx/tests/test_intersphinx.py","content":"# -*- coding: utf-8 -*-\n\"\"\"\n    test_intersphinx\n    ~~~~~~~~~~~~~~~~\n\n    Test the intersphinx extension.\n\n    :copyright: Copyright 2007-2013 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport zlib\nimport posixpath\ntry:\n    from io import BytesIO\nexcept ImportError:\n    from cStringIO import StringIO as BytesIO\n\nfrom docutils import nodes\n\nfrom sphinx import addnodes\nfrom sphinx.ext.intersphinx import read_inventory_v1, read_inventory_v2, \\\n     load_mappings, missing_reference\n\nfrom util import with_app, with_tempdir, write_file\n\n\ninventory_v1 = '''\\\n# Sphinx inventory version 1\n# Project: foo\n# Version: 1.0\nmodule mod foo.html\nmodule.cls class foo.html\n'''.encode('utf-8')\n\ninventory_v2 = '''\\\n# Sphinx inventory version 2\n# Project: foo\n# Version: 2.0\n# The remainder of this file is compressed with zlib.\n'''.encode('utf-8') + zlib.compress('''\\\nmodule1 py:module 0 foo.html#module-module1 Long Module desc\nmodule2 py:module 0 foo.html#module-$ -\nmodule1.func py:function 1 sub/foo.html#$ -\nCFunc c:function 2 cfunc.html#CFunc -\na term std:term -1 glossary.html#term-a-term -\n'''.encode('utf-8'))\n\n\ndef test_read_inventory_v1():\n    f = BytesIO(inventory_v1)\n    f.readline()\n    invdata = read_inventory_v1(f, '/util', posixpath.join)\n    assert invdata['py:module']['module'] == \\\n           ('foo', '1.0', '/util/foo.html#module-module', '-')\n    assert invdata['py:class']['module.cls'] == \\\n           ('foo', '1.0', '/util/foo.html#module.cls', '-')\n\n\ndef test_read_inventory_v2():\n    f = BytesIO(inventory_v2)\n    f.readline()\n    invdata1 = read_inventory_v2(f, '/util', posixpath.join)\n\n    # try again with a small buffer size to test the chunking algorithm\n    f = BytesIO(inventory_v2)\n    f.readline()\n    invdata2 = read_inventory_v2(f, '/util', posixpath.join, bufsize=5)\n\n    assert invdata1 == invdata2\n\n    assert len(invdata1['py:module']) == 2\n    assert invdata1['py:module']['module1'] == \\\n           ('foo', '2.0', '/util/foo.html#module-module1', 'Long Module desc')\n    assert invdata1['py:module']['module2'] == \\\n           ('foo', '2.0', '/util/foo.html#module-module2', '-')\n    assert invdata1['py:function']['module1.func'][2] == \\\n           '/util/sub/foo.html#module1.func'\n    assert invdata1['c:function']['CFunc'][2] == '/util/cfunc.html#CFunc'\n    assert invdata1['std:term']['a term'][2] == \\\n           '/util/glossary.html#term-a-term'\n\n\n@with_app(confoverrides={'extensions': 'sphinx.ext.intersphinx'})\n@with_tempdir\ndef test_missing_reference(tempdir, app):\n    inv_file = tempdir / 'inventory'\n    write_file(inv_file, inventory_v2)\n    app.config.intersphinx_mapping = {\n        'http://docs.python.org/': inv_file,\n        'py3k': ('http://docs.python.org/py3k/', inv_file),\n    }\n    app.config.intersphinx_cache_limit = 0\n\n    # load the inventory and check if it's done correctly\n    load_mappings(app)\n    inv = app.env.intersphinx_inventory\n\n    assert inv['py:module']['module2'] == \\\n           ('foo', '2.0', 'http://docs.python.org/foo.html#module-module2', '-')\n\n    # create fake nodes and check referencing\n\n    def fake_node(domain, type, target, content, **attrs):\n        contnode = nodes.emphasis(content, content)\n        node = addnodes.pending_xref('')\n        node['reftarget'] = target\n        node['reftype'] = type\n        node['refdomain'] = domain\n        node.attributes.update(attrs)\n        node += contnode\n        return node, contnode\n\n    def reference_check(*args, **kwds):\n        node, contnode = fake_node(*args, **kwds)\n        return missing_reference(app, app.env, node, contnode)\n\n    # check resolution when a target is found\n    rn = reference_check('py', 'func', 'module1.func', 'foo')\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'http://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'foo'\n\n    # create unresolvable nodes and check None return value\n    assert reference_check('py', 'foo', 'module1.func', 'foo') is None\n    assert reference_check('py', 'func', 'foo', 'foo') is None\n    assert reference_check('py', 'func', 'foo', 'foo') is None\n\n    # check handling of prefixes\n\n    # prefix given, target found: prefix is stripped\n    rn = reference_check('py', 'mod', 'py3k:module2', 'py3k:module2')\n    assert rn[0].astext() == 'module2'\n\n    # prefix given, but not in title: nothing stripped\n    rn = reference_check('py', 'mod', 'py3k:module2', 'module2')\n    assert rn[0].astext() == 'module2'\n\n    # prefix given, but explicit: nothing stripped\n    rn = reference_check('py', 'mod', 'py3k:module2', 'py3k:module2',\n                         refexplicit=True)\n    assert rn[0].astext() == 'py3k:module2'\n\n    # prefix given, target not found and nonexplicit title: prefix is stripped\n    node, contnode = fake_node('py', 'mod', 'py3k:unknown', 'py3k:unknown',\n                               refexplicit=False)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn is None\n    assert contnode[0].astext() == 'unknown'\n\n    # prefix given, target not found and explicit title: nothing is changed\n    node, contnode = fake_node('py', 'mod', 'py3k:unknown', 'py3k:unknown',\n                               refexplicit=True)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn is None\n    assert contnode[0].astext() == 'py3k:unknown'\n\n\n@with_app(confoverrides={'extensions': 'sphinx.ext.intersphinx'})\n@with_tempdir\ndef test_load_mappings_warnings(tempdir, app):\n    \"\"\"\n    load_mappings issues a warning if new-style mapping\n    identifiers are not alphanumeric\n    \"\"\"\n    inv_file = tempdir / 'inventory'\n    write_file(inv_file, inventory_v2)\n    app.config.intersphinx_mapping = {\n        'http://docs.python.org/': inv_file,\n        'py3k': ('http://docs.python.org/py3k/', inv_file),\n        'repoze.workflow': ('http://docs.repoze.org/workflow/', inv_file),\n        'django-taggit': ('http://django-taggit.readthedocs.org/en/latest/',\n                          inv_file)\n    }\n\n    app.config.intersphinx_cache_limit = 0\n    # load the inventory and check if it's done correctly\n    load_mappings(app)\n    assert len(app._warning.content) == 2\n"}
{"repo_name":"xianjunzhengbackup/Cloud-Native-Python","ref":"refs/heads/master","path":"env/lib/python3.6/site-packages/pip/_vendor/progress/__init__.py","content":"# Copyright (c) 2012 Giorgos Verigakis \u003cverigak@gmail.com\u003e\n#\n# Permission to use, copy, modify, and distribute this software for any\n# purpose with or without fee is hereby granted, provided that the above\n# copyright notice and this permission notice appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nfrom __future__ import division\n\nfrom collections import deque\nfrom datetime import timedelta\nfrom math import ceil\nfrom sys import stderr\nfrom time import time\n\n\n__version__ = '1.2'\n\n\nclass Infinite(object):\n    file = stderr\n    sma_window = 10\n\n    def __init__(self, *args, **kwargs):\n        self.index = 0\n        self.start_ts = time()\n        self._ts = self.start_ts\n        self._dt = deque(maxlen=self.sma_window)\n        for key, val in kwargs.items():\n            setattr(self, key, val)\n\n    def __getitem__(self, key):\n        if key.startswith('_'):\n            return None\n        return getattr(self, key, None)\n\n    @property\n    def avg(self):\n        return sum(self._dt) / len(self._dt) if self._dt else 0\n\n    @property\n    def elapsed(self):\n        return int(time() - self.start_ts)\n\n    @property\n    def elapsed_td(self):\n        return timedelta(seconds=self.elapsed)\n\n    def update(self):\n        pass\n\n    def start(self):\n        pass\n\n    def finish(self):\n        pass\n\n    def next(self, n=1):\n        if n \u003e 0:\n            now = time()\n            dt = (now - self._ts) / n\n            self._dt.append(dt)\n            self._ts = now\n\n        self.index = self.index + n\n        self.update()\n\n    def iter(self, it):\n        for x in it:\n            yield x\n            self.next()\n        self.finish()\n\n\nclass Progress(Infinite):\n    def __init__(self, *args, **kwargs):\n        super(Progress, self).__init__(*args, **kwargs)\n        self.max = kwargs.get('max', 100)\n\n    @property\n    def eta(self):\n        return int(ceil(self.avg * self.remaining))\n\n    @property\n    def eta_td(self):\n        return timedelta(seconds=self.eta)\n\n    @property\n    def percent(self):\n        return self.progress * 100\n\n    @property\n    def progress(self):\n        return min(1, self.index / self.max)\n\n    @property\n    def remaining(self):\n        return max(self.max - self.index, 0)\n\n    def start(self):\n        self.update()\n\n    def goto(self, index):\n        incr = index - self.index\n        self.next(incr)\n\n    def iter(self, it):\n        try:\n            self.max = len(it)\n        except TypeError:\n            pass\n\n        for x in it:\n            yield x\n            self.next()\n        self.finish()\n"}
{"repo_name":"thierry1985/project-1022","ref":"refs/heads/master","path":"MISC/TFD-0.2.2/translate/pddl/parser.py","content":"__all__ = [\"ParseError\", \"parse_nested_list\"]\n\nclass ParseError(Exception):\n  pass\n\n# Basic functions for parsing PDDL (Lisp) files.\ndef parse_nested_list(input_file):\n  tokens = tokenize(input_file)\n  next_token = tokens.next()\n  if next_token != \"(\":\n    raise ParseError(\"Expected '(', got %s.\" % next_token)\n  result = list(parse_list_aux(tokens))\n  for tok in tokens:  # Check that generator is exhausted.\n    raise ParseError(\"Unexpected token: %s.\" % tok)\n  return result\n  \ndef tokenize(input):\n  for line in input:\n    line = line.split(\";\", 1)[0]  # Strip comments.\n    line = line.replace(\"(\", \" ( \").replace(\")\", \" ) \").replace(\"?\", \" ?\")\n    for token in line.split():\n      yield token.lower()\n\ndef parse_list_aux(tokenstream):\n  # Leading \"(\" has already been swallowed.\n  while True:\n    try:\n      token = tokenstream.next()\n    except StopIteration:\n      raise ParseError()\n    if token == \")\":\n      return\n    elif token == \"(\":\n      yield list(parse_list_aux(tokenstream))\n    else:\n      yield token\n\n"}
{"repo_name":"andrewburnheimer/ptpop","ref":"refs/heads/master","path":"ptpop/Console.py","content":"#!/usr/local/bin/python\n'''\nConsole Class\n'''\n'''\nTo Do:\n    -\n'''\n\nfrom Listener import Listener\nfrom _version import __version__\nimport time\n\n# =============================================================================\n# Console\n# \n# Inheriting from `object` (top-level class)\n# =============================================================================\nclass Console(object):\n    def __init__(self, args=None):\n        '''\n        Console Initialization\n        Input Attributes:\n        -----------------\n        self.args -\u003e argparse.Namespace: object holding attributes set\n                                         on command-line.\n        '''\n\n        # Default Values\n        delay = 3.0\n        number = 1 # XXX should be = 0\n        command = [ ]\n        interface = 'eth0'\n        listen = False\n        host = 'localhost'\n        if args:\n            delay = float(args.delay) if args.delay else delay\n            number = args.number if (args.number != None) else number\n            command = args.command if args.command else command\n            interface = args.interface if args.interface else interface\n            listen = args.listen if args.listen else listen\n            host = args.host if args.host else host\n\n        # Input Checks\n        if command != [ ]:\n            raise NotImplementedError('Issuing commands to hosts has ' +\n                    'not been implemented yet')\n\n        # init ...\n        if listen:\n            self.listener = Listener(interface)\n            key = '''\nremote          Dly St Dom Pr1  Cl Acc   Var  Pr2       Uniq       SyncT  DlyT  AnnT\n===================================================================================='''.strip()\n\n\n            while number \u003e 0:\n                # Report output directly to console\n                fmt='%a %b %d %Y %H:%M:%S'\n                t = time.time()\n                time_str = time.strftime(fmt, time.localtime(t))\n                time_msecs = int((t - int(t)) * 1000)\n\n                print time_str + '.%03d ' % (time_msecs) + time.tzname[0]\n                print key\n\n                # output data seen in since last iteration\n                neighbor_stats = self.listener.ptp_neighbors\n                for neighbor in neighbor_stats:\n                    print self.listener.ptp_neighbors[neighbor]\n\n                print\n                number -= 1\n                if number \u003c= 0:\n                    exit(0)\n                    # No need to wait after the last iteration\n                time.sleep(delay)\n\n            # Enter into the interactive environment, exit when q is\n            # issued\n\n        else:\n            for supplied_command in command:\n                command = supplied_command.lower()\n\n                if command == 'rv' or command == 'readvar':\n                    None\n                    # Assuming to be similar to NTPQ\n            # root@raspberrypi:/home/puppet# ntpq -n -c rv -c peers\n            #associd=0 status=0615 leap_none, sync_ntp, 1 event, clock_sync,\n            #version=\"ntpd 4.2.6p5@1.2349-o Mon Nov  2 04:29:47 UTC 2015 (1)\",\n            #processor=\"armv6l\", system=\"Linux/4.1.17+\", leap=00, stratum=3,\n            #precision=-20, rootdelay=2.916, rootdisp=60.561,\n            #refid=3.44.174.43,\n            #reftime=da7f3666.54078831  Mon, Feb 29 2016 21:28:06.328,\n            #clock=da7f38cd.57a72dc6  Mon, Feb 29 2016 21:38:21.342,\n            #peer=7185, tc=8,\n            #mintc=3, offset=9.208, frequency=-48.954, sys_jitter=0.000,\n            #clk_jitter=16.919, clk_wander=4.216\n                elif command == 'peers':\n                    None\n                    # Assuming to be similar to NTPQ\n            #     remote           refid      st t when poll reach   delay   offset  jitter\n            #==============================================================================\n            #*useclsifl158.tf 3.199.96.254     2 u   14 1024  377    1.582    0.186   0.919\n                else:\n                    raise NotImplementedError('Unknown command, \\'' +\n                            command + '\\'')\n\n# __main__.py is executed when the package is instantiated\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(prog='ptpop', description='Gain ' +\n        'insight into the operations of IEEE 1588 Precision Time Protocol ' +\n        'domains on a network. Press the \\'q\\' key to quit.')\n\n    command_choices=['readvar', 'rv', 'peers']\n    parser.add_argument('host', type=str, nargs='?', help='each of the ' +\n                        'commands will be sent to the PTP servers ' +\n                        'running on the host provided, localhost by ' +\n                        'default.')\n    parser.add_argument('-c', '--command', type=str, action='append',\n                        help='a command to run on the provided host, ' +\n                        'i.e. ' + str(command_choices) + ', \\'readvar\\' ' +\n                        'by default. Multiple commands can be issued.')\n    parser.add_argument('-i', '--interface', type=str,\n                        help='interface to issue commands on or to ' +\n                        'observe on in listen mode.')\n    parser.add_argument('-l', '--listen', action='store_true',\n                        help='don\\'t contact any PTP servers, but ' +\n                        'report on any services currently observed ' +\n                        'on the network, instead.')\n    parser.add_argument('-d', '--delay', metavar='SECS.TENTHS', type=str,\n                        help='Specifies the delay between screen ' +\n                        'updates when interactive. Can be changed while ' +\n                        'running using the \\'d\\' key. Negative ' +\n                        'numbers are not allowed. Setting this value ' +\n                        'to 0 is the same as issuing the \\'-n 1\\' ' +\n                        'option.')\n    parser.add_argument('-n', '--number', metavar='COUNT', type=int,\n                        help='Specifies the maximum number of iterations ' +\n                        'in interactive mode before ending.')\n    parser.add_argument('-v', '--version', action='version',\n                        version='%(prog)s ' + __version__)\n\n    args = parser.parse_args()\n    try:\n        c = Console(args)\n\n    except Exception as e:\n        print type(e).__name__ + \": \" + str(e.message)\n        exit(-1)\n\nif __name__ == '__main__':\n    main()\n"}
{"repo_name":"achoy/cwapi","ref":"refs/heads/master","path":"backend/py-server/flask/lib/python3.6/site-packages/six.py","content":"\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\n# Copyright (c) 2010-2015 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson \u003cbenjamin@python.org\u003e\"\n__version__ = \"1.10.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] \u003e= (3, 4)\n\nif PY3:\n    string_types = str,\n    integer_types = int,\n    class_types = type,\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = basestring,\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 \u003c\u003c 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n\n            def __len__(self):\n                return 1 \u003c\u003c 31\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 \u003c\u003c 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 \u003c\u003c 63) - 1)\n        del X\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\"),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n                \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n                \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n                \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n                      \"moves.urllib_error\", \"moves.urllib.error\")\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n                      \"moves.urllib_request\", \"moves.urllib.request\")\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n                      \"moves.urllib_response\", \"moves.urllib.response\")\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n                      \"moves.urllib\")\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n    def advance_iterator(it):\n        return it.next()\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(get_unbound_function,\n         \"\"\"Get the function out of a possibly unbound function\"\"\")\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems,\n         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(iterlists,\n         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n\n\nif PY3:\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n    unichr = chr\n    import struct\n    int2byte = struct.Struct(\"\u003eB\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] \u003c= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\nelse:\n    def b(s):\n        return s\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        if value is None:\n            value = tp()\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n\nelse:\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\"\"\"def reraise(tp, value, tb=None):\n    raise tp, value, tb\n\"\"\")\n\n\nif sys.version_info[:2] == (3, 2):\n    exec_(\"\"\"def raise_from(value, from_value):\n    if from_value is None:\n        raise value\n    raise value from from_value\n\"\"\")\nelif sys.version_info[:2] \u003e (3, 2):\n    exec_(\"\"\"def raise_from(value, from_value):\n    raise value from from_value\n\"\"\")\nelse:\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (isinstance(fp, file) and\n                    isinstance(data, unicode) and\n                    fp.encoding is not None):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\"\\n\")\n            space = unicode(\" \")\n        else:\n            newline = \"\\n\"\n            space = \" \"\n        if sep is None:\n            sep = space\n        if end is None:\n            end = newline\n        for i, arg in enumerate(args):\n            if i:\n                write(sep)\n            write(arg)\n        write(end)\nif sys.version_info[:2] \u003c (3, 3):\n    _print = print_\n\n    def print_(*args, **kwargs):\n        fp = kwargs.get(\"file\", sys.stdout)\n        flush = kwargs.pop(\"flush\", False)\n        _print(*args, **kwargs)\n        if flush and fp is not None:\n            fp.flush()\n\n_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n\nif sys.version_info[0:2] \u003c (3, 4):\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\n              updated=functools.WRAPPER_UPDATES):\n        def wrapper(f):\n            f = functools.wraps(wrapped, assigned, updated)(f)\n            f.__wrapped__ = wrapped\n            return f\n        return wrapper\nelse:\n    wraps = functools.wraps\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(meta):\n\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n    return type.__new__(metaclass, 'temporary_class', (), {})\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper\n\n\ndef python_2_unicode_compatible(klass):\n    \"\"\"\n    A decorator that defines __unicode__ and __str__ methods under Python 2.\n    Under Python 3 it does nothing.\n\n    To support Python 2 and 3 with a single code base, define a __str__ method\n    returning text and apply this decorator to the class.\n    \"\"\"\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                             \"to %s because it doesn't define __str__().\" %\n                             klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass\n\n\n# Complete the moves implementation.\n# This code is at the end of this module to speed up module loading.\n# Turn this module into a package.\n__path__ = []  # required for PEP 302 and PEP 451\n__package__ = __name__  # see PEP 366 @ReservedAssignment\nif globals().get(\"__spec__\") is not None:\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\n# Remove other six meta path importers, since they cause problems. This can\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\n# this for some reason.)\nif sys.meta_path:\n    for i, importer in enumerate(sys.meta_path):\n        # Here's some real nastiness: Another \"instance\" of the six module might\n        # be floating around. Therefore, we can't use isinstance() to check for\n        # the six meta path importer, since the other six instance will have\n        # inserted an importer with different class.\n        if (type(importer).__name__ == \"_SixMetaPathImporter\" and\n                importer.name == __name__):\n            del sys.meta_path[i]\n            break\n    del i, importer\n# Finally, add the importer to the meta path import hook.\nsys.meta_path.append(_importer)\n"}
{"repo_name":"sbstp/streamlink","ref":"refs/heads/master","path":"src/streamlink/plugins/alieztv.py","content":"import re\n\nfrom os.path import splitext\n\nfrom streamlink.compat import urlparse, unquote\nfrom streamlink.plugin import Plugin\nfrom streamlink.plugin.api import http, validate\nfrom streamlink.stream import HTTPStream, RTMPStream\n\n_url_re = re.compile(\"\"\"\n    http(s)?://(\\w+\\.)?aliez.tv\n    (?:\n        /live/[^/]+\n    )?\n    (?:\n        /video/\\d+/[^/]+\n    )?\n\"\"\", re.VERBOSE)\n_file_re = re.compile(\"\\\"?file\\\"?:\\s+['\\\"]([^'\\\"]+)['\\\"]\")\n_swf_url_re = re.compile(\"swfobject.embedSWF\\(\\\"([^\\\"]+)\\\",\")\n\n_schema = validate.Schema(\n    validate.union({\n        \"urls\": validate.all(\n            validate.transform(_file_re.findall),\n            validate.map(unquote),\n            [validate.url()]\n        ),\n        \"swf\": validate.all(\n            validate.transform(_swf_url_re.search),\n            validate.any(\n                None,\n                validate.all(\n                    validate.get(1),\n                    validate.url(\n                        scheme=\"http\",\n                        path=validate.endswith(\"swf\")\n                    )\n                )\n            )\n        )\n    })\n)\n\n\nclass Aliez(Plugin):\n    @classmethod\n    def can_handle_url(self, url):\n        return _url_re.match(url)\n\n    def _get_streams(self):\n        res = http.get(self.url, schema=_schema)\n        streams = {}\n        for url in res[\"urls\"]:\n            parsed = urlparse(url)\n            if parsed.scheme.startswith(\"rtmp\"):\n                params = {\n                    \"rtmp\": url,\n                    \"pageUrl\": self.url,\n                    \"live\": True\n                }\n                if res[\"swf\"]:\n                    params[\"swfVfy\"] = res[\"swf\"]\n\n                stream = RTMPStream(self.session, params)\n                streams[\"live\"] = stream\n            elif parsed.scheme.startswith(\"http\"):\n                name = splitext(parsed.path)[1][1:]\n                stream = HTTPStream(self.session, url)\n                streams[name] = stream\n\n        return streams\n\n__plugin__ = Aliez\n"}
{"repo_name":"astronaut1712/taiga-back","ref":"refs/heads/master","path":"taiga/projects/wiki/models.py","content":"# Copyright (C) 2014 Andrey Antukh \u003cniwi@niwi.be\u003e\n# Copyright (C) 2014 Jess Espino \u003cjespinog@gmail.com\u003e\n# Copyright (C) 2014 David Barragn \u003cbameda@dbarragan.com\u003e\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see \u003chttp://www.gnu.org/licenses/\u003e.\n\nfrom django.db import models\nfrom django.contrib.contenttypes import generic\nfrom django.conf import settings\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.utils import timezone\nfrom taiga.projects.notifications.mixins import WatchedModelMixin\nfrom taiga.projects.occ import OCCModelMixin\n\n\nclass WikiPage(OCCModelMixin, WatchedModelMixin, models.Model):\n    project = models.ForeignKey(\"projects.Project\", null=False, blank=False,\n                                related_name=\"wiki_pages\", verbose_name=_(\"project\"))\n    slug = models.SlugField(max_length=500, db_index=True, null=False, blank=False,\n                            verbose_name=_(\"slug\"))\n    content = models.TextField(null=False, blank=True,\n                               verbose_name=_(\"content\"))\n    owner = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, blank=True,\n                              related_name=\"owned_wiki_pages\", verbose_name=_(\"owner\"))\n    last_modifier = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, blank=True,\n                              related_name=\"last_modified_wiki_pages\", verbose_name=_(\"last modifier\"))\n    created_date = models.DateTimeField(null=False, blank=False,\n                                        verbose_name=_(\"created date\"),\n                                        default=timezone.now)\n    modified_date = models.DateTimeField(null=False, blank=False,\n                                         verbose_name=_(\"modified date\"))\n    attachments = generic.GenericRelation(\"attachments.Attachment\")\n    _importing = None\n\n    class Meta:\n        verbose_name = \"wiki page\"\n        verbose_name_plural = \"wiki pages\"\n        ordering = [\"project\", \"slug\"]\n        unique_together = (\"project\", \"slug\",)\n        permissions = (\n            (\"view_wikipage\", \"Can view wiki page\"),\n        )\n\n    def __str__(self):\n        return \"project {0} - {1}\".format(self.project_id, self.slug)\n\n    def save(self, *args, **kwargs):\n        if not self._importing or not self.modified_date:\n            self.modified_date = timezone.now()\n\n        return super().save(*args, **kwargs)\n\n\nclass WikiLink(models.Model):\n    project = models.ForeignKey(\"projects.Project\", null=False, blank=False,\n                                related_name=\"wiki_links\", verbose_name=_(\"project\"))\n    title = models.CharField(max_length=500, null=False, blank=False)\n    href = models.SlugField(max_length=500, db_index=True, null=False, blank=False,\n                            verbose_name=_(\"href\"))\n    order = models.PositiveSmallIntegerField(default=1, null=False, blank=False,\n                                             verbose_name=_(\"order\"))\n\n    class Meta:\n        verbose_name = \"wiki link\"\n        verbose_name_plural = \"wiki links\"\n        ordering = [\"project\", \"order\"]\n        unique_together = (\"project\", \"href\")\n\n    def __str__(self):\n        return self.title\n"}
{"repo_name":"helenst/django","ref":"refs/heads/master","path":"django/contrib/gis/db/backends/mysql/introspection.py","content":"from MySQLdb.constants import FIELD_TYPE\n\nfrom django.contrib.gis.gdal import OGRGeomType\nfrom django.db.backends.mysql.introspection import DatabaseIntrospection\n\n\nclass MySQLIntrospection(DatabaseIntrospection):\n    # Updating the data_types_reverse dictionary with the appropriate\n    # type for Geometry fields.\n    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()\n    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'\n\n    def get_geometry_type(self, table_name, geo_col):\n        cursor = self.connection.cursor()\n        try:\n            # In order to get the specific geometry type of the field,\n            # we introspect on the table definition using `DESCRIBE`.\n            cursor.execute('DESCRIBE %s' %\n                           self.connection.ops.quote_name(table_name))\n            # Increment over description info until we get to the geometry\n            # column.\n            for column, typ, null, key, default, extra in cursor.fetchall():\n                if column == geo_col:\n                    # Using OGRGeomType to convert from OGC name to Django field.\n                    # MySQL does not support 3D or SRIDs, so the field params\n                    # are empty.\n                    field_type = OGRGeomType(typ).django\n                    field_params = {}\n                    break\n        finally:\n            cursor.close()\n\n        return field_type, field_params\n\n    def supports_spatial_index(self, cursor, table_name):\n        # Supported with MyISAM, or InnoDB on MySQL 5.7.5+\n        storage_engine = self.get_storage_engine(cursor, table_name)\n        return (\n            (storage_engine == 'InnoDB' and self.connection.mysql_version \u003e= (5, 7, 5)) or\n            storage_engine == 'MyISAM'\n        )\n"}
{"repo_name":"PaulWay/insights-core","ref":"refs/heads/master","path":"insights/parsers/tests/test_foreman_log.py","content":"from insights.tests import context_wrap\nfrom insights.parsers.foreman_log import SatelliteLog, ProductionLog\nfrom insights.parsers.foreman_log import CandlepinLog, ProxyLog\n\n\nPRODUCTION_LOG = \"\"\"\n2015-11-13 03:30:07 [I] Completed 200 OK in 1783ms (Views: 0.2ms | ActiveRecord: 172.9ms)\n2015-11-13 03:30:07 [I] Processing by Katello::Api::V2::RepositoriesController#sync_complete as JSON\n2015-11-13 03:30:07 [I]   Parameters: {\"call_report\"=\u003e\"[FILTERED]\", \"event_type\"=\u003e\"repo.sync.finish\", \"payload\"=\u003e{\"importer_id\"=\u003e\"yum_importer\", \"exception\"=\u003enil, \"repo_id\"=\u003e\"1-Gulfstream_Aerospace_Corp_-Red_Hat_Enterprise_Linux_Server-Red_Hat_Satellite_Tools_6_1_for_RHEL_6_Server_RPMs_i386\", \"traceback\"=\u003enil, \"started\"=\u003e\"2015-11-13T08:30:00Z\", \"_ns\"=\u003e\"repo_sync_results\", \"completed\"=\u003e\"2015-11-13T08:30:06Z\", \"importer_type_id\"=\u003e\"yum_importer\", \"error_message\"=\u003enil, \"summary\"=\u003e{\"content\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"comps\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"distribution\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"errata\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"metadata\"=\u003e{\"state\"=\u003e\"FINISHED\"}}, \"added_count\"=\u003e0, \"result\"=\u003e\"success\", \"updated_count\"=\u003e3, \"details\"=\u003e{\"content\"=\u003e{\"size_total\"=\u003e0, \"items_left\"=\u003e0, \"items_total\"=\u003e0, \"state\"=\u003e\"FINISHED\", \"size_left\"=\u003e0, \"details\"=\u003e{\"rpm_total\"=\u003e0, \"rpm_done\"=\u003e0, \"drpm_total\"=\u003e0, \"drpm_done\"=\u003e0}, \"error_details\"=\u003e[]}, \"comps\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"distribution\"=\u003e{\"items_total\"=\u003e0, \"state\"=\u003e\"FINISHED\", \"error_details\"=\u003e[], \"items_left\"=\u003e0}, \"errata\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"metadata\"=\u003e{\"state\"=\u003e\"FINISHED\"}}, \"id\"=\u003e\"56459f8ef301a213bbfd87bb\", \"removed_count\"=\u003e0}, \"token\"=\u003e\"oQumn3XsKrdRkijuvpCNhKF2PDWZt6az\", \"api_version\"=\u003e\"v2\", \"repository\"=\u003e{}}\n2015-11-13 03:30:07 [I] Sync_complete called for Red Hat Satellite Tools 6.1 for RHEL 6 Server RPMs i386, running after_sync.\n2015-11-13 03:30:09 [I] Completed 200 OK in 1995ms (Views: 0.2ms | ActiveRecord: 81.5ms)\n2015-11-13 03:30:10 [I] Processing by Katello::Api::V2::RepositoriesController#sync_complete as JSON\n2015-11-13 03:30:10 [I]   Parameters: {\"call_report\"=\u003e\"[FILTERED]\", \"event_type\"=\u003e\"repo.sync.finish\", \"payload\"=\u003e{\"importer_id\"=\u003e\"yum_importer\", \"exception\"=\u003enil, \"repo_id\"=\u003e\"1-Gulfstream_Aerospace_Corp_-Red_Hat_Enterprise_Linux_Server-Red_Hat_Satellite_Tools_6_1_for_RHEL_5_Server_RPMs_i386\", \"traceback\"=\u003enil, \"started\"=\u003e\"2015-11-13T08:30:05Z\", \"_ns\"=\u003e\"repo_sync_results\", \"completed\"=\u003e\"2015-11-13T08:30:10Z\", \"importer_type_id\"=\u003e\"yum_importer\", \"error_message\"=\u003enil, \"summary\"=\u003e{\"content\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"comps\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"distribution\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"errata\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"metadata\"=\u003e{\"state\"=\u003e\"FINISHED\"}}, \"added_count\"=\u003e0, \"result\"=\u003e\"success\", \"updated_count\"=\u003e3, \"details\"=\u003e{\"content\"=\u003e{\"size_total\"=\u003e0, \"items_left\"=\u003e0, \"items_total\"=\u003e0, \"state\"=\u003e\"FINISHED\", \"size_left\"=\u003e0, \"details\"=\u003e{\"rpm_total\"=\u003e0, \"rpm_done\"=\u003e0, \"drpm_total\"=\u003e0, \"drpm_done\"=\u003e0}, \"error_details\"=\u003e[]}, \"comps\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"distribution\"=\u003e{\"items_total\"=\u003e0, \"state\"=\u003e\"FINISHED\", \"error_details\"=\u003e[], \"items_left\"=\u003e0}, \"errata\"=\u003e{\"state\"=\u003e\"FINISHED\"}, \"metadata\"=\u003e{\"state\"=\u003e\"FINISHED\"}}, \"id\"=\u003e\"56459f92f301a2137cd6b802\", \"removed_count\"=\u003e0}, \"token\"=\u003e\"oQumn3XsKrdRkijuvpCNhKF2PDWZt6az\", \"api_version\"=\u003e\"v2\", \"repository\"=\u003e{}}\n2015-11-13 03:30:10 [I] Sync_complete called for Red Hat Satellite Tools 6.1 for RHEL 5 Server RPMs i386, running after_sync.\n2015-11-13 03:30:11 [I] Connecting to database specified by database.yml\n2015-11-13 03:30:11 [I] Connecting to database specified by database.yml\n2015-11-13 03:30:11 [I] Completed 200 OK in 818ms (Views: 0.2ms | ActiveRecord: 77.2ms)\n2015-11-13 03:30:17 [I] Connecting to database specified by database.yml\n2015-11-13 03:30:26 [I] Sync_complete called for RHN Tools for Red Hat Enterprise Linux 5 Server RPMs x86_64 5Server, running after_sync.\n2015-11-13 03:50:46 [I] Completed 200 OK in 2583ms (Views: 2.7ms | ActiveRecord: 0.3ms)\n2015-11-13 06:58:25 [I]   Parameters: {\"id\"=\u003e\"cfd7275b-8cce-4323-8d1f-55ef85eca883\"}\n2015-11-13 06:58:25 [I] Completed 200 OK in 249ms (Views: 3.1ms | ActiveRecord: 0.3ms)\n2015-11-13 06:59:26 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2015-11-13 06:59:26 [I]   Parameters: {\"id\"=\u003e\"cfd7275b-8cce-4323-8d1f-55ef85eca883\"}\n2015-11-13 06:59:26 [I] Completed 200 OK in 84ms (Views: 3.1ms | ActiveRecord: 0.3ms)\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\n2015-11-13 07:09:22 [I]   Parameters: {\"facts\"=\u003e\"[FILTERED]\", \"name\"=\u003e\"infrhnpl002.gac.gulfaero.com\", \"certname\"=\u003e\"infrhnpl002.gac.gulfaero.com\", \"apiv\"=\u003e\"v2\", :host=\u003e{\"name\"=\u003e\"infrhnpl002.gac.gulfaero.com\", \"certname\"=\u003e\"infrhnpl002.gac.gulfaero.com\"}}\n2015-11-13 07:09:22 [I] Import facts for 'infrhnpl002.gac.gulfaero.com' completed. Added: 0, Updated: 6, Deleted 0 facts\n2015-11-13 07:09:22 [I] Completed 201 Created in 251ms (Views: 179.3ms | ActiveRecord: 0.0ms)\n2015-11-13 07:09:22 [I] Processing by HostsController#externalNodes as YML\n2015-11-13 07:09:22 [I]   Parameters: {\"name\"=\u003e\"infrhnpl002.gac.gulfaero.com\"}\n2015-11-13 07:09:22 [I]   Rendered text template (0.0ms)\n2015-11-13 07:09:22 [I] Completed 200 OK in 48ms (Views: 0.5ms | ActiveRecord: 6.6ms)\n2015-11-13 07:09:22 [I] Processing by Api::V2::ReportsController#create as JSON\n2015-11-13 07:09:22 [I]   Parameters: {\"report\"=\u003e\"[FILTERED]\", \"apiv\"=\u003e\"v2\"}\n2015-11-13 07:09:22 [I]   Rendered text template (0.0ms)\n2015-11-13 07:09:22 [I] processing report for infrhnpl002.gac.gulfaero.com\n2015-11-13 07:09:22 [I] Imported report for infrhnpl002.gac.gulfaero.com in 0.02 seconds\n2015-11-13 07:09:22 [I] Completed 201 Created in 28ms (Views: 1.2ms | ActiveRecord: 0.0ms)\n2015-11-13 07:30:17 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\n2015-11-13 07:30:25 [I] Client connected.\n2015-11-13 07:30:25 [I] Connected to server.\n2015-11-13 07:30:25 [I] Client connected.\n2015-11-13 07:30:25 [I] Connected to server.\n2015-11-13 07:30:25 [I] Client connected.\n2015-11-13 07:30:25 [I] Connected to server.\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\n2015-11-13 07:30:32 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2015-11-13 07:30:32 [I]   Parameters: {\"id\"=\u003e\"cfd7275b-8cce-4323-8d1f-55ef85eca883\"}\n2015-11-13 07:30:32 [I] Completed 200 OK in 110ms (Views: 2.7ms | ActiveRecord: 0.3ms)\n2015-11-13 07:30:33 [I] 2015-11-13 07:30:33 -0500: Expired 48 Reports\n2015-11-13 07:30:33 [I] Client disconnected.\n2015-11-13 09:41:58 [I] Completed 200 OK in 93ms (Views: 2.9ms | ActiveRecord: 0.3ms)\n2015-11-13 09:42:58 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2015-11-13 09:42:58 [I]   Parameters: {\"id\"=\u003e\"cfd7275b-8cce-4323-8d1f-55ef85eca883\"}\n2015-11-13 09:42:58 [I] Completed 200 OK in 80ms (Views: 3.6ms | ActiveRecord: 0.3ms)\n2015-11-13 09:43:58 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2015-11-13 09:43:58 [I]   Parameters: {\"id\"=\u003e\"cfd7275b-8cce-4323-8d1f-55ef85eca883\"}\n2015-11-13 09:43:59 [I] Completed 200 OK in 80ms (Views: 2.9ms | ActiveRecord: 0.3ms)\n\"\"\".strip()\n\n\nSATELLITE_OUT = \"\"\"\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_cert]/require: requires Class[Certs::Pulp_client]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_cert]/require: requires Exec[foreman-rake-db:seed]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_key]/require: requires Class[Certs::Pulp_client]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_key]/require: requires Exec[foreman-rake-db:seed]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/before: requires Class[Foreman::Database]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/before: requires Exec[foreman-rake-db:migrate]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/notify: subscribes to Service[foreman-tasks]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/notify: subscribes to Class[Foreman::Service]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/Foreman::Config::Passenger::Fragment[katello]/require: requires Class[Foreman::Config::Passenger]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/notify: subscribes to Class[Certs::Candlepin]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Cert[kam1opapp999.connex.bclc.com-qpid-broker]/notify: subscribes to Pubkey[/etc/pki/katello/certs/kam1opapp999.connex.bclc.com-qpid-broker.crt]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Pubkey[/etc/pki/katello/certs/kam1opapp999.connex.bclc.com-qpid-broker.crt]/notify: subscribes to Privkey[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Privkey[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]/notify: subscribes to File[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]/notify: subscribes to File[/etc/pki/katello/nssdb]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb]/notify: subscribes to Exec[generate-nss-password]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[generate-nss-password]/before: requires File[/etc/pki/katello/nssdb/nss_db_password-file]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/nss_db_password-file]/notify: subscribes to Exec[create-nss-db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete ca]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete broker]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete amqp-client]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/notify: subscribes to Certs::Ssltools::Certutil[ca]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/cert8.db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/key3.db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/secmod.db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/cert8.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/key3.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/secmod.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[broker]/notify: subscribes to Exec[generate-pfx-for-nss-db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[generate-pfx-for-nss-db]/notify: subscribes to Exec[add-private-key-to-nss-db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[add-private-key-to-nss-db]/notify: subscribes to Service[qpidd]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/notify: subscribes to Class[Candlepin]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Cert[java-client]/notify: subscribes to Pubkey[/etc/pki/katello/certs/java-client.crt]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/pki/katello/keystore_password-file]/notify: subscribes to Exec[candlepin-generate-ssl-keystore]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[candlepin-generate-ssl-keystore]/notify: subscribes to File[/usr/share/tomcat/conf/keystore]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/usr/share/tomcat/conf/keystore]/notify: subscribes to Service[tomcat]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Pubkey[/etc/pki/katello/certs/java-client.crt]/notify: subscribes to Privkey[/etc/pki/katello/private/java-client.key]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Privkey[/etc/pki/katello/private/java-client.key]/notify: subscribes to Certs::Ssltools::Certutil[amqp-client]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/subscribe: subscribes to Exec[create-nss-db]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/notify: subscribes to Service[qpidd]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/notify: subscribes to File[/etc/candlepin/certs/amqp]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/candlepin/certs/amqp]/notify: subscribes to Exec[create candlepin qpid exchange]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[create candlepin qpid exchange]/require: requires Service[qpidd]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[create candlepin qpid exchange]/notify: subscribes to Exec[import CA into Candlepin truststore]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[import CA into Candlepin truststore]/notify: subscribes to Exec[import client certificate into Candlepin keystore]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[import client certificate into Candlepin keystore]/notify: subscribes to File[/etc/candlepin/certs/amqp/candlepin.jks]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/candlepin/certs/amqp/candlepin.jks]/notify: subscribes to Service[tomcat]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin/notify: subscribes to Class[Qpid]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Install/notify: subscribes to Class[Candlepin::Config]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Config/notify: subscribes to Class[Candlepin::Database]\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Database/notify: subscribes to Class[Candlepin::Service]\n\"\"\".strip()\n\nCANDLEPIN_LOG = \"\"\"\n2016-09-09 13:45:52,650 [req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b\n2016-09-09 13:45:52,784 [req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc, org=ING_Luxembourg_SA] INFO  org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\"application/json\", time=134\n2016-09-09 13:45:52,947 [req=909ca4c5-f24e-4212-8f23-cc754d06ac57, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b/content_overrides\n2016-09-09 13:45:52,976 [req=909ca4c5-f24e-4212-8f23-cc754d06ac57, org=] INFO org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\"application/json\", time=29\n2016-09-09 13:45:53,072 [req=49becd26-5dfe-4d2f-8667-470519230d88, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b/release\n2016-09-09 13:45:53,115 [req=49becd26-5dfe-4d2f-8667-470519230d88, org=ING_Luxembourg_SA] INFO  org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\"application/json\", time=43\n\"\"\".strip()\n\nPROXY_LOG = \"\"\"\n127.0.0.1 - - [31/May/2016:09:42:28 -0400] \"GET /puppet/environments/KT_Encore_Library_RHEL_6_5/classes HTTP/1.1\" 200 76785 6.1205\n127.0.0.1 - - [31/May/2016:09:42:38 -0400] \"GET /puppet/environments/KT_Encore_Library_RHEL_7_6/classes HTTP/1.1\" 200 76785 4.4754\n127.0.0.1 - - [31/May/2016:09:42:49 -0400] \"GET /puppet/environments/KT_Encore_Library_RHEL6_8/classes HTTP/1.1\" 200 76785 4.5776\n127.0.0.1 - - [31/May/2016:09:57:34 -0400] \"GET /tftp/serverName HTTP/1.1\" 200 38 0.0014\nE, [2016-05-31T09:57:34.884636 #4494] ERROR -- : Record 172.16.100.0/172.16.100.17 not found ]\n\"\"\".strip()\n\n\ndef test_production_log():\n    fm_log = ProductionLog(context_wrap(PRODUCTION_LOG))\n    assert 2 == len(fm_log.get(\"Rendered text template\"))\n    assert \"Expired 48 Reports\" in fm_log\n    assert fm_log.get(\"Completed 200 OK in 93\")[0] == \\\n        \"2015-11-13 09:41:58 [I] Completed 200 OK in 93ms (Views: 2.9ms | ActiveRecord: 0.3ms)\"\n\n\ndef test_proxy_log():\n    px_log = ProxyLog(context_wrap(PROXY_LOG))\n    assert \"ERROR -- \" in px_log\n    assert len(px_log.get(\"KT_Encore_Library_RHEL\")) == 3\n\n\ndef test_candlepin_log():\n    cp_log = CandlepinLog(context_wrap(CANDLEPIN_LOG))\n    assert \"req=49becd26-5dfe-4d2f-8667-470519230d88\" in cp_log\n    assert len(cp_log.get(\"req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc\")) == 2\n\n\ndef test_satellite_log():\n    sat_log = SatelliteLog(context_wrap(SATELLITE_OUT))\n    assert \"subscribes to Class[Qpid]\" in sat_log\n    assert len(sat_log.get(\"notify: subscribes to Class[\")) == 7\n"}
{"repo_name":"Thraxis/pymedusa","ref":"refs/heads/master","path":"lib/html5lib/treewalkers/etree.py","content":"from __future__ import absolute_import, division, unicode_literals\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    try:\n        from ordereddict import OrderedDict\n    except ImportError:\n        OrderedDict = dict\n\nimport re\n\nfrom six import string_types\n\nfrom . import _base\nfrom ..utils import moduleFactoryFactory\n\ntag_regexp = re.compile(\"{([^}]*)}(.*)\")\n\n\ndef getETreeBuilder(ElementTreeImplementation):\n    ElementTree = ElementTreeImplementation\n    ElementTreeCommentType = ElementTree.Comment(\"asd\").tag\n\n    class TreeWalker(_base.NonRecursiveTreeWalker):\n        \"\"\"Given the particular ElementTree representation, this implementation,\n        to avoid using recursion, returns \"nodes\" as tuples with the following\n        content:\n\n        1. The current element\n\n        2. The index of the element relative to its parent\n\n        3. A stack of ancestor elements\n\n        4. A flag \"text\", \"tail\" or None to indicate if the current node is a\n           text node; either the text or tail of the current element (1)\n        \"\"\"\n        def getNodeDetails(self, node):\n            if isinstance(node, tuple):  # It might be the root Element\n                elt, key, parents, flag = node\n                if flag in (\"text\", \"tail\"):\n                    return _base.TEXT, getattr(elt, flag)\n                else:\n                    node = elt\n\n            if not(hasattr(node, \"tag\")):\n                node = node.getroot()\n\n            if node.tag in (\"DOCUMENT_ROOT\", \"DOCUMENT_FRAGMENT\"):\n                return (_base.DOCUMENT,)\n\n            elif node.tag == \"\u003c!DOCTYPE\u003e\":\n                return (_base.DOCTYPE, node.text,\n                        node.get(\"publicId\"), node.get(\"systemId\"))\n\n            elif node.tag == ElementTreeCommentType:\n                return _base.COMMENT, node.text\n\n            else:\n                assert isinstance(node.tag, string_types), type(node.tag)\n                # This is assumed to be an ordinary element\n                match = tag_regexp.match(node.tag)\n                if match:\n                    namespace, tag = match.groups()\n                else:\n                    namespace = None\n                    tag = node.tag\n                attrs = OrderedDict()\n                for name, value in list(node.attrib.items()):\n                    match = tag_regexp.match(name)\n                    if match:\n                        attrs[(match.group(1), match.group(2))] = value\n                    else:\n                        attrs[(None, name)] = value\n                return (_base.ELEMENT, namespace, tag,\n                        attrs, len(node) or node.text)\n\n        def getFirstChild(self, node):\n            if isinstance(node, tuple):\n                element, key, parents, flag = node\n            else:\n                element, key, parents, flag = node, None, [], None\n\n            if flag in (\"text\", \"tail\"):\n                return None\n            else:\n                if element.text:\n                    return element, key, parents, \"text\"\n                elif len(element):\n                    parents.append(element)\n                    return element[0], 0, parents, None\n                else:\n                    return None\n\n        def getNextSibling(self, node):\n            if isinstance(node, tuple):\n                element, key, parents, flag = node\n            else:\n                return None\n\n            if flag == \"text\":\n                if len(element):\n                    parents.append(element)\n                    return element[0], 0, parents, None\n                else:\n                    return None\n            else:\n                if element.tail and flag != \"tail\":\n                    return element, key, parents, \"tail\"\n                elif key \u003c len(parents[-1]) - 1:\n                    return parents[-1][key + 1], key + 1, parents, None\n                else:\n                    return None\n\n        def getParentNode(self, node):\n            if isinstance(node, tuple):\n                element, key, parents, flag = node\n            else:\n                return None\n\n            if flag == \"text\":\n                if not parents:\n                    return element\n                else:\n                    return element, key, parents, None\n            else:\n                parent = parents.pop()\n                if not parents:\n                    return parent\n                else:\n                    return parent, list(parents[-1]).index(parent), parents, None\n\n    return locals()\n\ngetETreeModule = moduleFactoryFactory(getETreeBuilder)\n"}
{"repo_name":"xxxIsaacPeralxxx/anim-studio-tools","ref":"refs/heads/master","path":"kip/houdini/code/kip_houdini/convert.py","content":"#                 Dr. D Studios - Software Disclaimer\n#\n# Copyright 2009 Dr D Studios Pty Limited (ACN 127 184 954) (Dr. D Studios), its\n# affiliates and/or its licensors.\n#\n###############################################################################\n\"\"\"\nThis module will help TD's to convert houdini animation curve to nuke or maya\nthe other way around also.\n\n.. note::\n\n    Please make sure you are running in proper kipHoudini environment\n\n.. warning::\n\n    Dont import this module as standalone , use this module with kip project\n\n\"\"\"\n\n__authors__ = [\"kurian.os\"]\n__version__ = \"$Revision: 104961 $\".split()[1]\n__revision__ = __version__\n__date__ = \"$Date:  July 19, 2011 12:00:00 PM$\".split()[1]\n__copyright__ = \"2011\"\n__license__ = \"Copyright 2011 Dr D Studios Pty Limited\"\n__contact__ = \"kurian.os@drdstudios.com\"\n__status__ = \"Development\"\n\n\nimport os\nimport traceback\n#import hou\nimport napalm.core as nap_core\nimport node_curves as node_curves\nimport kip.kip_reader as kip_reader\nreload(node_curves)\nreload(kip_reader)\nfrom rodin import logging\nfrom kip.kip_curve_class import *\nfrom kip.kip_napalm_class import *\nfrom kip.utils.kipError import *\nfrom kip.template import *\n\n\nrodin_logger = logging.get_logger('kipHoudini')\nnapalm_func = Napalm()\n\nGLOBAL_FPS = 24\nGLOBAL_TIME = 1\n\n\nclass HoudiniWriter(object):\n\n    \"\"\"\n    Creating houdini curve writer class\n\n    *Parents:*\n\n        None\n\n    *Children:*\n\n        * :func:`writeOutCurves`\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Base init function for houdini convert.write Class.\n        \"\"\"\n        rodin_logger.info(\"kip houdini writing class initialized\")\n        self.houdini_version = \"houdini,%s\" % hou.applicationVersionString()\n        self.kip_houdini_version = \"kipHoudini%s\" % os.getenv(\"DRD_KIPHOUDINI_VERSION\")\n\n    def writeOutCurves(self, nap_file_name = None , houdini_nodes = [],\n                            houdini_node_attributes = [], start_frame = None,\n                            end_frame = None, write_xml = False, silent = False,\n                            left_eyes = [], right_eyes = [], map_file_name = None):\n\n        \"\"\"\n        This function will create a curve class object first and then it will write out the napalm file.\n\n        .. warning::\n\n            If you are unable to write out napalm file or write_status=False that means napalm failed to write out.\n\n        :param  nap_file_name: User must pass a file where he want to write out curves and make sure you supply a .nap or .xml file format(strict)\n\n        :type nap_file_name: string\n\n        :param houdini_nodes: list of houdini objects(strict)\n\n        :type houdini_nodes: list\n\n        :param houdini_node_attribute: if you want to replace attribute from the map file then you can specify the override attribute here\n\n        :type houdini_node_attribute: list\n\n        :param start_frame: start frame to capture\n\n        :type start_frame: int\n\n        :param end_frame: end frame to capture\n\n        :type end_frame: int\n\n        :param write_xml: If you want to write out a xml file instead of napalm file then this should be true\n\n        :type end_frame: string\n\n        :param left_eyes: Left eye objects\n\n        :type left_eyes: list\n\n        :param right_eyes: Right eye objects\n\n        :type right_eyes: list\n\n        :param map_file_name: Filepath of napalm channel data\n\n        :type map_file_name: string\n\n        :return: Status,channel file , map file\n\n        :rtype: boot,string,string\n\n        Example\n\n            \u003e\u003e\u003e import kip_houdini.convert as kh\n            \u003e\u003e\u003e reload(kh)\n            \u003e\u003e\u003e khcw = kh.HoudiniWriter()\n            \u003e\u003e\u003e status,nap_file,map_file=khcw.writeOutCurves(nap_file_name = \"/tmp/houdini_kip_test_s.nap\",map_file_name= \"/tmp/houdini_kip_test_m.nap\",houdini_nodes = [\"/obj/geo/xform_1\",\"/obj/geo/xform_2\"],left_eyes=[\"/obj/geo/xform_1\"],right_eyes=[\"/obj/geo/xform_2\"])\n\n        \"\"\"\n        if nap_file_name:\n            node_curv = node_curves.NodeCurves()\n            get_all_curves = node_curv.getCurves(houdini_node_curves = houdini_nodes, \\\n                                houdini_attribute_curves = houdini_node_attributes, \\\n                                start_frame = start_frame, end_frame = end_frame, \\\n                                silent = silent, left_eye_curves = left_eyes, \\\n                                right_eye_curves = right_eyes)\n            if write_xml:\n                if not nap_file_name.endswith(\".xml\"):\n                    split_base_ext = os.path.splitext(nap_file_name)\n                    if split_base_ext[-1]:\n                        nap_file_name = \"%s/.xml\" % (split_base_ext[0])\n                    else:\n                        nap_file_name = \"%s/.xml\" % (nap_file_name)\n            else:\n                if not nap_file_name.endswith(\".nap\"):\n                    raise KipBaseError(\"Unknown file extension found in %s !\" % nap_file_name)\n\n            write_status, map_file, nap_file = napalm_func.write(nap_file_name, get_all_curves, \\\n                                        debug = True, map_file_name = map_file_name, \\\n                                        software = self.houdini_version, \\\n                                        app_version = self.kip_houdini_version)\n\n            rodin_logger.info(\"%s %s %s\" % (write_status, map_file, nap_file))\n            return (write_status, map_file, nap_file)\n        else:\n            raise KipBaseError(\"Expected napalm file name for write curve !\")\n\nclass HoudiniReader(object):\n    \"\"\"\n\n    Creating houdini curve reader class\n\n    *Parents:*\n\n        None\n\n    *Children:*\n\n        * :func:`houSetAttr`\n\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Base init function for houdini convert.write Class.\n        \"\"\"\n        rodin_logger.info(\"kip houdini read class initialized\")\n        self.nuke_tan_types = {\"spline\":\"spline()\", \"linear\":\"linear()\", \\\n                                    \"constant\":\"constant()\", \"cubic\":\"bezier()\"}\n\n        self.channel_match = {'translateX':'tx', 'translateY':'ty', 'translateZ':'tz', \\\n                                'rotateX':'rx', 'rotateY':'ry', 'rotateZ':'rz', \\\n                                'scaleX':'sx', 'scaleY':'sy', 'scaleZ':'sz'}\n\n    def houSetAttr(self, nap_file_name = None, houdini_nodes = [], houdini_node_attribute = None,\n                            map_file_name = None, offset_value = 0, start_frame = None,\n                            end_frame = None, attribute_map = None):\n        \"\"\"\n        This function will get all curve data from a map and channel file then those data will be applied to proper nodes\n\n        :param  nap_file_name: User must pass a file where he want to write out curves and make sure you supply a .nap or .xml file format\n\n        :type nap_file_name: string\n\n        :param houdini_nodes: list of houdini objects\n\n        :type houdini_nodes: list\n\n        :param houdini_node_attribute: if you want to replace attribute from the map file then you can specify the override attribute here\n\n        :type houdini_node_attribute: string\n\n        :param map_file_name: Filepath of napalm channel data\n\n        :type map_file_name: string\n\n        :param offset_value: Animation key offset value\n\n        :type offset_value: int\n\n        :param start_frame: start frame to capture\n\n        :type start_frame: int\n\n        :param end_frame: end frame to capture\n\n        :type end_frame: int\n\n        :param attribute_map: This a template object from template module\n\n        :type attribute_map: list of tuple\n\n        Example\n\n            \u003e\u003e\u003e import kip_houdini.convert as kh\n            \u003e\u003e\u003e reload(kh)\n            \u003e\u003e\u003e khpr=kh.HoudiniReader()\n            \u003e\u003e\u003e import kip.template as template\n            \u003e\u003e\u003e attr_mp = template.KipTemplates()\n            \u003e\u003e\u003e attr_mp.ATTRMAP={\"t1.cutatt1\":\"/obj/geo1/xform1.ottr_1\",\"t1.cutatt2\":\"/obj/geo1/xform1.ottr_2\",\"t2.cutatt1\":\"/obj/geo1/xform1.ottr_3\",\"t2.cutatt2\":\"/obj/geo1/xform1.ottr_4\"}\n            \u003e\u003e\u003e a = attr_mp.ATTRMAP\n            \u003e\u003e\u003e khpr.houSetAttr(nap_file_name=\"/tmp/single_maya_test.nap\",houdini_nodes=\"/obj/geo1/xform1\",attribute_map=a)\n\n        \"\"\"\n        if nap_file_name:\n\n            if not map_file_name:\n                map_file_name = kip_reader.build_map_file_name(nap_file_name)\n            header_info = kip_reader.header(map_file_name)\n            array_index = kip_reader.find_software_index(header_info[\"client_software\"])\n\n            houdini_node_list = houdini_nodes\n            knob_read = kip_reader.ReadCurve()\n            get_curve_class = knob_read.getCurves(nap_file_name = nap_file_name, \\\n                                map_file_name = map_file_name, offset_value = offset_value)\n\n        for each_node in get_curve_class:\n            node_key = get_curve_class.index(each_node)\n            current_node_curve = each_node[2]\n            curent_source_node = each_node[0]\n            for each_curve in current_node_curve:\n                curve_attr\t\t= each_curve[1]\n                current_key_dict = each_curve[2]\n                time_keys \t\t= current_key_dict[\"time\"]\n                key_value\t\t= current_key_dict[\"key_value\"]\n                in_angle\t\t= current_key_dict[\"in_angle\"]\n                out_angle\t\t= current_key_dict[\"out_angle\"]\n                in_weight\t\t= current_key_dict[\"in_weight\"]\n                out_weight\t\t= current_key_dict[\"out_weight\"]\n                in_tan_type\t\t= current_key_dict[\"in_tan_type\"]\n                out_tan_type\t= current_key_dict[\"out_tan_type\"]\n                in_slope\t\t= current_key_dict[\"in_slope\"]\n                out_slope\t\t= current_key_dict[\"out_slope\"]\n                try:\n                    for time in time_keys:\n\n                        if houdini_node_attribute:\n                            curve_attr = houdini_node_attribute\n                        else:\n                            if attribute_map:\n                                temp_attr_keys = attribute_map.keys()\n                                for each_template in temp_attr_keys:\n                                    source_details = each_template.split(\".\")\n                                    current_node_attr = \"%s.%s\" % (curent_source_node, \\\n                                                                        each_curve[1])\n                                    if current_node_attr == each_template:\n                                        destenation_details = attribute_map[each_template]\\\n                                                                                .split(\".\")\n                                        curve_attr = destenation_details[1]\n                                        current_houdini_node = destenation_details[0]\n                                        current_houdini_node = hou.node(current_houdini_node)\n                                        break\n                            else:\n                                current_houdini_node = hou.node(houdini_node_list[node_key])\n                        if start_frame and end_frame:\n                            if time in range(start_frame, end_frame + 1):\n                                key_index = time_keys.index(time)\n                            else:\n                                print \"%s not in range not applying the key\" % time\n                                continue\n                        else:\n                            key_index = time_keys.index(time)\n                        in_tan_v = in_tan_type[key_index]\n                        if self.nuke_tan_types.has_key(in_tan_v):\n                            in_tan_v = self.nuke_tan_types[in_tan_v]\n                        else:\n                            in_tan_v = \"bezier()\"\n                        hkey = hou.Keyframe()\n                        hkey.setTime((time_keys[key_index]/GLOBAL_FPS))\n                        hkey.setValue(key_value[key_index])\n                        hkey.setExpression(\"bezier()\")\n                        hkey.setExpression(\"spline()\")\n                        hkey.setInAccel(in_weight[key_index])\n                        hkey.setAccel(out_weight[key_index])\n                        hkey.setInSlope(in_slope[key_index])\n                        hkey.setSlope(out_slope[key_index])\n                        this_node_attr = curve_attr\n                        if self.channel_match.has_key(curve_attr):\n                            this_node_attr = self.channel_match[curve_attr]\n                        hou_nod = current_houdini_node.parm(this_node_attr).setKeyframe(hkey)\n                except:\n                    traceback.print_exc()\n                    raise KipBaseError(\"No objects found in node list!\")\n        rodin_logger.info(\"Aniamtion curve trasfer is finished !\")\n        return True\n\ndef header(map_file_name):\n    \"\"\"\n\n    This function will return a dict of header details from map file\n\n    :param map_file_name: Filepath of napalm channel data\n\n    :type map_file_name: string\n\n    :return: header details\n\n    :rtype: dict\n\n    \"\"\"\n    if os.path.exists(map_file_name):\n        nap_header = kip_reader.header(map_file_name)\n        return nap_header\n    return None\n\n# Copyright 2008-2012 Dr D Studios Pty Limited (ACN 127 184 954) (Dr. D Studios)\n#\n# This file is part of anim-studio-tools.\n#\n# anim-studio-tools is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# anim-studio-tools is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with anim-studio-tools.  If not, see \u003chttp://www.gnu.org/licenses/\u003e.\n\n"}
{"repo_name":"dentaku65/plugin.video.italyalacarta","ref":"refs/heads/master","path":"lib/gdata/tlslite/utils/PyCrypto_RSAKey.py","content":"\"\"\"PyCrypto RSA implementation.\"\"\"\n\nfrom cryptomath import *\n\nfrom RSAKey import *\nfrom Python_RSAKey import Python_RSAKey\n\nif pycryptoLoaded:\n\n    from Crypto.PublicKey import RSA\n\n    class PyCrypto_RSAKey(RSAKey):\n        def __init__(self, n=0, e=0, d=0, p=0, q=0, dP=0, dQ=0, qInv=0):\n            if not d:\n                self.rsa = RSA.construct( (n, e) )\n            else:\n                self.rsa = RSA.construct( (n, e, d, p, q) )\n\n        def __getattr__(self, name):\n            return getattr(self.rsa, name)\n\n        def hasPrivateKey(self):\n            return self.rsa.has_private()\n\n        def hash(self):\n            return Python_RSAKey(self.n, self.e).hash()\n\n        def _rawPrivateKeyOp(self, m):\n            s = numberToString(m)\n            byteLength = numBytes(self.n)\n            if len(s)== byteLength:\n                pass\n            elif len(s) == byteLength-1:\n                s = '\\0' + s\n            else:\n                raise AssertionError()\n            c = stringToNumber(self.rsa.decrypt((s,)))\n            return c\n\n        def _rawPublicKeyOp(self, c):\n            s = numberToString(c)\n            byteLength = numBytes(self.n)\n            if len(s)== byteLength:\n                pass\n            elif len(s) == byteLength-1:\n                s = '\\0' + s\n            else:\n                raise AssertionError()\n            m = stringToNumber(self.rsa.encrypt(s, None)[0])\n            return m\n\n        def writeXMLPublicKey(self, indent=''):\n            return Python_RSAKey(self.n, self.e).write(indent)\n\n        def generate(bits):\n            key = PyCrypto_RSAKey()\n            def f(numBytes):\n                return bytesToString(getRandomBytes(numBytes))\n            key.rsa = RSA.generate(bits, f)\n            return key\n        generate = staticmethod(generate)\n"}
{"repo_name":"itsjeyd/edx-platform","ref":"refs/heads/master","path":"cms/djangoapps/contentstore/tests/test_import_draft_order.py","content":"\"\"\"\nTests Draft import order.\n\"\"\"\nfrom xmodule.modulestore.xml_importer import import_course_from_xml\n\nfrom xmodule.modulestore.tests.django_utils import ModuleStoreTestCase\nfrom xmodule.modulestore.django import modulestore\nfrom django.conf import settings\n\nTEST_DATA_DIR = settings.COMMON_TEST_DATA_ROOT\n\n\n# This test is in the CMS module because the test configuration to use a draft\n# modulestore is dependent on django.\nclass DraftReorderTestCase(ModuleStoreTestCase):\n\n    def test_order(self):\n        \"\"\"\n        Verify that drafts are imported in the correct order.\n        \"\"\"\n        store = modulestore()\n        course_items = import_course_from_xml(\n            store, self.user.id, TEST_DATA_DIR, ['import_draft_order'], create_if_not_present=True\n        )\n        course_key = course_items[0].id\n        sequential = store.get_item(course_key.make_usage_key('sequential', '0f4f7649b10141b0bdc9922dcf94515a'))\n        verticals = sequential.children\n\n        # The order that files are read in from the file system is not guaranteed (cannot rely on\n        # alphabetical ordering, for example). Therefore, I have added a lot of variation in filename and desired\n        # ordering so that the test reliably failed with the bug, at least on Linux.\n        #\n        # 'a', 'b', 'c', 'd', and 'z' are all drafts, with 'index_in_children_list' of\n        #  2 ,  4 ,  6 ,  5 , and  0  respectively.\n        #\n        # '5a05be9d59fc4bb79282c94c9e6b88c7' and 'second' are public verticals.\n        self.assertEqual(7, len(verticals))\n        self.assertEqual(course_key.make_usage_key('vertical', 'z'), verticals[0])\n        self.assertEqual(course_key.make_usage_key('vertical', '5a05be9d59fc4bb79282c94c9e6b88c7'), verticals[1])\n        self.assertEqual(course_key.make_usage_key('vertical', 'a'), verticals[2])\n        self.assertEqual(course_key.make_usage_key('vertical', 'second'), verticals[3])\n        self.assertEqual(course_key.make_usage_key('vertical', 'b'), verticals[4])\n        self.assertEqual(course_key.make_usage_key('vertical', 'd'), verticals[5])\n        self.assertEqual(course_key.make_usage_key('vertical', 'c'), verticals[6])\n\n        # Now also test that the verticals in a second sequential are correct.\n        sequential = store.get_item(course_key.make_usage_key('sequential', 'secondseq'))\n        verticals = sequential.children\n        # 'asecond' and 'zsecond' are drafts with 'index_in_children_list' 0 and 2, respectively.\n        # 'secondsubsection' is a public vertical.\n        self.assertEqual(3, len(verticals))\n        self.assertEqual(course_key.make_usage_key('vertical', 'asecond'), verticals[0])\n        self.assertEqual(course_key.make_usage_key('vertical', 'secondsubsection'), verticals[1])\n        self.assertEqual(course_key.make_usage_key('vertical', 'zsecond'), verticals[2])\n"}
{"repo_name":"Baumelbi/IntroPython2016","ref":"refs/heads/master","path":"Solutions/Session06/test_mailroom2.py","content":"#!/usr/bin/env python\n\n\"\"\"\nunit tests for the mailroom program\n\"\"\"\nimport os\n\nimport mailroom2 as mailroom\n\n# so that it's there for the tests\nmailroom.donor_db = mailroom.get_donor_db()\n\n\ndef test_list_donors():\n    listing = mailroom.list_donors()\n\n    # hard to test this throughly -- better not to hard code the entire\n    # thing. But check for a few aspects -- this will catch the likely\n    # errors\n    assert listing.startswith(\"Donor list:\\n\")\n    assert \"Jeff Bezos\" in listing\n    assert \"William Gates III\" in listing\n    assert len(listing.split('\\n')) == 5\n\n\ndef test_find_donor():\n    \"\"\" checks a donor that is there, but with odd case and spaces\"\"\"\n    donor = mailroom.find_donor(\"jefF beZos \")\n\n    assert donor[0] == \"Jeff Bezos\"\n\n\ndef test_find_donor_not():\n    \"test one that's not there\"\n    donor = mailroom.find_donor(\"Jeff Bzos\")\n\n    assert donor is None\n\n\ndef test_gen_letter():\n    \"\"\" test the donor letter \"\"\"\n\n    # create a sample donor\n    donor = (\"Fred Flintstone\", [432.45, 65.45, 230.0])\n    letter = mailroom.gen_letter(donor)\n    # what to test? tricky!\n    assert letter.startswith(\"Dear Fred Flintstone\")\n    assert letter.endswith(\"-The Team\\n\")\n    assert \"donation of $230.00\" in letter\n\n\ndef test_add_donor():\n    name = \"Fred Flintstone  \"\n\n    donor = mailroom.add_donor(name)\n    donor[1].append(300)\n    assert donor[0] == \"Fred Flintstone\"\n    assert donor[1] == [300]\n    assert mailroom.find_donor(name) == donor\n\n\ndef test_generate_donor_report():\n\n    report = mailroom.generate_donor_report()\n\n    print(report)  # printing so you can see it if it fails\n    # this is pretty tough to test\n    # these are not great, because they will fail if unimportant parts of the\n    # report are changed.\n    # but at least you know that codes working now.\n    assert report.startswith(\"Donor Name                | Total Given | Num Gifts | Average Gift\")\n\n    assert \"Jeff Bezos                  $    877.33           1   $     877.33\" in report\n\n\ndef test_save_letters_to_disk():\n    \"\"\"\n    This only tests that the files get created, but that's a start\n\n    Note that the contents of the letter was already\n    tested with test_gen_letter\n    \"\"\"\n\n    mailroom.save_letters_to_disk()\n\n    assert os.path.isfile('Jeff_Bezos.txt')\n    assert os.path.isfile('William_Gates_III.txt')\n    # check that it'snot empty:\n    with open('William_Gates_III.txt') as f:\n        size = len(f.read())\n    assert size \u003e 0\n\n\nif __name__ == \"__main__\":\n    # this is best run with a test runner, like pytest\n    # But if not, at least this will run them all.\n    test_list_donors()\n    test_find_donor()\n    test_find_donor_not()\n    test_gen_letter()\n    test_add_donor()\n    test_generate_donor_report()\n    test_save_letters_to_disk()\n    print(\"All tests Passed\")\n"}
{"repo_name":"odoousers2014/odoo","ref":"refs/heads/master","path":"addons/website_sale_delivery/models/sale_order.py","content":"# -*- coding: utf-8 -*-\n\nfrom openerp.osv import orm, fields\nfrom openerp import SUPERUSER_ID\nfrom openerp.addons import decimal_precision\n\n\nclass delivery_carrier(orm.Model):\n    _name = 'delivery.carrier'\n    _inherit = ['delivery.carrier', 'website.published.mixin']\n\n    _columns = {\n        'website_description': fields.text('Description for the website'),\n    }\n    _defaults = {\n        'website_published': True\n    }\n\n\nclass SaleOrder(orm.Model):\n    _inherit = 'sale.order'\n\n    def _amount_all_wrapper(self, cr, uid, ids, field_name, arg, context=None):        \n        \"\"\" Wrapper because of direct method passing as parameter for function fields \"\"\"\n        return self._amount_all(cr, uid, ids, field_name, arg, context=context)\n\n    def _amount_all(self, cr, uid, ids, field_name, arg, context=None):\n        res = super(SaleOrder, self)._amount_all(cr, uid, ids, field_name, arg, context=context)\n        currency_pool = self.pool.get('res.currency')\n        for order in self.browse(cr, uid, ids, context=context):\n            line_amount = sum([line.price_subtotal for line in order.order_line if line.is_delivery])\n            currency = order.pricelist_id.currency_id\n            res[order.id]['amount_delivery'] = currency_pool.round(cr, uid, currency, line_amount)\n        return res\n\n    def _get_order(self, cr, uid, ids, context=None):\n        result = {}\n        for line in self.pool.get('sale.order.line').browse(cr, uid, ids, context=context):\n            result[line.order_id.id] = True\n        return result.keys()\n\n    _columns = {\n        'amount_delivery': fields.function(\n            _amount_all_wrapper, type='float', digits_compute=decimal_precision.get_precision('Account'),\n            string='Delivery Amount',\n            store={\n                'sale.order': (lambda self, cr, uid, ids, c={}: ids, ['order_line'], 10),\n                'sale.order.line': (_get_order, ['price_unit', 'tax_id', 'discount', 'product_uom_qty'], 10),\n            },\n            multi='sums', help=\"The amount without tax.\", track_visibility='always'\n        ),\n        'website_order_line': fields.one2many(\n            'sale.order.line', 'order_id',\n            string='Order Lines displayed on Website', readonly=True,\n            domain=[('is_delivery', '=', False)],\n            help='Order Lines to be displayed on the website. They should not be used for computation purpose.',\n        ),\n    }\n\n    def _check_carrier_quotation(self, cr, uid, order, force_carrier_id=None, context=None):\n        carrier_obj = self.pool.get('delivery.carrier')\n\n        # check to add or remove carrier_id\n        if not order:\n            return False\n        if all(line.product_id.type == \"service\" for line in order.website_order_line):\n            order.write({'carrier_id': None})\n            self.pool['sale.order']._delivery_unset(cr, SUPERUSER_ID, [order.id], context=context)\n            return True\n        else: \n            carrier_id = force_carrier_id or order.carrier_id.id\n            carrier_ids = self._get_delivery_methods(cr, uid, order, context=context)\n            if carrier_id:\n                if carrier_id not in carrier_ids:\n                    carrier_id = False\n                else:\n                    carrier_ids.remove(carrier_id)\n                    carrier_ids.insert(0, carrier_id)\n            if force_carrier_id or not carrier_id or not carrier_id in carrier_ids:\n                for delivery_id in carrier_ids:\n                    grid_id = carrier_obj.grid_get(cr, SUPERUSER_ID, [delivery_id], order.partner_shipping_id.id)\n                    if grid_id:\n                        carrier_id = delivery_id\n                        break\n                order.write({'carrier_id': carrier_id})\n            if carrier_id:\n                order.delivery_set()\n            else:\n                order._delivery_unset()                    \n\n        return bool(carrier_id)\n\n    def _get_delivery_methods(self, cr, uid, order, context=None):\n        carrier_obj = self.pool.get('delivery.carrier')\n        delivery_ids = carrier_obj.search(cr, uid, [('website_published','=',True)], context=context)\n        # Following loop is done to avoid displaying delivery methods who are not available for this order\n        # This can surely be done in a more efficient way, but at the moment, it mimics the way it's\n        # done in delivery_set method of sale.py, from delivery module\n        for delivery_id in carrier_obj.browse(cr, SUPERUSER_ID, delivery_ids, context=dict(context, order_id=order.id)):\n            if not delivery_id.available:\n                delivery_ids.remove(delivery_id.id)\n        return delivery_ids\n\n    def _get_errors(self, cr, uid, order, context=None):\n        errors = super(SaleOrder, self)._get_errors(cr, uid, order, context=context)\n        if not self._get_delivery_methods(cr, uid, order, context=context):\n            errors.append(('No delivery method available', 'There is no available delivery method for your order'))            \n        return errors\n\n    def _get_website_data(self, cr, uid, order, context=None):\n        \"\"\" Override to add delivery-related website data. \"\"\"\n        values = super(SaleOrder, self)._get_website_data(cr, uid, order, context=context)\n        # We need a delivery only if we have stockable products\n        has_stockable_products = False\n        for line in order.order_line:\n            if line.product_id.type in ('consu', 'product'):\n                has_stockable_products = True\n        if not has_stockable_products:\n            return values\n\n        delivery_ctx = dict(context, order_id=order.id)\n        DeliveryCarrier = self.pool.get('delivery.carrier')\n        delivery_ids = self._get_delivery_methods(cr, uid, order, context=context)\n\n        values['deliveries'] = DeliveryCarrier.browse(cr, SUPERUSER_ID, delivery_ids, context=delivery_ctx)\n        return values\n"}
{"repo_name":"bollu/sandhi","ref":"refs/heads/master","path":"modules/gr36/grc/gui/Connection.py","content":"\"\"\"\nCopyright 2007, 2008, 2009 Free Software Foundation, Inc.\nThis file is part of GNU Radio\n\nGNU Radio Companion is free software; you can redistribute it and/or\nmodify it under the terms of the GNU General Public License\nas published by the Free Software Foundation; either version 2\nof the License, or (at your option) any later version.\n\nGNU Radio Companion is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\n\"\"\"\n\nimport Utils\nfrom Element import Element\nimport Colors\nfrom Constants import CONNECTOR_ARROW_BASE, CONNECTOR_ARROW_HEIGHT\n\nclass Connection(Element):\n\t\"\"\"\n\tA graphical connection for ports.\n\tThe connection has 2 parts, the arrow and the wire.\n\tThe coloring of the arrow and wire exposes the status of 3 states:\n\t\tenabled/disabled, valid/invalid, highlighted/non-highlighted.\n\tThe wire coloring exposes the enabled and highlighted states.\n\tThe arrow coloring exposes the enabled and valid states.\n\t\"\"\"\n\n\tdef __init__(self): Element.__init__(self)\n\n\tdef get_coordinate(self):\n\t\t\"\"\"\n\t\tGet the 0,0 coordinate.\n\t\tCoordinates are irrelevant in connection.\n\t\t@return 0, 0\n\t\t\"\"\"\n\t\treturn (0, 0)\n\n\tdef get_rotation(self):\n\t\t\"\"\"\n\t\tGet the 0 degree rotation.\n\t\tRotations are irrelevant in connection.\n\t\t@return 0\n\t\t\"\"\"\n\t\treturn 0\n\n\tdef create_shapes(self):\n\t\t\"\"\"Precalculate relative coordinates.\"\"\"\n\t\tElement.create_shapes(self)\n\t\tself._sink_rot = None\n\t\tself._source_rot = None\n\t\tself._sink_coor = None\n\t\tself._source_coor = None\n\t\t#get the source coordinate\n\t\tconnector_length = self.get_source().get_connector_length()\n\t\tself.x1, self.y1 = Utils.get_rotated_coordinate((connector_length, 0), self.get_source().get_rotation())\n\t\t#get the sink coordinate\n\t\tconnector_length = self.get_sink().get_connector_length() + CONNECTOR_ARROW_HEIGHT\n\t\tself.x2, self.y2 = Utils.get_rotated_coordinate((-connector_length, 0), self.get_sink().get_rotation())\n\t\t#build the arrow\n\t\tself.arrow = [(0, 0),\n\t\t\tUtils.get_rotated_coordinate((-CONNECTOR_ARROW_HEIGHT, -CONNECTOR_ARROW_BASE/2), self.get_sink().get_rotation()),\n\t\t\tUtils.get_rotated_coordinate((-CONNECTOR_ARROW_HEIGHT, CONNECTOR_ARROW_BASE/2), self.get_sink().get_rotation()),\n\t\t]\n\t\tself._update_after_move()\n\t\tif not self.get_enabled(): self._arrow_color = Colors.CONNECTION_DISABLED_COLOR\n\t\telif not self.is_valid(): self._arrow_color = Colors.CONNECTION_ERROR_COLOR\n\t\telse: self._arrow_color = Colors.CONNECTION_ENABLED_COLOR\n\n\tdef _update_after_move(self):\n\t\t\"\"\"Calculate coordinates.\"\"\"\n\t\tself.clear() #FIXME do i want this here?\n\t\t#source connector\n\t\tsource = self.get_source()\n\t\tX, Y = source.get_connector_coordinate()\n\t\tx1, y1 = self.x1 + X, self.y1 + Y\n\t\tself.add_line((x1, y1), (X, Y))\n\t\t#sink connector\n\t\tsink = self.get_sink()\n\t\tX, Y = sink.get_connector_coordinate()\n\t\tx2, y2 = self.x2 + X, self.y2 + Y\n\t\tself.add_line((x2, y2), (X, Y))\n\t\t#adjust arrow\n\t\tself._arrow = [(x+X, y+Y) for x,y in self.arrow]\n\t\t#add the horizontal and vertical lines in this connection\n\t\tif abs(source.get_connector_direction() - sink.get_connector_direction()) == 180:\n\t\t\t#2 possible point sets to create a 3-line connector\n\t\t\tmid_x, mid_y = (x1 + x2)/2.0, (y1 + y2)/2.0\n\t\t\tpoints = [((mid_x, y1), (mid_x, y2)), ((x1, mid_y), (x2, mid_y))]\n\t\t\t#source connector -\u003e points[0][0] should be in the direction of source (if possible)\n\t\t\tif Utils.get_angle_from_coordinates((x1, y1), points[0][0]) != source.get_connector_direction(): points.reverse()\n\t\t\t#points[0][0] -\u003e sink connector should not be in the direction of sink\n\t\t\tif Utils.get_angle_from_coordinates(points[0][0], (x2, y2)) == sink.get_connector_direction(): points.reverse()\n\t\t\t#points[0][0] -\u003e source connector should not be in the direction of source\n\t\t\tif Utils.get_angle_from_coordinates(points[0][0], (x1, y1)) == source.get_connector_direction(): points.reverse()\n\t\t\t#create 3-line connector\n\t\t\tp1, p2 = map(int, points[0][0]), map(int, points[0][1])\n\t\t\tself.add_line((x1, y1), p1)\n\t\t\tself.add_line(p1, p2)\n\t\t\tself.add_line((x2, y2), p2)\n\t\telse:\n\t\t\t#2 possible points to create a right-angled connector\n\t\t\tpoints = [(x1, y2), (x2, y1)]\n\t\t\t#source connector -\u003e points[0] should be in the direction of source (if possible)\n\t\t\tif Utils.get_angle_from_coordinates((x1, y1), points[0]) != source.get_connector_direction(): points.reverse()\n\t\t\t#points[0] -\u003e sink connector should not be in the direction of sink\n\t\t\tif Utils.get_angle_from_coordinates(points[0], (x2, y2)) == sink.get_connector_direction(): points.reverse()\n\t\t\t#points[0] -\u003e source connector should not be in the direction of source\n\t\t\tif Utils.get_angle_from_coordinates(points[0], (x1, y1)) == source.get_connector_direction(): points.reverse()\n\t\t\t#create right-angled connector\n\t\t\tself.add_line((x1, y1), points[0])\n\t\t\tself.add_line((x2, y2), points[0])\n\n\tdef draw(self, gc, window):\n\t\t\"\"\"\n\t\tDraw the connection.\n\t\t@param gc the graphics context\n\t\t@param window the gtk window to draw on\n\t\t\"\"\"\n\t\tsink = self.get_sink()\n\t\tsource = self.get_source()\n\t\t#check for changes\n\t\tif self._sink_rot != sink.get_rotation() or self._source_rot != source.get_rotation(): self.create_shapes()\n\t\telif self._sink_coor != sink.get_coordinate() or self._source_coor != source.get_coordinate(): self._update_after_move()\n\t\t#cache values\n\t\tself._sink_rot = sink.get_rotation()\n\t\tself._source_rot = source.get_rotation()\n\t\tself._sink_coor = sink.get_coordinate()\n\t\tself._source_coor = source.get_coordinate()\n\t\t#draw\n\t\tif self.is_highlighted(): border_color = Colors.HIGHLIGHT_COLOR\n\t\telif self.get_enabled(): border_color = Colors.CONNECTION_ENABLED_COLOR\n\t\telse: border_color = Colors.CONNECTION_DISABLED_COLOR\n\t\tElement.draw(self, gc, window, bg_color=None, border_color=border_color)\n\t\t#draw arrow on sink port\n\t\tgc.set_foreground(self._arrow_color)\n\t\twindow.draw_polygon(gc, True, self._arrow)\n"}
{"repo_name":"agabrown/PyGaia","ref":"refs/heads/master","path":"pygaia/utils.py","content":"__all__ = ['enum', 'degreesToRadians', 'radiansToDegrees']\n\nimport numpy as np\n\nfrom pygaia.astrometry.constants import auKmYearPerSec\n\ndef enum(typename, field_names):\n    \"\"\"\n    Create a new enumeration type.\n  \n    Code is copyright (c) Gabriel Genellina, 2010, MIT License.\n\n    Parameters\n    ----------\n\n    typename - Name of the enumerated type\n    field_names - Names of the fields of the enumerated type\n    \"\"\"\n\n    if isinstance(field_names, str):\n        field_names = field_names.replace(',', ' ').split()\n    d = dict((reversed(nv) for nv in enumerate(field_names)), __slots__ = ())\n    return type(typename, (object,), d)()\n\ndef degreesToRadians(angle):\n    \"\"\"\n    Convert from degrees to radians.\n\n    Parameters\n    ----------\n\n    angle - angle in degrees\n\n    Returns\n    -------\n\n    Angle in radians.\n    \"\"\"\n    return angle/180.0*np.pi\n\ndef radiansToDegrees(angle):\n    \"\"\"\n    Convert from radians to degrees.\n\n    Parameters\n    ----------\n\n    angle - angle in radians.\n\n    Returns\n    -------\n \n    Angle in degrees.\n    \"\"\"\n    return angle/np.pi*180.0\n\ndef construct_covariance_matrix(cvec, parallax, radial_velocity, radial_velocity_error):\n    \"\"\"\n    Take the astrometric parameter standard uncertainties and the uncertainty correlations as quoted in\n    the Gaia catalogue and construct the covariance matrix.\n\n    Parameters\n    ----------\n\n    cvec : array_like\n        Array of shape (15,) (1 source) or (n,15) (n sources) for the astrometric parameter standard\n        uncertainties and their correlations, as listed in the Gaia catalogue [ra_error, dec_error,\n        parallax_error, pmra_error, pmdec_error, ra_dec_corr, ra_parallax_corr, ra_pmra_corr,\n        ra_pmdec_corr, dec_parallax_corr, dec_pmra_corr, dec_pmdec_corr, parallax_pmra_corr,\n        parallax_pmdec_corr, pmra_pmdec_corr]. Units are (mas^2, mas^2/yr, mas^2/yr^2).\n    \n    parallax : array_like (n elements)\n        Source parallax (mas).\n    \n    radial_velocity : array_like (n elements)\n        Source radial velocity (km/s, does not have to be from Gaia RVS!). If the radial velocity is not\n        known it can be set to zero.\n\n    radial_velocity_error : array_like (n elements)\n        Source radial velocity  uncertainty (km/s). If the radial velocity is not know this can be set to\n        the radial velocity dispersion for the population the source was drawn from.\n\n    Returns\n    -------\n\n    Covariance matrix as a 6x6 array.\n    \"\"\"\n\n    if np.ndim(cvec)==1:\n        cmat = np.zeros((1,6,6))\n        nsources = 1\n        cv = np.atleast_2d(cvec)\n    else:\n        nsources = cvec.shape[0]\n        cmat = np.zeros((nsources,6,6))\n        cv = cvec\n    for k in range(nsources):\n        cmat[k,0:5,0:5] = cv[k,0:5]**2\n\n    iu = np.triu_indices(5,k=1)\n    for k in range(10):\n        i = iu[0][k]\n        j = iu[1][k]\n        cmat[:,i,j] = cv[:,i]*cv[:,j]*cv[:,k+5]\n        cmat[:,j,i] = cmat[:,i,j]\n\n    for k in range(nsources):\n        cmat[k,0:5,5] = cmat[k,0:5,2]*np.atleast_1d(radial_velocity)[k]/auKmYearPerSec\n    cmat[:,5,0:5] = cmat[:,0:5,5]\n    cmat[:,5,5] = cmat[:,2,2]*(radial_velocity**2 + radial_velocity_error**2)/auKmYearPerSec**2 + \\\n            (parallax*radial_velocity_error/auKmYearPerSec)**2\n\n    return np.squeeze(cmat)\n"}
{"repo_name":"brandonivey/django-marimo","ref":"refs/heads/master","path":"marimo/templatetags/writecapture.py","content":"import json\nimport random\n\nfrom django import template\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nregister = template.Library()\n\ndef jsescape(string):\n    \"\"\" escaping so that javascript can be safely put into json dicts\n        for some reason json newline escaping isn't enough??\n    \"\"\"\n    return string.replace('\u003cscript','$BEGINSCRIPT').replace('\u003c/script\u003e','$ENDSCRIPT').replace('\\n', '$NEWLINE').replace('\\r','')\n\n@register.tag(name='writecapture')\ndef write_capture(parser, token):\n    \"\"\"\n        Syntax::\n            {% writecapture [filter] [\"prototype\"] [\"widget_id\"] %}\n                \u003cscript src=\"evil.js\"\u003e\n                    document.write('this is evil')\n                \u003cscript\u003e\n            {% endwritecapture %}\n\n        Wraps the enclosed HTML inside of a marimo writecapture widget.\n\n        The ``filter`` argument is a boolean (default False) that turns on a\n        writecapture feature called writeOnGetElementById. This fixes some\n        extra-bad scripts.\n\n        The ``prototype`` argument defaults to 'writecapture.' You will only\n        need to use this if you have subclassed marimo's built-in writecapture\n        widget and want to use that instead.\n\n        The ``widget_id`` argument defaults to a 'writecapture_\u003crandomnumber\u003e.'\n        Use this only if you need to specify an alternate element id in the DOM\n        to write to (otherwise one will be created for you at the site of the\n        {%writecapture%} invocation)..\n\n    \"\"\"\n    # TODO should work with marimo fast and widget_id should be resolved maybe\n    tokens = token.split_contents()\n    if len(tokens) \u003e 4:\n        raise template.TemplateSyntaxError(\"writecapture block takes at most 3 arguments\")\n    nodelist = parser.parse(('endwritecapture',))\n    parser.delete_first_token()\n\n    if len(tokens) \u003e 1:\n        script_filter = tokens[1]\n        if script_filter == 'False':\n            script_filter = False\n        elif script_filter == 'True':\n            script_filter = True\n        else:\n            script_filter = template.Variable(script_filter)\n    else:\n        script_filter = False\n\n    return WriteCaptureNode(nodelist, script_filter, *tokens[2:])\n\nclass WriteCaptureNode(template.Node):\n    def __init__(self, nodelist, script_filter=False, prototype='writecapture_widget', widget_id=None):\n        self.nodelist = nodelist\n        self.script_filter = script_filter\n        self.prototype = prototype\n        self.widget_id = widget_id\n        if not self.widget_id:\n            self.widget_id = 'writecapture' + str(random.randint(0,99999999))\n\n    def render(self, context):\n        eviloutput = jsescape(self.nodelist.render(context))\n        if isinstance(self.script_filter, template.Variable):\n            self.script_filter = bool(self.script_filter.resolve(context))\n        # Set this flag in your template tag for advanced write capture widget sanitation.\n        # Source: https://github.com/iamnoah/writeCapture/wiki/Usage\n\n        global_compatibility_mode = context.get('wc_compatibility_mode', None)\n        if global_compatibility_mode is None:\n            wc_compatibility_mode = self.script_filter\n        else:\n            wc_compatibility_mode = global_compatibility_mode\n\n        widget_dict = dict(widget_prototype=self.prototype,\n                            id=self.widget_id,\n                            html=eviloutput,\n                            wc_compatibility_mode = wc_compatibility_mode,\n                         )\n        output = \"\"\"\u003cdiv id=\"{widget_id}\"\u003e\u003c/div\u003e\n\u003cscript type=\"text/javascript\"\u003e\n    marimo.emit('{widget_id}_ready');\n    marimo.add_widget({widget_json});\n\u003c/script\u003e\"\"\"\n        output = output.format(\n            widget_id=self.widget_id,\n            widget_json=json.dumps(widget_dict),\n        )\n        return output\n\n@register.tag(name='writecapture_delay')\ndef write_capture_delay(parser, token):\n    \"\"\"\n        Syntax::\n            {% writecapture_delay [event_name] %}\n    \"\"\"\n    tokens = token.split_contents()\n    if len(tokens) \u003e 2:\n        raise template.TemplateSyntaxError(\"writecapture_delay takes at most 1 argument\")\n    if len(tokens) == 2:\n        return WriteCaptureDelayNode(tokens[1])\n    return WriteCaptureDelayNode()\n\nclass WriteCaptureDelayNode(template.Node):\n    def __init__(self, event=None):\n        self.event = event\n\n    def render(self, context):\n        output = ''\n        if self.event is None:\n            self.event = 'write_' + str(random.randint(0,999999))\n            output = \"\"\"\u003cscript type=\"text/javascript\"\u003emarimo.emit('%s');\u003c/script\u003e\"\"\" % self.event\n\n        # this should only be used once per page if it's uses a second time\n        # overwrite but log an error\n        wc_delay = context.get('marimo_writecapture_delay', None)\n        if not wc_delay:\n            logger.error(\"The writecapture_delay was called but didn't find \"\n                         \"marimo_writecapture_delay in the context. The tag \"\n                         \"depends on the Marimo middleware and context_processor.\")\n            return output\n        if wc_delay.marimo_event:\n            logger.error('Overwriting the marimo event delay %s with %s' %\n                         (wc_delay.marimo_event, self.event))\n        wc_delay.marimo_event = self.event\n        return output\n\n@register.tag(name='writecapture_delay')\ndef write_capture_delay(parser, token):\n    \"\"\"\n        Syntax::\n            {% writecapture_delay [event_name] %}\n    \"\"\"\n    tokens = token.split_contents()\n    if len(tokens) \u003e 2:\n        raise template.TemplateSyntaxError(\"writecapture_delay takes at most 1 argument\")\n    if len(tokens) == 2:\n        return WriteCaptureDelayNode(tokens[1])\n    return WriteCaptureDelayNode()\n\nclass WriteCaptureDelayNode(template.Node):\n    def __init__(self, event=None):\n        self.event = event\n\n    def render(self, context):\n        output = ''\n        if self.event is None:\n            self.event = 'write_' + str(random.randint(0,999999))\n            output = \"\"\"\u003cscript type=\"text/javascript\"\u003emarimo.emit('%s');\u003c/script\u003e\"\"\" % self.event\n\n        # this should only be used once per page if it's uses a second time\n        # overwrite but log an error\n        wc_delay = context.get('marimo_writecapture_delay', None)\n        if not wc_delay:\n            logger.error(\"The writecapture_delay was called but didn't find \"\n                         \"marimo_writecapture_delay in the context. The tag \"\n                         \"depends on the Marimo middleware and context_processor.\")\n            return output\n        if wc_delay.marimo_event:\n            logger.error('Overwriting the marimo event delay %s with %s' %\n                         (wc_delay.marimo_event, self.event))\n        wc_delay.marimo_event = self.event\n        return output\n\n@register.tag(name='writecapture_delay')\ndef write_capture_delay(parser, token):\n    \"\"\"\n        Syntax::\n            {% writecapture_delay [event_name] %}\n    \"\"\"\n    tokens = token.split_contents()\n    if len(tokens) \u003e 2:\n        raise template.TemplateSyntaxError(\"writecapture_delay takes at most 1 argument\")\n    if len(tokens) == 2:\n        return WriteCaptureDelayNode(tokens[1])\n    return WriteCaptureDelayNode()\n\nclass WriteCaptureDelayNode(template.Node):\n    def __init__(self, event=None):\n        self.event = event\n\n    def render(self, context):\n        output = ''\n        if self.event is None:\n            self.event = 'write_' + str(random.randint(0,999999))\n            output = \"\"\"\u003cscript type=\"text/javascript\"\u003emarimo.emit('%s');\u003c/script\u003e\"\"\" % self.event\n\n        # this should only be used once per page if it's uses a second time\n        # overwrite but log an error\n        wc_delay = context.get('marimo_writecapture_delay', None)\n        if not wc_delay:\n            logger.error(\"The writecapture_delay was called but didn't find \"\n                         \"marimo_writecapture_delay in the context. The tag \"\n                         \"depends on the Marimo middleware and context_processor.\")\n            return output\n        if wc_delay.marimo_event:\n            logger.error('Overwriting the marimo event delay %s with %s' %\n                         (wc_delay.marimo_event, self.event))\n        wc_delay.marimo_event = self.event\n        return output\n"}
{"repo_name":"jonathonwalz/ansible","ref":"refs/heads/devel","path":"test/units/modules/network/f5/test_bigip_iapp_service.py","content":"# -*- coding: utf-8 -*-\n#\n# Copyright 2017 F5 Networks Inc.\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see \u003chttp://www.gnu.org/licenses/\u003e.\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport os\nimport json\nimport sys\n\nfrom nose.plugins.skip import SkipTest\nif sys.version_info \u003c (2, 7):\n    raise SkipTest(\"F5 Ansible modules require Python \u003e= 2.7\")\n\nfrom ansible.compat.tests import unittest\nfrom ansible.compat.tests.mock import patch, Mock\nfrom ansible.module_utils import basic\nfrom ansible.module_utils._text import to_bytes\nfrom ansible.module_utils.f5_utils import AnsibleF5Client\n\ntry:\n    from library.bigip_iapp_service import Parameters\n    from library.bigip_iapp_service import ModuleManager\n    from library.bigip_iapp_service import ArgumentSpec\nexcept ImportError:\n    try:\n        from ansible.modules.network.f5.bigip_iapp_service import Parameters\n        from ansible.modules.network.f5.bigip_iapp_service import ModuleManager\n        from ansible.modules.network.f5.bigip_iapp_service import ArgumentSpec\n    except ImportError:\n        raise SkipTest(\"F5 Ansible modules require the f5-sdk Python library\")\n\nfixture_path = os.path.join(os.path.dirname(__file__), 'fixtures')\nfixture_data = {}\n\n\ndef set_module_args(args):\n    args = json.dumps({'ANSIBLE_MODULE_ARGS': args})\n    basic._ANSIBLE_ARGS = to_bytes(args)\n\n\ndef load_fixture(name):\n    path = os.path.join(fixture_path, name)\n\n    if path in fixture_data:\n        return fixture_data[path]\n\n    with open(path) as f:\n        data = f.read()\n\n    try:\n        data = json.loads(data)\n    except Exception:\n        pass\n\n    fixture_data[path] = data\n    return data\n\n\nclass TestParameters(unittest.TestCase):\n\n    def test_module_parameters_keys(self):\n        args = load_fixture('create_iapp_service_parameters_f5_http.json')\n        p = Parameters(args)\n\n        # Assert the top-level keys\n        assert p.name == 'http_example'\n        assert p.partition == 'Common'\n        assert p.template == '/Common/f5.http'\n\n    def test_module_parameters_lists(self):\n        args = load_fixture('create_iapp_service_parameters_f5_http.json')\n        p = Parameters(args)\n\n        assert 'lists' in p._values\n\n        assert p.lists[0]['name'] == 'irules__irules'\n        assert p.lists[0]['encrypted'] == 'no'\n        assert len(p.lists[0]['value']) == 1\n        assert p.lists[0]['value'][0] == '/Common/lgyft'\n\n        assert p.lists[1]['name'] == 'net__client_vlan'\n        assert p.lists[1]['encrypted'] == 'no'\n        assert len(p.lists[1]['value']) == 1\n        assert p.lists[1]['value'][0] == '/Common/net2'\n\n    def test_module_parameters_tables(self):\n        args = load_fixture('create_iapp_service_parameters_f5_http.json')\n        p = Parameters(args)\n\n        assert 'tables' in p._values\n\n        assert 'columnNames' in p.tables[0]\n        assert len(p.tables[0]['columnNames']) == 1\n        assert p.tables[0]['columnNames'][0] == 'name'\n\n        assert 'name' in p.tables[0]\n        assert p.tables[0]['name'] == 'pool__hosts'\n\n        assert 'rows' in p.tables[0]\n        assert len(p.tables[0]['rows']) == 1\n        assert 'row' in p.tables[0]['rows'][0]\n        assert len(p.tables[0]['rows'][0]['row']) == 1\n        assert p.tables[0]['rows'][0]['row'][0] == 'demo.example.com'\n\n        assert len(p.tables[1]['rows']) == 2\n        assert 'row' in p.tables[0]['rows'][0]\n        assert len(p.tables[1]['rows'][0]['row']) == 2\n        assert p.tables[1]['rows'][0]['row'][0] == '10.1.1.1'\n        assert p.tables[1]['rows'][0]['row'][1] == '0'\n        assert p.tables[1]['rows'][1]['row'][0] == '10.1.1.2'\n        assert p.tables[1]['rows'][1]['row'][1] == '0'\n\n    def test_module_parameters_variables(self):\n        args = load_fixture('create_iapp_service_parameters_f5_http.json')\n        p = Parameters(args)\n\n        assert 'variables' in p._values\n        assert len(p.variables) == 34\n\n        # Assert one configuration value\n        assert 'name' in p.variables[0]\n        assert 'value' in p.variables[0]\n        assert p.variables[0]['name'] == 'afm__dos_security_profile'\n        assert p.variables[0]['value'] == '/#do_not_use#'\n\n        # Assert a second configuration value\n        assert 'name' in p.variables[1]\n        assert 'value' in p.variables[1]\n        assert p.variables[1]['name'] == 'afm__policy'\n        assert p.variables[1]['value'] == '/#do_not_use#'\n\n    def test_api_parameters_variables(self):\n        args = dict(\n            variables=[\n                dict(\n                    name=\"client__http_compression\",\n                    encrypted=\"no\",\n                    value=\"/#create_new#\"\n                )\n            ]\n        )\n        p = Parameters(args)\n        assert p.variables[0]['name'] == 'client__http_compression'\n\n    def test_api_parameters_tables(self):\n        args = dict(\n            tables=[\n                {\n                    \"name\": \"pool__members\",\n                    \"columnNames\": [\n                        \"addr\",\n                        \"port\",\n                        \"connection_limit\"\n                    ],\n                    \"rows\": [\n                        {\n                            \"row\": [\n                                \"12.12.12.12\",\n                                \"80\",\n                                \"0\"\n                            ]\n                        },\n                        {\n                            \"row\": [\n                                \"13.13.13.13\",\n                                \"443\",\n                                10\n                            ]\n                        }\n                    ]\n                }\n            ]\n        )\n        p = Parameters(args)\n        assert p.tables[0]['name'] == 'pool__members'\n        assert p.tables[0]['columnNames'] == ['addr', 'port', 'connection_limit']\n        assert len(p.tables[0]['rows']) == 2\n        assert 'row' in p.tables[0]['rows'][0]\n        assert 'row' in p.tables[0]['rows'][1]\n        assert p.tables[0]['rows'][0]['row'] == ['12.12.12.12', '80', '0']\n        assert p.tables[0]['rows'][1]['row'] == ['13.13.13.13', '443', '10']\n\n    def test_module_template_same_partition(self):\n        args = dict(\n            template='foo',\n            partition='bar'\n        )\n        p = Parameters(args)\n        assert p.template == '/bar/foo'\n\n    def test_module_template_same_partition_full_path(self):\n        args = dict(\n            template='/bar/foo',\n            partition='bar'\n        )\n        p = Parameters(args)\n        assert p.template == '/bar/foo'\n\n    def test_module_template_different_partition_full_path(self):\n        args = dict(\n            template='/Common/foo',\n            partition='bar'\n        )\n        p = Parameters(args)\n        assert p.template == '/Common/foo'\n\n\n@patch('ansible.module_utils.f5_utils.AnsibleF5Client._get_mgmt_root',\n       return_value=True)\nclass TestManager(unittest.TestCase):\n\n    def setUp(self):\n        self.spec = ArgumentSpec()\n\n    def test_create_service(self, *args):\n        parameters = load_fixture('create_iapp_service_parameters_f5_http.json')\n        set_module_args(dict(\n            name='foo',\n            template='f5.http',\n            parameters=parameters,\n            state='present',\n            password='passsword',\n            server='localhost',\n            user='admin'\n        ))\n\n        client = AnsibleF5Client(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode,\n            f5_product_name=self.spec.f5_product_name\n        )\n        mm = ModuleManager(client)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(return_value=False)\n        mm.create_on_device = Mock(return_value=True)\n\n        results = mm.exec_module()\n        assert results['changed'] is True\n\n    def test_update_agent_status_traps(self, *args):\n        parameters = load_fixture('update_iapp_service_parameters_f5_http.json')\n        set_module_args(dict(\n            name='foo',\n            template='f5.http',\n            parameters=parameters,\n            state='present',\n            password='passsword',\n            server='localhost',\n            user='admin'\n        ))\n\n        # Configure the parameters that would be returned by querying the\n        # remote device\n        parameters = load_fixture('create_iapp_service_parameters_f5_http.json')\n        current = Parameters(parameters)\n\n        client = AnsibleF5Client(\n            argument_spec=self.spec.argument_spec,\n            supports_check_mode=self.spec.supports_check_mode,\n            f5_product_name=self.spec.f5_product_name\n        )\n        mm = ModuleManager(client)\n\n        # Override methods to force specific logic in the module to happen\n        mm.exists = Mock(return_value=True)\n        mm.update_on_device = Mock(return_value=True)\n        mm.read_current_from_device = Mock(return_value=current)\n\n        results = mm.exec_module()\n        assert results['changed'] is True\n"}
{"repo_name":"eaglexmw/seascope","ref":"refs/heads/master","path":"src/view/filecontext/plugins/ctags_view/CtagsManager.py","content":"# Copyright (c) 2010 Anil Kumar\n# All rights reserved.\n#\n# License: BSD \n\nimport subprocess\nimport re, os\n\ndef _eintr_retry_call(func, *args):\n\twhile True:\n\t\ttry:\n\t\t\treturn func(*args)\n\t\texcept OSError, e:\n\t\t\tif e.errno == errno.EINTR:\n\t\t\t\tcontinue\n\t\t\traise\n\ndef cmdForFile(f):\n\tsuffix_cmd_map = []\n\tcustom_map = os.getenv('SEASCOPE_CTAGS_SUFFIX_CMD_MAP')\n\tif custom_map:\n\t\tcustom_map = eval(custom_map)\n\t\tsuffix_cmd_map += custom_map\n\t#args = 'ctags -n -u --fields=+K -f - --extra=+q'\n\t#args = 'ctags -n -u --fields=+Ki -f -'\n\targs = 'ctags -n -u --fields=+K -f -'\n\tsuffix_cmd_map.append( ['', args] )\n\tfor (suffix, cmd) in suffix_cmd_map:\n\t\tif f.endswith(suffix):\n\t\t\treturn cmd\n\treturn None\n\ndef ct_query(filename):\n\targs = cmdForFile(filename)\n\targs = args.split()\n\targs.append(filename)\n\ttry:\n\t\tproc = subprocess.Popen(args, stdout=subprocess.PIPE)\n\t\t(out_data, err_data) = _eintr_retry_call(proc.communicate)\n\t\tout_data = out_data.split('\\n')\n\texcept Exception as e:\n\t\tout_data =  [\n\t\t\t\t'Failed to run ctags cmd\\tignore\\t0;\\t ',\n\t\t\t\t'cmd: %s\\tignore\\t0;\\t ' % ' '.join(args),\n\t\t\t\t'error: %s\\tignore\\t0;\\t ' % str(e),\n\t\t\t\t'ctags not installed ?\\tignore\\t0;\\t ',\n\t\t\t]\n\tres = []\n\tfor line in out_data:\n\t\tif (line == ''):\n\t\t\tbreak\n\t\tline = line.split('\\t')\n\t\tnum = line[2].split(';', 1)[0]\n\t\tline = [line[0], num, line[3]]\n\t\tres.append(line)\n\treturn res\n\nis_OrderedDict_available = False\ntry:\n\t# OrderedDict available only in python \u003e= 2.7\n\tfrom collections import OrderedDict\n\tis_OrderedDict_available = True\nexcept:\n\tpass\n\ndef emptyOrderedDict():\n\tif is_OrderedDict_available:\n\t\treturn OrderedDict({})\n\treturn {}\n\nclass CtagsTreeBuilder:\n\tdef __init__(self):\n\t\tself.symTree = emptyOrderedDict()\n\n\tdef cmdForFile(self, f):\n\t\tsuffix_cmd_map = []\n\t\tcustom_map = os.getenv('SEASCOPE_CTAGS_SUFFIX_CMD_MAP')\n\t\tif custom_map:\n\t\t\tcustom_map = eval(custom_map)\n\t\t\tsuffix_cmd_map += custom_map\n\t\t#args = 'ctags -n -u --fields=+K -f - --extra=+q'\n\t\t#args = 'ctags -n -u --fields=+Ki -f -'\n\t\targs = 'ctags -n -u --fields=+K-f-t -f -'\n\t\tsuffix_cmd_map.append( ['', args] )\n\t\tfor (suffix, cmd) in suffix_cmd_map:\n\t\t\tif f.endswith(suffix):\n\t\t\t\treturn cmd\n\t\treturn None\n\n\tdef runCtags(self, f):\n\t\targs = self.cmdForFile(f)\n\t\targs = args.split()\n\t\targs.append(f)\n\t\t# In python \u003e= 2.7 can use subprocess.check_output\n\t\t# output = subprocess.check_output(args)\n\t\t# return output\n\t\tproc = subprocess.Popen(args, stdout=subprocess.PIPE)\n\t\t(out_data, err_data) = proc.communicate()\n\t\treturn out_data\n\n\tdef parseCtagsOutput(self, data):\n\t\tdata = re.split('\\r?\\n', data)\n\t\tres = []\n\t\tfor line in data:\n\t\t\tif line == '':\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tline = line.split('\\t', 4)\n\t\t\t\tres.append(line)\n\t\t\texcept:\n\t\t\t\tprint 'bad line:', line\n\t\treturn res\n\n\n\tdef addToSymLayout(self, sc):\n\t\tt = self.symTree\n\t\tif sc and sc != '':\n\t\t\tfor s in re.split('::|\\.', sc):\n\t\t\t\tif s not in t:\n\t\t\t\t\tt[s] = emptyOrderedDict()\n\t\t\t\tt = t[s]\n\n\tdef addToSymTree(self, sc, line):\n\t\tt = self.symTree\n\t\tif sc and sc != '':\n\t\t\tfor s in re.split('::|\\.', sc):\n\t\t\t\tassert s in t\n\t\t\t\tt = t[s]\n\n\t\tcline = [line[0], line[2].split(';')[0], line[3]]\n\t\tif line[0] in t:\n\t\t\t#print line[0], 'in', t\n\t\t\tx = t[line[0]]\n\t\t\tif '+' not in x:\n\t\t\t\tx['+'] = cline\n\t\t\t\treturn\n\t\tif '*' not in t:\n\t\t\tt['*'] = []\n\t\tt['*'].append(cline)\n\t\t#print '...', t, line\n\n\tdef buildTree(self, data):\n\t\ttype_list = [ 'namespace', 'class', 'interface', 'struct', 'union', 'enum', 'function' ]\n\t\t# build layout using 5th field\n\t\tfor line in data:\n\t\t\tif len(line) == 4:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tsd = dict([ x.split(':', 1) for x in line[4].split('\\t')])\n\t\t\texcept:\n\t\t\t\tprint 'bad line', line\n\t\t\t\tcontinue\n\t\t\tline[4] = sd\n\t\t\tcount = 0\n\t\t\tfor t in type_list:\n\t\t\t\tif t in sd:\n\t\t\t\t\tself.addToSymLayout(sd[t])\n\t\t\t\t\tcount = count + 1\n\t\t\tif count != 1:\n\t\t\t\tprint '******** count == 1 *********'\n\t\t\t\tprint data\n\t\t\t\tprint line\n\t\t\t#assert count == 1\n\t\t\n\t\tif len(self.symTree) == 0:\n\t\t\treturn (data, False)\n\t\t\n\t\tfor line in data:\n\t\t\tif len(line) == 4:\n\t\t\t\tself.addToSymTree(None, line)\n\t\t\t\tcontinue\n\t\t\tsd = line[4]\n\t\t\tcount = 0\n\t\t\tfor t in type_list:\n\t\t\t\tif t in sd:\n\t\t\t\t\tself.addToSymTree(sd[t], line)\n\t\t\t\t\tcount = count + 1\n\t\t\tif count != 1:\n\t\t\t\tprint '******** count == 1 *********'\n\t\t\t\tprint data\n\t\t\t\tprint line\n\t\t\t#assert count == 1\n\n\t\treturn (self.symTree, True)\n\n\tdef doQuery(self, filename):\n\t\ttry:\n\t\t\toutput = self.runCtags(filename)\n\t\t\toutput = self.parseCtagsOutput(output)\n\t\t\toutput = self.buildTree(output)\n\t\texcept Exception as e:\n\t\t\tprint str(e)\n\t\t\toutput = [None, False]\n\t\treturn output\n\n\ndef ct_tree_query(filename):\n\tct = CtagsTreeBuilder()\n\toutput = ct.doQuery(filename)\n\treturn output\n\nif __name__ == '__main__':\n\timport optparse\n\timport sys\n\tdepth = 0\n\tdef recursePrint(t):\n\t\tglobal depth\n\t\tfor k, v in t.items():\n\t\t\tif k == '*':\n\t\t\t\tfor line in v:\n\t\t\t\t\tprint '%s%s' % (' ' * depth, line)\n\t\t\t\tcontinue\n\t\t\tif k == '+':\n\t\t\t\tcontinue\n\n\t\t\tif '+' in v:\n\t\t\t\tk = v['+']\n\t\t\tprint '%s%s' % (' ' * depth, k)\n\t\t\t\t\n\t\t\tdepth = depth + 4\n\t\t\trecursePrint(v)\n\t\t\tdepth = depth - 4\n\n\top = optparse.OptionParser()\n\t(options, args) = op.parse_args()\n\tif len(args) != 1:\n\t\tprint 'Please specify a file'\n\t\tsys.exit(-1)\n\n\t(output, isTree) = ct_tree_query(args[0])\n\tif isTree:\n\t\trecursePrint(output)\n\telse:\n\t\tfor line in output:\n\t\t\tprint line\n\n"}
{"repo_name":"Thhhza/XlsxWriter","ref":"refs/heads/master","path":"xlsxwriter/test/comparison/test_chart_column04.py","content":"###############################################################################\n#\n# Tests for XlsxWriter.\n#\n# Copyright (c), 2013-2015, John McNamara, jmcnamara@cpan.org\n#\n\nfrom ..excel_comparsion_test import ExcelComparisonTest\nfrom ...workbook import Workbook\n\n\nclass TestCompareXLSXFiles(ExcelComparisonTest):\n    \"\"\"\n    Test file created by XlsxWriter against a file created by Excel.\n\n    \"\"\"\n\n    def setUp(self):\n        self.maxDiff = None\n\n        filename = 'chart_column04.xlsx'\n\n        test_dir = 'xlsxwriter/test/comparison/'\n        self.got_filename = test_dir + '_test_' + filename\n        self.exp_filename = test_dir + 'xlsx_files/' + filename\n\n        self.ignore_files = []\n        self.ignore_elements = {'xl/workbook.xml': ['\u003cfileVersion', '\u003ccalcPr']}\n\n    def test_create_file(self):\n        \"\"\"Test the creation of a simple XlsxWriter file.\"\"\"\n\n        workbook = Workbook(self.got_filename)\n\n        worksheet = workbook.add_worksheet()\n        chart = workbook.add_chart({'type': 'column'})\n\n        chart.axis_ids = [63591936, 63593856]\n        chart.axis2_ids = [63613568, 63612032]\n\n        data = [[1, 2, 3, 4, 5],\n                [6, 8, 6, 4, 2]]\n\n        worksheet.write_column('A1', data[0])\n        worksheet.write_column('B1', data[1])\n\n        chart.add_series({'values': '=Sheet1!$A$1:$A$5'})\n        chart.add_series({'values': '=Sheet1!$B$1:$B$5', 'y2_axis': 1})\n\n        worksheet.insert_chart('E9', chart)\n\n        workbook.close()\n\n        self.assertExcelEqual()\n"}
{"repo_name":"mbrukman/libcloud","ref":"refs/heads/trunk","path":"libcloud/test/compute/test_gogrid.py","content":"# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport sys\nimport unittest\n\nfrom libcloud.utils.py3 import httplib\nfrom libcloud.utils.py3 import urlparse\nfrom libcloud.utils.py3 import parse_qs\n\nfrom libcloud.compute.base import NodeState, NodeLocation\nfrom libcloud.common.types import LibcloudError, InvalidCredsError\nfrom libcloud.common.gogrid import GoGridIpAddress\nfrom libcloud.compute.drivers.gogrid import GoGridNodeDriver\nfrom libcloud.compute.base import Node, NodeImage, NodeSize\n\nfrom libcloud.test import MockHttp               # pylint: disable-msg=E0611\nfrom libcloud.test.compute import TestCaseMixin  # pylint: disable-msg=E0611\nfrom libcloud.test.file_fixtures import ComputeFileFixtures  # pylint: disable-msg=E0611\n\n\nclass GoGridTests(unittest.TestCase, TestCaseMixin):\n\n    def setUp(self):\n        GoGridNodeDriver.connectionCls.conn_classes = (None, GoGridMockHttp)\n        GoGridMockHttp.type = None\n        self.driver = GoGridNodeDriver(\"foo\", \"bar\")\n\n    def _get_test_512Mb_node_size(self):\n        return NodeSize(id='512Mb',\n                        name=None,\n                        ram=None,\n                        disk=None,\n                        bandwidth=None,\n                        price=None,\n                        driver=self.driver)\n\n    def test_create_node(self):\n        image = NodeImage(1531, None, self.driver)\n        node = self.driver.create_node(\n            name='test1',\n            image=image,\n            size=self._get_test_512Mb_node_size())\n        self.assertEqual(node.name, 'test1')\n        self.assertTrue(node.id is not None)\n        self.assertEqual(node.extra['password'], 'bebebe')\n\n    def test_list_nodes(self):\n        node = self.driver.list_nodes()[0]\n\n        self.assertEqual(node.id, '90967')\n        self.assertEqual(node.extra['password'], 'bebebe')\n        self.assertEqual(node.extra['description'], 'test server')\n\n    def test_reboot_node(self):\n        node = Node(90967, None, None, None, None, self.driver)\n        ret = self.driver.reboot_node(node)\n        self.assertTrue(ret)\n\n    def test_reboot_node_not_successful(self):\n        GoGridMockHttp.type = 'FAIL'\n        node = Node(90967, None, None, None, None, self.driver)\n\n        try:\n            self.driver.reboot_node(node)\n        except Exception:\n            pass\n        else:\n            self.fail('Exception was not thrown')\n\n    def test_destroy_node(self):\n        node = Node(90967, None, None, None, None, self.driver)\n        ret = self.driver.destroy_node(node)\n        self.assertTrue(ret)\n\n    def test_list_images(self):\n        images = self.driver.list_images()\n        image = images[0]\n        self.assertEqual(len(images), 4)\n        self.assertEqual(image.name, 'CentOS 5.3 (32-bit) w/ None')\n        self.assertEqual(image.id, '1531')\n\n        location = NodeLocation(\n            id='gogrid/GSI-939ef909-84b8-4a2f-ad56-02ccd7da05ff.img',\n            name='test location', country='Slovenia',\n            driver=self.driver)\n        images = self.driver.list_images(location=location)\n        image = images[0]\n        self.assertEqual(len(images), 4)\n        self.assertEqual(image.name, 'CentOS 5.3 (32-bit) w/ None')\n        self.assertEqual(image.id, '1531')\n\n    def test_malformed_reply(self):\n        GoGridMockHttp.type = 'FAIL'\n        try:\n            self.driver.list_images()\n        except LibcloudError:\n            e = sys.exc_info()[1]\n            self.assertTrue(isinstance(e, LibcloudError))\n        else:\n            self.fail(\"test should have thrown\")\n\n    def test_invalid_creds(self):\n        GoGridMockHttp.type = 'FAIL'\n        try:\n            self.driver.list_nodes()\n        except InvalidCredsError:\n            e = sys.exc_info()[1]\n            self.assertTrue(e.driver is not None)\n            self.assertEqual(e.driver.name, self.driver.name)\n        else:\n            self.fail(\"test should have thrown\")\n\n    def test_node_creation_without_free_public_ips(self):\n        GoGridMockHttp.type = 'NOPUBIPS'\n        try:\n            image = NodeImage(1531, None, self.driver)\n            self.driver.create_node(\n                name='test1',\n                image=image,\n                size=self._get_test_512Mb_node_size())\n        except LibcloudError:\n            e = sys.exc_info()[1]\n            self.assertTrue(isinstance(e, LibcloudError))\n            self.assertTrue(e.driver is not None)\n            self.assertEqual(e.driver.name, self.driver.name)\n        else:\n            self.fail(\"test should have thrown\")\n\n    def test_list_locations(self):\n        locations = self.driver.list_locations()\n        location_names = [location.name for location in locations]\n\n        self.assertEqual(len(locations), 2)\n        for i in 0, 1:\n            self.assertTrue(isinstance(locations[i], NodeLocation))\n        self.assertTrue(\"US-West-1\" in location_names)\n        self.assertTrue(\"US-East-1\" in location_names)\n\n    def test_ex_save_image(self):\n        node = self.driver.list_nodes()[0]\n        image = self.driver.ex_save_image(node, \"testimage\")\n        self.assertEqual(image.name, \"testimage\")\n\n    def test_ex_edit_image(self):\n        image = self.driver.list_images()[0]\n        ret = self.driver.ex_edit_image(image=image, public=False,\n                                        ex_description=\"test\", name=\"testname\")\n\n        self.assertTrue(isinstance(ret, NodeImage))\n\n    def test_ex_edit_node(self):\n        node = Node(id=90967, name=None, state=None,\n                    public_ips=None, private_ips=None, driver=self.driver)\n        ret = self.driver.ex_edit_node(node=node,\n                                       size=self._get_test_512Mb_node_size())\n\n        self.assertTrue(isinstance(ret, Node))\n\n    def test_ex_list_ips(self):\n        ips = self.driver.ex_list_ips()\n\n        expected_ips = {\"192.168.75.66\": GoGridIpAddress(id=\"5348099\",\n                                                         ip=\"192.168.75.66\", public=True, state=\"Unassigned\",\n                                                         subnet=\"192.168.75.64/255.255.255.240\"),\n                        \"192.168.75.67\": GoGridIpAddress(id=\"5348100\",\n                                                         ip=\"192.168.75.67\", public=True, state=\"Assigned\",\n                                                         subnet=\"192.168.75.64/255.255.255.240\"),\n                        \"192.168.75.68\": GoGridIpAddress(id=\"5348101\",\n                                                         ip=\"192.168.75.68\", public=False, state=\"Unassigned\",\n                                                         subnet=\"192.168.75.64/255.255.255.240\")}\n\n        self.assertEqual(len(expected_ips), 3)\n\n        for ip in ips:\n            self.assertTrue(ip.ip in expected_ips)\n            self.assertEqual(ip.public, expected_ips[ip.ip].public)\n            self.assertEqual(ip.state, expected_ips[ip.ip].state)\n            self.assertEqual(ip.subnet, expected_ips[ip.ip].subnet)\n\n            del expected_ips[ip.ip]\n\n        self.assertEqual(len(expected_ips), 0)\n\n    def test_get_state_invalid(self):\n        state = self.driver._get_state('invalid')\n        self.assertEqual(state, NodeState.UNKNOWN)\n\n\nclass GoGridMockHttp(MockHttp):\n\n    fixtures = ComputeFileFixtures('gogrid')\n\n    def _api_grid_image_list(self, method, url, body, headers):\n        body = self.fixtures.load('image_list.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_image_list_FAIL(self, method, url, body, headers):\n        body = \"\u003ch3\u003esome non valid json here\u003c/h3\u003e\"\n        return (httplib.SERVICE_UNAVAILABLE, body, {},\n                httplib.responses[httplib.SERVICE_UNAVAILABLE])\n\n    def _api_grid_server_list(self, method, url, body, headers):\n        body = self.fixtures.load('server_list.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    _api_grid_server_list_NOPUBIPS = _api_grid_server_list\n\n    def _api_grid_server_list_FAIL(self, method, url, body, headers):\n        return (httplib.FORBIDDEN,\n                \"123\", {}, httplib.responses[httplib.FORBIDDEN])\n\n    def _api_grid_ip_list(self, method, url, body, headers):\n        body = self.fixtures.load('ip_list.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_ip_list_NOPUBIPS(self, method, url, body, headers):\n        body = self.fixtures.load('ip_list_empty.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_server_power(self, method, url, body, headers):\n        body = self.fixtures.load('server_power.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_server_power_FAIL(self, method, url, body, headers):\n        body = self.fixtures.load('server_power_fail.json')\n        return (httplib.NOT_FOUND, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_server_add(self, method, url, body, headers):\n        body = self.fixtures.load('server_add.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    _api_grid_server_add_NOPUBIPS = _api_grid_server_add\n\n    def _api_grid_server_delete(self, method, url, body, headers):\n        body = self.fixtures.load('server_delete.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_server_edit(self, method, url, body, headers):\n        body = self.fixtures.load('server_edit.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_support_password_list(self, method, url, body, headers):\n        body = self.fixtures.load('password_list.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    _api_support_password_list_NOPUBIPS = _api_support_password_list\n\n    def _api_grid_image_save(self, method, url, body, headers):\n        body = self.fixtures.load('image_save.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_grid_image_edit(self, method, url, body, headers):\n        # edit method is quite similar to save method from the response\n        # perspective\n        body = self.fixtures.load('image_save.json')\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\n    def _api_common_lookup_list(self, method, url, body, headers):\n        _valid_lookups = (\"ip.datacenter\",)\n\n        lookup = parse_qs(urlparse.urlparse(url).query)[\"lookup\"][0]\n        if lookup in _valid_lookups:\n            fixture_path = \"lookup_list_%s.json\" % \\\n                (lookup.replace(\".\", \"_\"))\n        else:\n            raise NotImplementedError\n        body = self.fixtures.load(fixture_path)\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\n\nif __name__ == '__main__':\n    sys.exit(unittest.main())\n"}
{"repo_name":"s40523220/2016fallcp_hw","ref":"refs/heads/gh-pages","path":"plugin/liquid_tags/test_generation.py","content":"# -*- coding: utf-8 -*-\r\nfrom __future__ import print_function\r\n\r\nimport filecmp\r\nimport os\r\nimport unittest\r\nfrom shutil import rmtree\r\nfrom tempfile import mkdtemp\r\n\r\nimport pytest\r\nfrom pelican import Pelican\r\nfrom pelican.settings import read_settings\r\n\r\nfrom .notebook import IPYTHON_VERSION\r\n\r\nPLUGIN_DIR = os.path.dirname(__file__)\r\nTEST_DATA_DIR = os.path.join(PLUGIN_DIR, 'test_data')\r\n\r\n\r\nclass TestFullRun(unittest.TestCase):\r\n    '''Test running Pelican with the Plugin'''\r\n\r\n    def setUp(self):\r\n        '''Create temporary output and cache folders'''\r\n        self.temp_path = mkdtemp(prefix='pelicantests.')\r\n        self.temp_cache = mkdtemp(prefix='pelican_cache.')\r\n        os.chdir(TEST_DATA_DIR)\r\n\r\n    def tearDown(self):\r\n        '''Remove output and cache folders'''\r\n        rmtree(self.temp_path)\r\n        rmtree(self.temp_cache)\r\n        os.chdir(PLUGIN_DIR)\r\n\r\n    @pytest.mark.skipif(IPYTHON_VERSION \u003e= 3,\r\n                        reason=\"output must be created with ipython version 2\")\r\n    def test_generate_with_ipython3(self):\r\n        '''Test generation of site with the plugin.'''\r\n\r\n        base_path = os.path.dirname(os.path.abspath(__file__))\r\n        base_path = os.path.join(base_path, 'test_data')\r\n        content_path = os.path.join(base_path, 'content')\r\n        output_path = os.path.join(base_path, 'output')\r\n        settings_path = os.path.join(base_path, 'pelicanconf.py')\r\n        settings = read_settings(path=settings_path,\r\n                                 override={'PATH': content_path,\r\n                                           'OUTPUT_PATH': self.temp_path,\r\n                                           'CACHE_PATH': self.temp_cache,\r\n                                           }\r\n                                 )\r\n\r\n        pelican = Pelican(settings)\r\n        pelican.run()\r\n\r\n        # test existence\r\n        assert os.path.exists(os.path.join(self.temp_path,\r\n                                           'test-ipython-notebook-nb-format-3.html'))\r\n        assert os.path.exists(os.path.join(self.temp_path,\r\n                                           'test-ipython-notebook-nb-format-4.html'))\r\n\r\n        # test differences\r\n        #assert filecmp.cmp(os.path.join(output_path,\r\n        #                                'test-ipython-notebook-v2.html'),\r\n        #                   os.path.join(self.temp_path,\r\n        #                                'test-ipython-notebook.html'))\r\n\r\n    @pytest.mark.skipif(IPYTHON_VERSION \u003c 3,\r\n                        reason=\"output must be created with ipython version 3\")\r\n    def test_generate_with_ipython2(self):\r\n        '''Test generation of site with the plugin.'''\r\n\r\n        base_path = os.path.dirname(os.path.abspath(__file__))\r\n        base_path = os.path.join(base_path, 'test_data')\r\n        content_path = os.path.join(base_path, 'content')\r\n        output_path = os.path.join(base_path, 'output')\r\n        settings_path = os.path.join(base_path, 'pelicanconf.py')\r\n        settings = read_settings(path=settings_path,\r\n                                 override={'PATH': content_path,\r\n                                           'OUTPUT_PATH': self.temp_path,\r\n                                           'CACHE_PATH': self.temp_cache,\r\n                                           }\r\n                                 )\r\n\r\n        pelican = Pelican(settings)\r\n        pelican.run()\r\n\r\n        # test existence\r\n        assert os.path.exists(os.path.join(self.temp_path,\r\n                                           'test-ipython-notebook-nb-format-3.html'))\r\n        assert os.path.exists(os.path.join(self.temp_path,\r\n                                           'test-ipython-notebook-nb-format-4.html'))\r\n\r\n        # test differences\r\n        #assert filecmp.cmp(os.path.join(output_path,\r\n        #                                'test-ipython-notebook-v3.html'),\r\n        #                   os.path.join(self.temp_path,\r\n        #                                'test-ipython-notebook.html'))\r\n"}
{"repo_name":"ffantast/magnum","ref":"refs/heads/master","path":"magnum/tests/unit/objects/test_objects.py","content":"#    Copyright 2015 IBM Corp.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport datetime\nimport gettext\n\nimport iso8601\nimport netaddr\nfrom oslo_utils import timeutils\nfrom oslo_versionedobjects import fields\n\nfrom magnum.common import context as magnum_context\nfrom magnum.common import exception\nfrom magnum.objects import base\nfrom magnum.objects import utils\nfrom magnum.tests import base as test_base\n\ngettext.install('magnum')\n\n\n@base.MagnumObjectRegistry.register\nclass MyObj(base.MagnumObject):\n    VERSION = '1.0'\n\n    fields = {'foo': fields.IntegerField(),\n              'bar': fields.StringField(),\n              'missing': fields.StringField(),\n              }\n\n    def obj_load_attr(self, attrname):\n        setattr(self, attrname, 'loaded!')\n\n    @base.remotable_classmethod\n    def query(cls, context):\n        obj = cls(context)\n        obj.foo = 1\n        obj.bar = 'bar'\n        obj.obj_reset_changes()\n        return obj\n\n    @base.remotable\n    def marco(self, context):\n        return 'polo'\n\n    @base.remotable\n    def update_test(self, context):\n        if context.project_id == 'alternate':\n            self.bar = 'alternate-context'\n        else:\n            self.bar = 'updated'\n\n    @base.remotable\n    def save(self, context):\n        self.obj_reset_changes()\n\n    @base.remotable\n    def refresh(self, context):\n        self.foo = 321\n        self.bar = 'refreshed'\n        self.obj_reset_changes()\n\n    @base.remotable\n    def modify_save_modify(self, context):\n        self.bar = 'meow'\n        self.save()\n        self.foo = 42\n\n\nclass MyObj2(object):\n    @classmethod\n    def obj_name(cls):\n        return 'MyObj'\n\n    @base.remotable_classmethod\n    def get(cls, *args, **kwargs):\n        pass\n\n\nclass TestSubclassedObject(MyObj):\n    fields = {'new_field': fields.StringField()}\n\n\nclass TestUtils(test_base.TestCase):\n\n    def test_datetime_or_none(self):\n        naive_dt = datetime.datetime.now()\n        dt = timeutils.parse_isotime(timeutils.isotime(naive_dt))\n        self.assertEqual(utils.datetime_or_none(dt), dt)\n        self.assertEqual(utils.datetime_or_none(dt),\n                         naive_dt.replace(tzinfo=iso8601.iso8601.Utc(),\n                                          microsecond=0))\n        self.assertIsNone(utils.datetime_or_none(None))\n        self.assertRaises(ValueError, utils.datetime_or_none, 'foo')\n\n    def test_datetime_or_str_or_none(self):\n        dts = timeutils.isotime()\n        dt = timeutils.parse_isotime(dts)\n        self.assertEqual(utils.datetime_or_str_or_none(dt), dt)\n        self.assertIsNone(utils.datetime_or_str_or_none(None))\n        self.assertEqual(utils.datetime_or_str_or_none(dts), dt)\n        self.assertRaises(ValueError, utils.datetime_or_str_or_none, 'foo')\n\n    def test_int_or_none(self):\n        self.assertEqual(utils.int_or_none(1), 1)\n        self.assertEqual(utils.int_or_none('1'), 1)\n        self.assertIsNone(utils.int_or_none(None))\n        self.assertRaises(ValueError, utils.int_or_none, 'foo')\n\n    def test_str_or_none(self):\n        class Obj(object):\n            pass\n        self.assertEqual(utils.str_or_none('foo'), 'foo')\n        self.assertEqual(utils.str_or_none(1), '1')\n        self.assertIsNone(utils.str_or_none(None))\n\n    def test_ip_or_none(self):\n        ip4 = netaddr.IPAddress('1.2.3.4', 4)\n        ip6 = netaddr.IPAddress('1::2', 6)\n        self.assertEqual(utils.ip_or_none(4)('1.2.3.4'), ip4)\n        self.assertEqual(utils.ip_or_none(6)('1::2'), ip6)\n        self.assertIsNone(utils.ip_or_none(4)(None))\n        self.assertIsNone(utils.ip_or_none(6)(None))\n        self.assertRaises(netaddr.AddrFormatError, utils.ip_or_none(4), 'foo')\n        self.assertRaises(netaddr.AddrFormatError, utils.ip_or_none(6), 'foo')\n\n    def test_dt_serializer(self):\n        class Obj(object):\n            foo = utils.dt_serializer('bar')\n\n        obj = Obj()\n        obj.bar = timeutils.parse_isotime('1955-11-05T00:00:00Z')\n        self.assertEqual('1955-11-05T00:00:00Z', obj.foo())\n        obj.bar = None\n        self.assertIsNone(obj.foo())\n        obj.bar = 'foo'\n        self.assertRaises(AttributeError, obj.foo)\n\n    def test_dt_deserializer(self):\n        dt = timeutils.parse_isotime('1955-11-05T00:00:00Z')\n        self.assertEqual(utils.dt_deserializer(None, timeutils.isotime(dt)),\n                         dt)\n        self.assertIsNone(utils.dt_deserializer(None, None))\n        self.assertRaises(ValueError, utils.dt_deserializer, None, 'foo')\n\n\nclass _TestObject(object):\n    def test_hydration_type_error(self):\n        primitive = {'magnum_object.name': 'MyObj',\n                     'magnum_object.namespace': 'magnum',\n                     'magnum_object.version': '1.5',\n                     'magnum_object.data': {'foo': 'a'}}\n        self.assertRaises(ValueError, MyObj.obj_from_primitive, primitive)\n\n    def test_hydration(self):\n        primitive = {'magnum_object.name': 'MyObj',\n                     'magnum_object.namespace': 'magnum',\n                     'magnum_object.version': '1.5',\n                     'magnum_object.data': {'foo': 1}}\n        obj = MyObj.obj_from_primitive(primitive)\n        self.assertEqual(1, obj.foo)\n\n    def test_hydration_bad_ns(self):\n        primitive = {'magnum_object.name': 'MyObj',\n                     'magnum_object.namespace': 'foo',\n                     'magnum_object.version': '1.5',\n                     'magnum_object.data': {'foo': 1}}\n        self.assertRaises(exception.UnsupportedObjectError,\n                          MyObj.obj_from_primitive, primitive)\n\n    def test_dehydration(self):\n        expected = {'magnum_object.name': 'MyObj',\n                    'magnum_object.namespace': 'magnum',\n                    'magnum_object.version': '1.5',\n                    'magnum_object.data': {'foo': 1}}\n        obj = MyObj(self.context)\n        obj.foo = 1\n        obj.obj_reset_changes()\n        self.assertEqual(expected, obj.obj_to_primitive())\n\n    def test_get_updates(self):\n        obj = MyObj(self.context)\n        self.assertEqual({}, obj.obj_get_changes())\n        obj.foo = 123\n        self.assertEqual({'foo': 123}, obj.obj_get_changes())\n        obj.bar = 'test'\n        self.assertEqual({'foo': 123, 'bar': 'test'}, obj.obj_get_changes())\n        obj.obj_reset_changes()\n        self.assertEqual({}, obj.obj_get_changes())\n\n    def test_object_property(self):\n        obj = MyObj(self.context, foo=1)\n        self.assertEqual(1, obj.foo)\n\n    def test_object_property_type_error(self):\n        obj = MyObj(self.context)\n\n        def fail():\n            obj.foo = 'a'\n        self.assertRaises(ValueError, fail)\n\n    def test_load(self):\n        obj = MyObj(self.context)\n        self.assertEqual('loaded!', obj.bar)\n\n    def test_load_in_base(self):\n        class Foo(base.MagnumObject):\n            fields = {'foobar': fields.IntegerField()}\n        obj = Foo(self.context)\n        # NOTE(danms): Can't use assertRaisesRegexp() because of py26\n        raised = False\n        try:\n            obj.foobar\n        except NotImplementedError as ex:\n            raised = True\n        self.assertTrue(raised)\n        self.assertTrue('foobar' in str(ex))\n\n    def test_loaded_in_primitive(self):\n        obj = MyObj(self.context)\n        obj.foo = 1\n        obj.obj_reset_changes()\n        self.assertEqual('loaded!', obj.bar)\n        expected = {'magnum_object.name': 'MyObj',\n                    'magnum_object.namespace': 'magnum',\n                    'magnum_object.version': '1.0',\n                    'magnum_object.changes': ['bar'],\n                    'magnum_object.data': {'foo': 1,\n                                           'bar': 'loaded!'}}\n        self.assertEqual(expected, obj.obj_to_primitive())\n\n    def test_changes_in_primitive(self):\n        obj = MyObj(self.context)\n        obj.foo = 123\n        self.assertEqual(set(['foo']), obj.obj_what_changed())\n        primitive = obj.obj_to_primitive()\n        self.assertTrue('magnum_object.changes' in primitive)\n        obj2 = MyObj.obj_from_primitive(primitive)\n        self.assertEqual(set(['foo']), obj2.obj_what_changed())\n        obj2.obj_reset_changes()\n        self.assertEqual(set(), obj2.obj_what_changed())\n\n    def test_unknown_objtype(self):\n        self.assertRaises(exception.UnsupportedObjectError,\n                          base.MagnumObject.obj_class_from_name, 'foo', '1.0')\n\n    def test_with_alternate_context(self):\n        context1 = magnum_context.RequestContext('foo', 'foo')\n        context2 = magnum_context.RequestContext('bar', project_id='alternate')\n        obj = MyObj.query(context1)\n        obj.update_test(context2)\n        self.assertEqual('alternate-context', obj.bar)\n        self.assertRemotes()\n\n    def test_orphaned_object(self):\n        obj = MyObj.query(self.context)\n        obj._context = None\n        self.assertRaises(exception.OrphanedObjectError,\n                          obj.update_test)\n        self.assertRemotes()\n\n    def test_changed_1(self):\n        obj = MyObj.query(self.context)\n        obj.foo = 123\n        self.assertEqual(set(['foo']), obj.obj_what_changed())\n        obj.update_test(self.context)\n        self.assertEqual(set(['foo', 'bar']), obj.obj_what_changed())\n        self.assertEqual(123, obj.foo)\n        self.assertRemotes()\n\n    def test_changed_2(self):\n        obj = MyObj.query(self.context)\n        obj.foo = 123\n        self.assertEqual(set(['foo']), obj.obj_what_changed())\n        obj.save()\n        self.assertEqual(set([]), obj.obj_what_changed())\n        self.assertEqual(123, obj.foo)\n        self.assertRemotes()\n\n    def test_changed_3(self):\n        obj = MyObj.query(self.context)\n        obj.foo = 123\n        self.assertEqual(set(['foo']), obj.obj_what_changed())\n        obj.refresh()\n        self.assertEqual(set([]), obj.obj_what_changed())\n        self.assertEqual(321, obj.foo)\n        self.assertEqual('refreshed', obj.bar)\n        self.assertRemotes()\n\n    def test_changed_4(self):\n        obj = MyObj.query(self.context)\n        obj.bar = 'something'\n        self.assertEqual(set(['bar']), obj.obj_what_changed())\n        obj.modify_save_modify(self.context)\n        self.assertEqual(set(['foo']), obj.obj_what_changed())\n        self.assertEqual(42, obj.foo)\n        self.assertEqual('meow', obj.bar)\n        self.assertRemotes()\n\n    def test_static_result(self):\n        obj = MyObj.query(self.context)\n        self.assertEqual('bar', obj.bar)\n        result = obj.marco()\n        self.assertEqual('polo', result)\n        self.assertRemotes()\n\n    def test_updates(self):\n        obj = MyObj.query(self.context)\n        self.assertEqual(1, obj.foo)\n        obj.update_test()\n        self.assertEqual('updated', obj.bar)\n        self.assertRemotes()\n\n    def test_base_attributes(self):\n        dt = datetime.datetime(1955, 11, 5)\n        obj = MyObj(self.context)\n        obj.created_at = dt\n        obj.updated_at = dt\n        expected = {'magnum_object.name': 'MyObj',\n                    'magnum_object.namespace': 'magnum',\n                    'magnum_object.version': '1.0',\n                    'magnum_object.changes':\n                        ['created_at', 'updated_at'],\n                    'magnum_object.data':\n                        {'created_at': timeutils.isotime(dt),\n                         'updated_at': timeutils.isotime(dt)}\n                    }\n        actual = obj.obj_to_primitive()\n        # magnum_object.changes is built from a set and order is undefined\n        self.assertEqual(sorted(expected['magnum_object.changes']),\n                         sorted(actual['magnum_object.changes']))\n        del expected['magnum_object.changes'], actual['magnum_object.changes']\n        self.assertEqual(expected, actual)\n\n    def test_contains(self):\n        obj = MyObj(self.context)\n        self.assertFalse('foo' in obj)\n        obj.foo = 1\n        self.assertTrue('foo' in obj)\n        self.assertFalse('does_not_exist' in obj)\n\n    def test_obj_attr_is_set(self):\n        obj = MyObj(self.context, foo=1)\n        self.assertTrue(obj.obj_attr_is_set('foo'))\n        self.assertFalse(obj.obj_attr_is_set('bar'))\n        self.assertRaises(AttributeError, obj.obj_attr_is_set, 'bang')\n\n    def test_get(self):\n        obj = MyObj(self.context, foo=1)\n        # Foo has value, should not get the default\n        self.assertEqual(obj.get('foo', 2), 1)\n        # Foo has value, should return the value without error\n        self.assertEqual(obj.get('foo'), 1)\n        # Bar is not loaded, so we should get the default\n        self.assertEqual(obj.get('bar', 'not-loaded'), 'not-loaded')\n        # Bar without a default should lazy-load\n        self.assertEqual(obj.get('bar'), 'loaded!')\n        # Bar now has a default, but loaded value should be returned\n        self.assertEqual(obj.get('bar', 'not-loaded'), 'loaded!')\n        # Invalid attribute should raise AttributeError\n        self.assertRaises(AttributeError, obj.get, 'nothing')\n        # ...even with a default\n        self.assertRaises(AttributeError, obj.get, 'nothing', 3)\n\n    def test_object_inheritance(self):\n        base_fields = list(base.MagnumObject.fields.keys())\n        myobj_fields = ['foo', 'bar', 'missing'] + base_fields\n        myobj3_fields = ['new_field']\n        self.assertTrue(issubclass(TestSubclassedObject, MyObj))\n        self.assertEqual(len(myobj_fields), len(MyObj.fields))\n        self.assertEqual(set(myobj_fields), set(MyObj.fields.keys()))\n        self.assertEqual(len(myobj_fields) + len(myobj3_fields),\n                         len(TestSubclassedObject.fields))\n        self.assertEqual(set(myobj_fields) | set(myobj3_fields),\n                         set(TestSubclassedObject.fields.keys()))\n\n    def test_get_changes(self):\n        obj = MyObj(self.context)\n        self.assertEqual({}, obj.obj_get_changes())\n        obj.foo = 123\n        self.assertEqual({'foo': 123}, obj.obj_get_changes())\n        obj.bar = 'test'\n        self.assertEqual({'foo': 123, 'bar': 'test'}, obj.obj_get_changes())\n        obj.obj_reset_changes()\n        self.assertEqual({}, obj.obj_get_changes())\n\n    def test_obj_fields(self):\n        class TestObj(base.MagnumObject):\n            fields = {'foo': fields.IntegerField()}\n            obj_extra_fields = ['bar']\n\n            @property\n            def bar(self):\n                return 'this is bar'\n\n        obj = TestObj(self.context)\n        self.assertEqual(set(['created_at', 'updated_at', 'foo', 'bar']),\n                         set(obj.obj_fields))\n\n    def test_obj_constructor(self):\n        obj = MyObj(self.context, foo=123, bar='abc')\n        self.assertEqual(123, obj.foo)\n        self.assertEqual('abc', obj.bar)\n        self.assertEqual(set(['foo', 'bar']), obj.obj_what_changed())\n\n\nclass TestObjectSerializer(test_base.TestCase):\n\n    def test_object_serialization(self):\n        ser = base.MagnumObjectSerializer()\n        obj = MyObj(self.context)\n        primitive = ser.serialize_entity(self.context, obj)\n        self.assertTrue('magnum_object.name' in primitive)\n        obj2 = ser.deserialize_entity(self.context, primitive)\n        self.assertIsInstance(obj2, MyObj)\n        self.assertEqual(self.context, obj2._context)\n\n    def test_object_serialization_iterables(self):\n        ser = base.MagnumObjectSerializer()\n        obj = MyObj(self.context)\n        for iterable in (list, tuple, set):\n            thing = iterable([obj])\n            primitive = ser.serialize_entity(self.context, thing)\n            self.assertEqual(1, len(primitive))\n            for item in primitive:\n                self.assertFalse(isinstance(item, base.MagnumObject))\n            thing2 = ser.deserialize_entity(self.context, primitive)\n            self.assertEqual(1, len(thing2))\n            for item in thing2:\n                self.assertIsInstance(item, MyObj)\n"}
{"repo_name":"xboxfanj/android_kernel_htc_msm8974","ref":"refs/heads/lp5.0","path":"tools/perf/scripts/python/check-perf-trace.py","content":"# perf script event handlers, generated by perf script -g python\n# (c) 2010, Tom Zanussi \u003ctzanussi@gmail.com\u003e\n# Licensed under the terms of the GNU GPL License version 2\n#\n# This script tests basic functionality such as flag and symbol\n# strings, common_xxx() calls back into perf, begin, end, unhandled\n# events, etc.  Basically, if this script runs successfully and\n# displays expected results, Python scripting support should be ok.\n\nimport os\nimport sys\n\nsys.path.append(os.environ['PERF_EXEC_PATH'] + \\\n\t'/scripts/python/Perf-Trace-Util/lib/Perf/Trace')\n\nfrom Core import *\nfrom perf_trace_context import *\n\nunhandled = autodict()\n\ndef trace_begin():\n\tprint \"trace_begin\"\n\tpass\n\ndef trace_end():\n        print_unhandled()\n\ndef irq__softirq_entry(event_name, context, common_cpu,\n\tcommon_secs, common_nsecs, common_pid, common_comm,\n\tvec):\n\t\tprint_header(event_name, common_cpu, common_secs, common_nsecs,\n\t\t\tcommon_pid, common_comm)\n\n                print_uncommon(context)\n\n\t\tprint \"vec=%s\\n\" % \\\n\t\t(symbol_str(\"irq__softirq_entry\", \"vec\", vec)),\n\ndef kmem__kmalloc(event_name, context, common_cpu,\n\tcommon_secs, common_nsecs, common_pid, common_comm,\n\tcall_site, ptr, bytes_req, bytes_alloc,\n\tgfp_flags):\n\t\tprint_header(event_name, common_cpu, common_secs, common_nsecs,\n\t\t\tcommon_pid, common_comm)\n\n                print_uncommon(context)\n\n\t\tprint \"call_site=%u, ptr=%u, bytes_req=%u, \" \\\n\t\t\"bytes_alloc=%u, gfp_flags=%s\\n\" % \\\n\t\t(call_site, ptr, bytes_req, bytes_alloc,\n\n\t\tflag_str(\"kmem__kmalloc\", \"gfp_flags\", gfp_flags)),\n\ndef trace_unhandled(event_name, context, event_fields_dict):\n    try:\n        unhandled[event_name] += 1\n    except TypeError:\n        unhandled[event_name] = 1\n\ndef print_header(event_name, cpu, secs, nsecs, pid, comm):\n\tprint \"%-20s %5u %05u.%09u %8u %-20s \" % \\\n\t(event_name, cpu, secs, nsecs, pid, comm),\n\n# print trace fields not included in handler args\ndef print_uncommon(context):\n    print \"common_preempt_count=%d, common_flags=%s, common_lock_depth=%d, \" \\\n        % (common_pc(context), trace_flag_str(common_flags(context)), \\\n               common_lock_depth(context))\n\ndef print_unhandled():\n    keys = unhandled.keys()\n    if not keys:\n        return\n\n    print \"\\nunhandled events:\\n\\n\",\n\n    print \"%-40s  %10s\\n\" % (\"event\", \"count\"),\n    print \"%-40s  %10s\\n\" % (\"----------------------------------------\", \\\n                                 \"-----------\"),\n\n    for event_name in keys:\n\tprint \"%-40s  %10d\\n\" % (event_name, unhandled[event_name])\n"}
{"repo_name":"Affix/CouchPotatoServer","ref":"refs/heads/master","path":"couchpotato/core/media/_base/providers/torrent/sceneaccess.py","content":"import traceback\n\nfrom bs4 import BeautifulSoup\nfrom couchpotato.core.helpers.encoding import toUnicode\nfrom couchpotato.core.helpers.variable import tryInt\nfrom couchpotato.core.logger import CPLog\nfrom couchpotato.core.media._base.providers.torrent.base import TorrentProvider\n\n\nlog = CPLog(__name__)\n\n\nclass Base(TorrentProvider):\n\n    urls = {\n        'test': 'https://www.sceneaccess.eu/',\n        'login': 'https://www.sceneaccess.eu/login',\n        'login_check': 'https://www.sceneaccess.eu/inbox',\n        'detail': 'https://www.sceneaccess.eu/details?id=%s',\n        'search': 'https://www.sceneaccess.eu/browse?c%d=%d',\n        'archive': 'https://www.sceneaccess.eu/archive?\u0026c%d=%d',\n        'download': 'https://www.sceneaccess.eu/%s',\n    }\n\n    http_time_between_calls = 1  # Seconds\n\n    def _searchOnTitle(self, title, media, quality, results):\n\n        url = self.buildUrl(title, media, quality)\n        data = self.getHTMLData(url)\n\n        if data:\n            html = BeautifulSoup(data)\n\n            try:\n                resultsTable = html.find('table', attrs = {'id': 'torrents-table'})\n                if resultsTable is None:\n                    return\n\n                entries = resultsTable.find_all('tr', attrs = {'class': 'tt_row'})\n                for result in entries:\n\n                    link = result.find('td', attrs = {'class': 'ttr_name'}).find('a')\n                    url = result.find('td', attrs = {'class': 'td_dl'}).find('a')\n                    leechers = result.find('td', attrs = {'class': 'ttr_leechers'}).find('a')\n                    torrent_id = link['href'].replace('details?id=', '')\n\n                    results.append({\n                        'id': torrent_id,\n                        'name': link['title'],\n                        'url': self.urls['download'] % url['href'],\n                        'detail_url': self.urls['detail'] % torrent_id,\n                        'size': self.parseSize(result.find('td', attrs = {'class': 'ttr_size'}).contents[0]),\n                        'seeders': tryInt(result.find('td', attrs = {'class': 'ttr_seeders'}).find('a').string),\n                        'leechers': tryInt(leechers.string) if leechers else 0,\n                        'get_more_info': self.getMoreInfo,\n                    })\n\n            except:\n                log.error('Failed getting results from %s: %s', (self.getName(), traceback.format_exc()))\n\n    def getMoreInfo(self, item):\n        full_description = self.getCache('sceneaccess.%s' % item['id'], item['detail_url'], cache_timeout = 25920000)\n        html = BeautifulSoup(full_description)\n        nfo_pre = html.find('div', attrs = {'id': 'details_table'})\n        description = toUnicode(nfo_pre.text) if nfo_pre else ''\n\n        item['description'] = description\n        return item\n\n    # Login\n    def getLoginParams(self):\n        return {\n            'username': self.conf('username'),\n            'password': self.conf('password'),\n            'submit': 'come on in',\n        }\n\n    def loginSuccess(self, output):\n        return '/inbox' in output.lower()\n\n    loginCheckSuccess = loginSuccess\n\n\nconfig = [{\n    'name': 'sceneaccess',\n    'groups': [\n        {\n            'tab': 'searcher',\n            'list': 'torrent_providers',\n            'name': 'SceneAccess',\n            'description': '\u003ca href=\"https://sceneaccess.eu/\"\u003eSceneAccess\u003c/a\u003e',\n            'wizard': True,\n            'icon': 'iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAAABnRSTlMAAAAAAABupgeRAAACT0lEQVR4AYVQS0sbURidO3OTmajJ5FElTTOkPmZ01GhHrIq0aoWAj1Vc+A/cuRMXbl24V9SlCGqrLhVFCrooEhCp2BAx0mobTY2kaR7qmOm87EXL1EWxh29xL+c7nPMdgGHYO5bF/gdbefnr6WlbWRnxluMwAB4Z0uEgXa7nwaDL7+/RNPzxbYvb/XJ0FBYVfd/ayh0fQ4qCGEHcm0KLRZUk7Pb2YRJPRwcsKMidnKD3t9VVT3s7BDh+z5FOZ3Vfn3h+Hltfx00mRRSRWFcUmmVNhYVqPn8dj3va2oh+txvcQRVF9ebm1fi4k+dRFbosY5rm4Hk7xxULQnJnx93S4g0EIEEQRoDLo6PrWEw8Pc0eHLwYGopMTDirqlJ7eyhYYGHhfgfHCcKYksZGVB/NcXI2mw6HhZERqrjYTNPHi4tFPh8aJIYIhgPlcCRDoZLW1s75+Z/7+59nZ/OJhLWigqAoKZX6Mjf3dXkZ3pydGYLc4aEoCCkInzQ1fRobS2xuvllaonkedfArnY5OTdGVldBkOADgqq2Nr6z8CIWaJietDHOhKB+HhwFKC6Gnq4ukKJvP9zcSbjYDXbeVlkKzuZBhnnV3e3t6UOmaJO0ODibW1hB1GYkg8R/gup7Z3TVZLJ5AILW9LcZiVpYtYBhw16O3t7cauckyeF9Tgz0ATpL2+nopmWycmbnY2LiKRjFk6/d7+/vRJfl4HGzV1T0UIM43MGBvaIBWK/YvwM5w+IMgGH8tkyEgvIpE7M3Nt6qqZrNyOq1kMmouh455Ggz+BhKY4GEc2CfwAAAAAElFTkSuQmCC',\n            'options': [\n                {\n                    'name': 'enabled',\n                    'type': 'enabler',\n                    'default': False,\n                },\n                {\n                    'name': 'username',\n                    'default': '',\n                },\n                {\n                    'name': 'password',\n                    'default': '',\n                    'type': 'password',\n                },\n                {\n                    'name': 'seed_ratio',\n                    'label': 'Seed ratio',\n                    'type': 'float',\n                    'default': 1,\n                    'description': 'Will not be (re)moved until this seed ratio is met.',\n                },\n                {\n                    'name': 'seed_time',\n                    'label': 'Seed time',\n                    'type': 'int',\n                    'default': 40,\n                    'description': 'Will not be (re)moved until this seed time (in hours) is met.',\n                },\n                {\n                    'name': 'extra_score',\n                    'advanced': True,\n                    'label': 'Extra Score',\n                    'type': 'int',\n                    'default': 20,\n                    'description': 'Starting score for each release found via this provider.',\n                }\n            ],\n        },\n    ],\n}]\n"}
{"repo_name":"vqw/frappe","ref":"refs/heads/develop","path":"frappe/model/delete_doc.py","content":"# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nimport frappe.model.meta\nfrom frappe.model.dynamic_links import get_dynamic_link_map\nimport frappe.defaults\nfrom frappe.utils.file_manager import remove_all\nfrom frappe.utils.password import delete_all_passwords_for\nfrom frappe import _\nfrom frappe.model.naming import revert_series_if_last\n\ndef delete_doc(doctype=None, name=None, force=0, ignore_doctypes=None, for_reload=False,\n\tignore_permissions=False, flags=None, ignore_on_trash=False):\n\t\"\"\"\n\t\tDeletes a doc(dt, dn) and validates if it is not submitted and not linked in a live record\n\t\"\"\"\n\tif not ignore_doctypes: ignore_doctypes = []\n\n\t# get from form\n\tif not doctype:\n\t\tdoctype = frappe.form_dict.get('dt')\n\t\tname = frappe.form_dict.get('dn')\n\n\tnames = name\n\tif isinstance(name, basestring):\n\t\tnames = [name]\n\n\tfor name in names or []:\n\n\t\t# already deleted..?\n\t\tif not frappe.db.exists(doctype, name):\n\t\t\treturn\n\n\t\t# delete attachments\n\t\tremove_all(doctype, name)\n\n\t\t# delete passwords\n\t\tdelete_all_passwords_for(doctype, name)\n\n\t\tdoc = None\n\t\tif doctype==\"DocType\":\n\t\t\tif for_reload:\n\n\t\t\t\ttry:\n\t\t\t\t\tdoc = frappe.get_doc(doctype, name)\n\t\t\t\texcept frappe.DoesNotExistError:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tdoc.run_method(\"before_reload\")\n\n\t\t\telse:\n\t\t\t\tdoc = frappe.get_doc(doctype, name)\n\n\t\t\t\tupdate_flags(doc, flags, ignore_permissions)\n\t\t\t\tcheck_permission_and_not_submitted(doc)\n\n\t\t\t\tfrappe.db.sql(\"delete from `tabCustom Field` where dt = %s\", name)\n\t\t\t\tfrappe.db.sql(\"delete from `tabCustom Script` where dt = %s\", name)\n\t\t\t\tfrappe.db.sql(\"delete from `tabProperty Setter` where doc_type = %s\", name)\n\t\t\t\tfrappe.db.sql(\"delete from `tabReport` where ref_doctype=%s\", name)\n\n\t\t\tdelete_from_table(doctype, name, ignore_doctypes, None)\n\n\t\telse:\n\t\t\tdoc = frappe.get_doc(doctype, name)\n\n\t\t\tif not for_reload:\n\t\t\t\tupdate_flags(doc, flags, ignore_permissions)\n\t\t\t\tcheck_permission_and_not_submitted(doc)\n\n\t\t\t\tif not ignore_on_trash:\n\t\t\t\t\tdoc.run_method(\"on_trash\")\n\t\t\t\t\tdoc.run_method('on_change')\n\n\t\t\t\tdynamic_linked_doctypes = [df.parent for df in get_dynamic_link_map().get(doc.doctype, [])]\n\t\t\t\tif \"ToDo\" in dynamic_linked_doctypes:\n\t\t\t\t\tdelete_linked_todos(doc)\n\n\t\t\t\tif \"Communication\" in dynamic_linked_doctypes:\n\t\t\t\t\tdelete_linked_communications(doc)\n\n\t\t\t\tif \"DocShare\" in dynamic_linked_doctypes:\n\t\t\t\t\tdelete_shared(doc)\n\n\t\t\t\tif \"Email Unsubscribe\" in dynamic_linked_doctypes:\n\t\t\t\t\tdelete_email_subscribe(doc)\n\n\t\t\t\t# check if links exist\n\t\t\t\tif not force:\n\t\t\t\t\tcheck_if_doc_is_linked(doc)\n\t\t\t\t\tcheck_if_doc_is_dynamically_linked(doc)\n\n\t\t\tupdate_naming_series(doc)\n\t\t\tdelete_from_table(doctype, name, ignore_doctypes, doc)\n\t\t\tdoc.run_method(\"after_delete\")\n\n\t\tif doc and not frappe.flags.in_patch:\n\t\t\ttry:\n\t\t\t\tdoc.notify_update()\n\t\t\t\tinsert_feed(doc)\n\t\t\texcept ImportError:\n\t\t\t\tpass\n\n\t\t# delete user_permissions\n\t\tfrappe.defaults.clear_default(parenttype=\"User Permission\", key=doctype, value=name)\n\ndef update_naming_series(doc):\n\tif doc.meta.autoname:\n\t\tif doc.meta.autoname.startswith(\"naming_series:\") \\\n\t\t\tand getattr(doc, \"naming_series\", None):\n\t\t\trevert_series_if_last(doc.naming_series, doc.name)\n\n\t\telif doc.meta.autoname.split(\":\")[0] not in (\"Prompt\", \"field\", \"hash\"):\n\t\t\trevert_series_if_last(doc.meta.autoname, doc.name)\n\ndef delete_from_table(doctype, name, ignore_doctypes, doc):\n\tif doctype!=\"DocType\" and doctype==name:\n\t\tfrappe.db.sql(\"delete from `tabSingles` where doctype=%s\", name)\n\telse:\n\t\tfrappe.db.sql(\"delete from `tab%s` where name=%s\" % (frappe.db.escape(doctype), \"%s\"), (name,))\n\n\t# get child tables\n\tif doc:\n\t\ttables = [d.options for d in doc.meta.get_table_fields()]\n\n\telse:\n\t\tdef get_table_fields(field_doctype):\n\t\t\treturn frappe.db.sql_list(\"\"\"select options from `tab{}` where fieldtype='Table'\n\t\t\t\tand parent=%s\"\"\".format(field_doctype), doctype)\n\n\t\ttables = get_table_fields(\"DocField\")\n\t\tif not frappe.flags.in_install==\"frappe\":\n\t\t\ttables += get_table_fields(\"Custom Field\")\n\n\t# delete from child tables\n\tfor t in list(set(tables)):\n\t\tif t not in ignore_doctypes:\n\t\t\tfrappe.db.sql(\"delete from `tab%s` where parenttype=%s and parent = %s\" % (t, '%s', '%s'), (doctype, name))\n\ndef update_flags(doc, flags=None, ignore_permissions=False):\n\tif ignore_permissions:\n\t\tif not flags: flags = {}\n\t\tflags[\"ignore_permissions\"] = ignore_permissions\n\n\tif flags:\n\t\tdoc.flags.update(flags)\n\ndef check_permission_and_not_submitted(doc):\n\t# permission\n\tif not doc.flags.ignore_permissions and frappe.session.user!=\"Administrator\" and (not doc.has_permission(\"delete\") or (doc.doctype==\"DocType\" and not doc.custom)):\n\t\tfrappe.msgprint(_(\"User not allowed to delete {0}: {1}\").format(doc.doctype, doc.name), raise_exception=True)\n\n\t# check if submitted\n\tif doc.docstatus == 1:\n\t\tfrappe.msgprint(_(\"{0} {1}: Submitted Record cannot be deleted.\").format(doc.doctype, doc.name),\n\t\t\traise_exception=True)\n\ndef check_if_doc_is_linked(doc, method=\"Delete\"):\n\t\"\"\"\n\t\tRaises excption if the given doc(dt, dn) is linked in another record.\n\t\"\"\"\n\tfrom frappe.model.rename_doc import get_link_fields\n\tlink_fields = get_link_fields(doc.doctype)\n\tlink_fields = [[lf['parent'], lf['fieldname'], lf['issingle']] for lf in link_fields]\n\n\tfor link_dt, link_field, issingle in link_fields:\n\t\tif not issingle:\n\t\t\titem = frappe.db.get_value(link_dt, {link_field:doc.name},\n\t\t\t\t[\"name\", \"parent\", \"parenttype\", \"docstatus\"], as_dict=True)\n\t\t\tif item and ((item.parent or item.name) != doc.name) \\\n\t\t\t\t\tand ((method==\"Delete\" and item.docstatus\u003c2) or (method==\"Cancel\" and item.docstatus==1)):\n\t\t\t\t# raise exception only if\n\t\t\t\t# linked to an non-cancelled doc when deleting\n\t\t\t\t# or linked to a submitted doc when cancelling\n\t\t\t\tfrappe.throw(_(\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\")\n\t\t\t\t\t.format(doc.doctype, doc.name, item.parenttype if item.parent else link_dt,\n\t\t\t\t\titem.parent or item.name), frappe.LinkExistsError)\n\ndef check_if_doc_is_dynamically_linked(doc, method=\"Delete\"):\n\t'''Raise `frappe.LinkExistsError` if the document is dynamically linked'''\n\tfor df in get_dynamic_link_map().get(doc.doctype, []):\n\t\tif df.parent in (\"Communication\", \"ToDo\", \"DocShare\", \"Email Unsubscribe\"):\n\t\t\t# don't check for communication and todo!\n\t\t\tcontinue\n\n\t\tmeta = frappe.get_meta(df.parent)\n\t\tif meta.issingle:\n\t\t\t# dynamic link in single doc\n\t\t\trefdoc = frappe.db.get_singles_dict(df.parent)\n\t\t\tif (refdoc.get(df.options)==doc.doctype\n\t\t\t\tand refdoc.get(df.fieldname)==doc.name\n\t\t\t\tand ((method==\"Delete\" and refdoc.docstatus \u003c 2)\n\t\t\t\t\tor (method==\"Cancel\" and refdoc.docstatus==1))\n\t\t\t\t):\n\t\t\t\t# raise exception only if\n\t\t\t\t# linked to an non-cancelled doc when deleting\n\t\t\t\t# or linked to a submitted doc when cancelling\n\t\t\t\tfrappe.throw(_(\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\").format(doc.doctype,\n\t\t\t\t\tdoc.name, df.parent, \"\"), frappe.LinkExistsError)\n\t\telse:\n\t\t\t# dynamic link in table\n\t\t\tfor refdoc in frappe.db.sql(\"\"\"select name, docstatus from `tab{parent}` where\n\t\t\t\t{options}=%s and {fieldname}=%s\"\"\".format(**df), (doc.doctype, doc.name), as_dict=True):\n\n\t\t\t\tif ((method==\"Delete\" and refdoc.docstatus \u003c 2) or (method==\"Cancel\" and refdoc.docstatus==1)):\n\t\t\t\t\t# raise exception only if\n\t\t\t\t\t# linked to an non-cancelled doc when deleting\n\t\t\t\t\t# or linked to a submitted doc when cancelling\n\t\t\t\t\tfrappe.throw(_(\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\")\\\n\t\t\t\t\t\t.format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\n\ndef delete_linked_todos(doc):\n\tdelete_doc(\"ToDo\", frappe.db.sql_list(\"\"\"select name from `tabToDo`\n\t\twhere reference_type=%s and reference_name=%s\"\"\", (doc.doctype, doc.name)),\n\t\tignore_permissions=True)\n\ndef delete_email_subscribe(doc):\n\tfrappe.db.sql('''delete from `tabEmail Unsubscribe`\n\t\twhere reference_doctype=%s and reference_name=%s''', (doc.doctype, doc.name))\n\ndef delete_linked_communications(doc):\n\t# delete comments\n\tfrappe.db.sql(\"\"\"delete from `tabCommunication`\n\t\twhere\n\t\t\tcommunication_type = 'Comment'\n\t\t\tand reference_doctype=%s and reference_name=%s\"\"\", (doc.doctype, doc.name))\n\n\t# make communications orphans\n\tfrappe.db.sql(\"\"\"update `tabCommunication`\n\t\tset reference_doctype=null, reference_name=null\n\t\twhere\n\t\t\tcommunication_type = 'Communication'\n\t\t\tand reference_doctype=%s\n\t\t\tand reference_name=%s\"\"\", (doc.doctype, doc.name))\n\n\t# make secondary references orphans\n\tfrappe.db.sql(\"\"\"update `tabCommunication`\n\t\tset link_doctype=null, link_name=null\n\t\twhere link_doctype=%s and link_name=%s\"\"\", (doc.doctype, doc.name))\n\n\tfrappe.db.sql(\"\"\"update `tabCommunication`\n\t\tset timeline_doctype=null, timeline_name=null\n\t\twhere timeline_doctype=%s and timeline_name=%s\"\"\", (doc.doctype, doc.name))\n\ndef insert_feed(doc):\n\tfrom frappe.utils import get_fullname\n\n\tif frappe.flags.in_install or frappe.flags.in_import or getattr(doc, \"no_feed_on_delete\", False):\n\t\treturn\n\n\tfrappe.get_doc({\n\t\t\"doctype\": \"Communication\",\n\t\t\"communication_type\": \"Comment\",\n\t\t\"comment_type\": \"Deleted\",\n\t\t\"reference_doctype\": doc.doctype,\n\t\t\"subject\": \"{0} {1}\".format(_(doc.doctype), doc.name),\n\t\t\"full_name\": get_fullname(doc.owner)\n\t}).insert(ignore_permissions=True)\n\ndef delete_shared(doc):\n\tdelete_doc(\"DocShare\", frappe.db.sql_list(\"\"\"select name from `tabDocShare`\n\t\twhere share_doctype=%s and share_name=%s\"\"\", (doc.doctype, doc.name)), ignore_on_trash=True)\n"}
{"repo_name":"kcompher/BuildingMachineLearningSystemsWithPython","ref":"refs/heads/master","path":"ch03/rel_post_01.py","content":"# This code is supporting material for the book\n# Building Machine Learning Systems with Python\n# by Willi Richert and Luis Pedro Coelho\n# published by PACKT Publishing\n#\n# It is made available under the MIT License\n\nimport os\nimport sys\n\nimport scipy as sp\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nDIR = r\"../data/toy\"\nposts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n\nnew_post = \"imaging databases\"\n\nimport nltk.stem\nenglish_stemmer = nltk.stem.SnowballStemmer('english')\n\n\nclass StemmedCountVectorizer(CountVectorizer):\n\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n\n# vectorizer = CountVectorizer(min_df=1, stop_words='english',\n# preprocessor=stemmer)\nvectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nclass StemmedTfidfVectorizer(TfidfVectorizer):\n\n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n\nvectorizer = StemmedTfidfVectorizer(\n    min_df=1, stop_words='english', charset_error='ignore')\nprint(vectorizer)\n\nX_train = vectorizer.fit_transform(posts)\n\nnum_samples, num_features = X_train.shape\nprint(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n\nnew_post_vec = vectorizer.transform([new_post])\nprint(new_post_vec, type(new_post_vec))\nprint(new_post_vec.toarray())\nprint(vectorizer.get_feature_names())\n\n\ndef dist_raw(v1, v2):\n    delta = v1 - v2\n    return sp.linalg.norm(delta.toarray())\n\n\ndef dist_norm(v1, v2):\n    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n\n    delta = v1_normalized - v2_normalized\n\n    return sp.linalg.norm(delta.toarray())\n\ndist = dist_norm\n\nbest_dist = sys.maxsize\nbest_i = None\n\nfor i in range(0, num_samples):\n    post = posts[i]\n    if post == new_post:\n        continue\n    post_vec = X_train.getrow(i)\n    d = dist(post_vec, new_post_vec)\n\n    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n\n    if d \u003c best_dist:\n        best_dist = d\n        best_i = i\n\nprint(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n"}
{"repo_name":"petecummings/django-cms","ref":"refs/heads/develop","path":"cms/south_migrations/0015_modified_by_added.py","content":"# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\ntry:\n    from django.contrib.auth import get_user_model\nexcept ImportError: # django \u003c 1.5\n    from django.contrib.auth.models import User\nelse:\n    User = get_user_model()\n\nuser_orm_label = '%s.%s' % (User._meta.app_label, User._meta.object_name)\nuser_model_label = '%s.%s' % (User._meta.app_label, User._meta.model_name)\nuser_ptr_name = '%s_ptr' % User._meta.object_name.lower()\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        # Dummy migration\n        pass\n\n\n\n    def backwards(self, orm):\n    # Dummy migration\n        pass\n\n\n    models = {\n        'auth.group': {\n            'Meta': {'object_name': 'Group'},\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [],\n                     {'unique': 'True', 'max_length': '80'}),\n            'permissions': ('django.db.models.fields.related.ManyToManyField', [],\n                            {'to': \"orm['auth.Permission']\", 'symmetrical': 'False',\n                             'blank': 'True'})\n        },\n        'auth.permission': {\n            'Meta': {\n                'ordering': \"('content_type__app_label', 'content_type__model', 'codename')\",\n                'unique_together': \"(('content_type', 'codename'),)\",\n                'object_name': 'Permission'},\n            'codename': (\n                'django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'content_type': ('django.db.models.fields.related.ForeignKey', [],\n                             {'to': \"orm['contenttypes.ContentType']\"}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'})\n        },\n        user_model_label: {\n            'Meta': {'object_name': User.__name__, 'db_table': \"'%s'\" % User._meta.db_table},\n            'date_joined': ('django.db.models.fields.DateTimeField', [],\n                            {'default': 'datetime.datetime.now'}),\n            'email': ('django.db.models.fields.EmailField', [],\n                      {'max_length': '75', 'blank': 'True'}),\n            'first_name': ('django.db.models.fields.CharField', [],\n                           {'max_length': '30', 'blank': 'True'}),\n            'groups': ('django.db.models.fields.related.ManyToManyField', [],\n                       {'to': \"orm['auth.Group']\", 'symmetrical': 'False',\n                        'blank': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'is_active': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'is_staff': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'is_superuser': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'last_login': ('django.db.models.fields.DateTimeField', [],\n                           {'default': 'datetime.datetime.now'}),\n            'last_name': ('django.db.models.fields.CharField', [],\n                          {'max_length': '30', 'blank': 'True'}),\n            'password': (\n                'django.db.models.fields.CharField', [], {'max_length': '128'}),\n            'user_permissions': (\n                'django.db.models.fields.related.ManyToManyField', [],\n                {'to': \"orm['auth.Permission']\", 'symmetrical': 'False',\n                 'blank': 'True'}),\n            'username': ('django.db.models.fields.CharField', [],\n                         {'unique': 'True', 'max_length': '30'})\n        },\n        'cms.cmsplugin': {\n            'Meta': {'object_name': 'CMSPlugin'},\n            'changed_date': ('django.db.models.fields.DateTimeField', [],\n                             {'auto_now': 'True', 'blank': 'True'}),\n            'creation_date': ('django.db.models.fields.DateTimeField', [],\n                              {'default': 'datetime.datetime.now'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'language': ('django.db.models.fields.CharField', [],\n                         {'max_length': '15', 'db_index': 'True'}),\n            'level': ('django.db.models.fields.PositiveIntegerField', [],\n                      {'db_index': 'True'}),\n            'lft': ('django.db.models.fields.PositiveIntegerField', [],\n                    {'db_index': 'True'}),\n            'parent': ('django.db.models.fields.related.ForeignKey', [],\n                       {'to': \"orm['cms.CMSPlugin']\", 'null': 'True',\n                        'blank': 'True'}),\n            'placeholder': ('django.db.models.fields.related.ForeignKey', [],\n                            {'to': \"orm['cms.Placeholder']\", 'null': 'True'}),\n            'plugin_type': ('django.db.models.fields.CharField', [],\n                            {'max_length': '50', 'db_index': 'True'}),\n            'position': ('django.db.models.fields.PositiveSmallIntegerField', [],\n                         {'null': 'True', 'blank': 'True'}),\n            'rght': ('django.db.models.fields.PositiveIntegerField', [],\n                     {'db_index': 'True'}),\n            'tree_id': ('django.db.models.fields.PositiveIntegerField', [],\n                        {'db_index': 'True'})\n        },\n        'cms.globalpagepermission': {\n            'Meta': {'object_name': 'GlobalPagePermission'},\n            'can_add': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_change': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_change_advanced_settings': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'can_change_permissions': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'can_delete': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_moderate': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_move_page': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_publish': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_recover_page': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_view': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'group': ('django.db.models.fields.related.ForeignKey', [],\n                      {'to': \"orm['auth.Group']\", 'null': 'True', 'blank': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'sites': ('django.db.models.fields.related.ManyToManyField', [],\n                      {'symmetrical': 'False', 'to': \"orm['sites.Site']\",\n                       'null': 'True', 'blank': 'True'}),\n            'user': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['%s']\" % user_orm_label, 'null': 'True', 'blank': 'True'})\n        },\n        'cms.page': {\n            'Meta': {'ordering': \"('site', 'tree_id', 'lft')\",\n                     'object_name': 'Page'},\n            'changed_by': (\n                'django.db.models.fields.CharField', [], {'max_length': '70'}),\n            'changed_date': ('django.db.models.fields.DateTimeField', [],\n                             {'auto_now': 'True', 'blank': 'True'}),\n            'created_by': (\n                'django.db.models.fields.CharField', [], {'max_length': '70'}),\n            'creation_date': ('django.db.models.fields.DateTimeField', [],\n                              {'auto_now_add': 'True', 'blank': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'in_navigation': ('django.db.models.fields.BooleanField', [],\n                              {'default': 'True', 'db_index': 'True'}),\n            'level': ('django.db.models.fields.PositiveIntegerField', [],\n                      {'db_index': 'True'}),\n            'lft': ('django.db.models.fields.PositiveIntegerField', [],\n                    {'db_index': 'True'}),\n            'limit_visibility_in_menu': (\n                'django.db.models.fields.SmallIntegerField', [],\n                {'default': 'None', 'null': 'True', 'db_index': 'True',\n                 'blank': 'True'}),\n            'login_required': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'moderator_state': ('django.db.models.fields.SmallIntegerField', [],\n                                {'default': '1', 'blank': 'True'}),\n            'navigation_extenders': ('django.db.models.fields.CharField', [],\n                                     {'db_index': 'True', 'max_length': '80',\n                                      'null': 'True', 'blank': 'True'}),\n            'parent': ('django.db.models.fields.related.ForeignKey', [],\n                       {'blank': 'True', 'related_name': \"'children'\",\n                        'null': 'True', 'to': \"orm['cms.Page']\"}),\n            'placeholders': ('django.db.models.fields.related.ManyToManyField', [],\n                             {'to': \"orm['cms.Placeholder']\",\n                              'symmetrical': 'False'}),\n            'publication_date': ('django.db.models.fields.DateTimeField', [],\n                                 {'db_index': 'True', 'null': 'True',\n                                  'blank': 'True'}),\n            'publication_end_date': ('django.db.models.fields.DateTimeField', [],\n                                     {'db_index': 'True', 'null': 'True',\n                                      'blank': 'True'}),\n            'published': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'publisher_is_draft': ('django.db.models.fields.BooleanField', [],\n                                   {'default': 'True', 'db_index': 'True'}),\n            'publisher_public': (\n                'django.db.models.fields.related.OneToOneField', [],\n                {'related_name': \"'publisher_draft'\", 'unique': 'True', 'null': 'True',\n                 'to': \"orm['cms.Page']\"}),\n            'publisher_state': ('django.db.models.fields.SmallIntegerField', [],\n                                {'default': '0', 'db_index': 'True'}),\n            'reverse_id': ('django.db.models.fields.CharField', [],\n                           {'db_index': 'True', 'max_length': '40', 'null': 'True',\n                            'blank': 'True'}),\n            'rght': ('django.db.models.fields.PositiveIntegerField', [],\n                     {'db_index': 'True'}),\n            'site': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['sites.Site']\"}),\n            'soft_root': ('django.db.models.fields.BooleanField', [],\n                          {'default': 'False', 'db_index': 'True'}),\n            'template': (\n                'django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'tree_id': ('django.db.models.fields.PositiveIntegerField', [],\n                        {'db_index': 'True'})\n        },\n        'cms.pagemoderator': {\n            'Meta': {'object_name': 'PageModerator'},\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'moderate_children': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'moderate_descendants': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'moderate_page': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'page': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['cms.Page']\"}),\n            'user': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['%s']\" % user_orm_label})\n        },\n        'cms.pagemoderatorstate': {\n            'Meta': {'ordering': \"('page', 'action', '-created')\",\n                     'object_name': 'PageModeratorState'},\n            'action': ('django.db.models.fields.CharField', [],\n                       {'max_length': '3', 'null': 'True', 'blank': 'True'}),\n            'created': ('django.db.models.fields.DateTimeField', [],\n                        {'auto_now_add': 'True', 'blank': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'message': ('django.db.models.fields.TextField', [],\n                        {'default': \"''\", 'max_length': '1000', 'blank': 'True'}),\n            'page': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['cms.Page']\"}),\n            'user': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['%s']\" % user_orm_label, 'null': 'True'})\n        },\n        'cms.pagepermission': {\n            'Meta': {'object_name': 'PagePermission'},\n            'can_add': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_change': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_change_advanced_settings': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'can_change_permissions': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'can_delete': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_moderate': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_move_page': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_publish': (\n                'django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'can_view': (\n                'django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'grant_on': (\n                'django.db.models.fields.IntegerField', [], {'default': '5'}),\n            'group': ('django.db.models.fields.related.ForeignKey', [],\n                      {'to': \"orm['auth.Group']\", 'null': 'True', 'blank': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'page': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['cms.Page']\", 'null': 'True', 'blank': 'True'}),\n            'user': ('django.db.models.fields.related.ForeignKey', [],\n                     {'to': \"orm['%s']\" % user_orm_label, 'null': 'True', 'blank': 'True'})\n        },\n        'cms.pageuser': {\n            'Meta': {'object_name': 'PageUser', '_ormbases': [user_orm_label]},\n            'created_by': ('django.db.models.fields.related.ForeignKey', [],\n                           {'related_name': \"'created_users'\",\n                            'to': \"orm['%s']\" % user_orm_label}),\n            'user_ptr': ('django.db.models.fields.related.OneToOneField', [],\n                         {'to': \"orm['%s']\" % user_orm_label, 'unique': 'True',\n                          'primary_key': 'True'})\n        },\n        'cms.pageusergroup': {\n            'Meta': {'object_name': 'PageUserGroup', '_ormbases': ['auth.Group']},\n            'created_by': ('django.db.models.fields.related.ForeignKey', [],\n                           {'related_name': \"'created_usergroups'\",\n                            'to': \"orm['%s']\" % user_orm_label}),\n            'group_ptr': ('django.db.models.fields.related.OneToOneField', [],\n                          {'to': \"orm['auth.Group']\", 'unique': 'True',\n                           'primary_key': 'True'})\n        },\n        'cms.placeholder': {\n            'Meta': {'object_name': 'Placeholder'},\n            'default_width': (\n                'django.db.models.fields.PositiveSmallIntegerField', [],\n                {'null': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'slot': ('django.db.models.fields.CharField', [],\n                     {'max_length': '50', 'db_index': 'True'})\n        },\n        'cms.title': {\n            'Meta': {'unique_together': \"(('language', 'page'),)\",\n                     'object_name': 'Title'},\n            'application_urls': ('django.db.models.fields.CharField', [],\n                                 {'db_index': 'True', 'max_length': '200',\n                                  'null': 'True', 'blank': 'True'}),\n            'creation_date': ('django.db.models.fields.DateTimeField', [],\n                              {'default': 'datetime.datetime.now'}),\n            'has_url_overwrite': ('django.db.models.fields.BooleanField', [],\n                                  {'default': 'False', 'db_index': 'True'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'language': ('django.db.models.fields.CharField', [],\n                         {'max_length': '15', 'db_index': 'True'}),\n            'menu_title': ('django.db.models.fields.CharField', [],\n                           {'max_length': '255', 'null': 'True', 'blank': 'True'}),\n            'meta_description': ('django.db.models.fields.TextField', [],\n                                 {'max_length': '255', 'null': 'True',\n                                  'blank': 'True'}),\n            'meta_keywords': ('django.db.models.fields.CharField', [],\n                              {'max_length': '255', 'null': 'True',\n                               'blank': 'True'}),\n            'page': ('django.db.models.fields.related.ForeignKey', [],\n                     {'related_name': \"'title_set'\", 'to': \"orm['cms.Page']\"}),\n            'page_title': ('django.db.models.fields.CharField', [],\n                           {'max_length': '255', 'null': 'True', 'blank': 'True'}),\n            'path': ('django.db.models.fields.CharField', [],\n                     {'max_length': '255', 'db_index': 'True'}),\n            'redirect': ('django.db.models.fields.CharField', [],\n                         {'max_length': '255', 'null': 'True', 'blank': 'True'}),\n            'slug': (\n                'django.db.models.fields.SlugField', [], {'max_length': '255'}),\n            'title': (\n                'django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        'contenttypes.contenttype': {\n            'Meta': {'ordering': \"('name',)\",\n                     'unique_together': \"(('app_label', 'model'),)\",\n                     'object_name': 'ContentType',\n                     'db_table': \"'django_content_type'\"},\n            'app_label': (\n                'django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'model': (\n                'django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '100'})\n        },\n        'sites.site': {\n            'Meta': {'ordering': \"('domain',)\", 'object_name': 'Site',\n                     'db_table': \"'django_site'\"},\n            'domain': (\n                'django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'id': (\n                'django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'})\n        }\n    }\n\n    complete_apps = ['cms']\n"}
{"repo_name":"chouseknecht/ansible","ref":"refs/heads/devel","path":"lib/ansible/module_utils/network/eos/argspec/facts/facts.py","content":"# -*- coding: utf-8 -*-\n# Copyright 2019 Red Hat\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\"\"\"\nThe arg spec for the eos facts module.\n\"\"\"\n\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\n\nclass FactsArgs(object):\n    \"\"\" The arg spec for the eos facts module\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    argument_spec = {\n        'gather_subset': dict(default=['!config'], type='list'),\n        'gather_network_resources': dict(type='list'),\n    }\n"}
{"repo_name":"alokotosh/mm-master","ref":"refs/heads/master","path":"mm/commands/misc.py","content":"import os\nimport json\nimport mm.util as util\nimport mm.config as config\nfrom mm.exceptions import *\nfrom mm.basecommand import Command\nfrom mm.sfdc_client import MavensMateClient\n\nclass GetActiveSessionCommand(Command):\n    def execute(self):\n        if 'username' not in self.params or self.params['username'] == None or self.params['username'] == '':\n            raise MMException('Please enter a Salesforce.com username')\n        if 'password' not in self.params or self.params['password'] == None or self.params['password'] == '':\n            raise MMException('Please enter a Salesforce.com password')\n        if 'org_type' not in self.params or self.params['org_type'] == None or self.params['org_type'] == '':\n            raise MMException('Please select an org type')\n        if 'org_type' in self.params and self.params['org_type'] == \"custom\" and \"org_url\" not in self.params:\n            raise MMException('To use a custom org type, please include a org_url parameter') \n        if 'org_type' in self.params and self.params['org_type'] == \"custom\" and \"org_url\" in self.params and self.params[\"org_url\"] == \"\":\n            raise MMException('Please specify the org url')    \n\n        config.logger.debug('=================\u003e')\n        config.logger.debug(self.params)\n\n        client = MavensMateClient(credentials={\n            \"username\" : self.params['username'],\n            \"password\" : self.params['password'],\n            \"org_type\" : self.params['org_type'],\n            \"org_url\"  : self.params.get('org_url', None)\n        }) \n        \n        response = {\n            \"sid\"                   : client.sid,\n            \"user_id\"               : client.user_id,\n            \"metadata_server_url\"   : client.metadata_server_url,\n            \"server_url\"            : client.server_url,\n            \"metadata\"              : client.get_org_metadata(subscription=self.params.get('subscription', None)),\n            \"org_metadata_types\"    : util.metadata_types(),\n            \"success\"               : True\n        }\n        return util.generate_response(response)\n\nclass IndexApexSymbolsCommand(Command):\n    aliases=[\"index_apex\",\"index_apex_file_properties\"]\n    \"\"\"\n        Updates symbol index for one or more Apex Classes. If files is not included or empty, will force a full refresh\n    \"\"\"\n    def execute(self):\n        return config.project.index_apex_symbols(self.params.get(\"files\", None))\n\nclass ResetMetadataContainerCommand(Command):\n    def execute(self):\n        return config.project.reset_metadata_container(accept=\"json\")\n\nclass OpenFileInClientCommand(Command):\n    \"\"\"\n        Opens the requested files in the plugin client (Sublime Text, etc.)\n    \"\"\"\n    def execute(self):\n        file_name = self.params[\"file_name\"]\n        extension = util.get_file_extension_no_period(file_name)\n        mtype = util.get_meta_type_by_suffix(extension)\n        full_file_path = os.path.join(config.project.location, \"src\", mtype[\"directoryName\"], file_name)\n        params = {\n            \"project_name\"  : config.project.project_name,\n            \"file_name\"     : full_file_path,\n            \"line_number\"   : self.params.get(\"line_number\", 0)\n        } \n        config.connection.run_subl_command(\"open_file_in_project\", json.dumps(params))\n        return util.generate_success_response(\"ok\")\n\nclass ExecuteApexCommand(Command):\n    aliases=[\"run_apex_script\"]\n    \"\"\"\n        executes a string of apex\n    \"\"\"\n    def execute(self):\n        if 'script_name' in self.params: #running an apex script\n            self.params[\"body\"] = util.get_file_as_string(os.path.join(config.project.location,\"apex-scripts\",self.params[\"script_name\"]))\n        if 'debug_categories' not in self.params and not os.path.isfile(os.path.join(config.project.location,\"config\",\".apex_script\")):\n            self.params[\"debug_categories\"] = [\n                {\n                    \"category\"  : \"Apex_code\",\n                    \"level\"     :  \"DEBUG\"\n                }\n            ]\n        elif os.path.isfile(os.path.join(config.project.location,\"config\",\".apex_script\")):\n            log_settings = util.parse_json_from_file(os.path.join(config.project.location,\"config\",\".apex_script\"))\n            categories = []\n            levels = log_settings[\"levels\"]\n            for category in levels.keys():\n                categories.append({\n                    \"category\"  : category,\n                    \"level\"     : levels[category]\n                })\n            self.params[\"debug_categories\"] = categories\n        elif 'debug_categories' not in self.params:\n            self.params[\"debug_categories\"] = [\n                {\n                    \"category\"  : \"Apex_code\",\n                    \"level\"     :  \"DEBUG\"\n                }\n            ]\n        return_log = self.params.get(\"return_log\", True)\n\n        execute_result = config.sfdc_client.execute_apex(self.params)\n        result = {\n            'column'                : execute_result['column'],\n            'compileProblem'        : execute_result['compileProblem'],\n            'compiled'              : execute_result['compiled'],\n            'exceptionMessage'      : execute_result['exceptionMessage'],\n            'exceptionStackTrace'   : execute_result['exceptionStackTrace'],\n            'line'                  : execute_result['line'],\n            'success'               : execute_result['success'],\n        }\n        if 'log' in execute_result and return_log:\n            result['log'] = execute_result['log']\n        if result['success']:\n            log_apex = config.connection.get_plugin_client_setting('mm_log_anonymous_apex', False)\n            if log_apex:\n                location = config.project.log_anonymous_apex(self.params['body'], execute_result['log'], self.params.get(\"script_name\", None))\n                result[\"log_location\"] = location\n        return util.generate_response(result)\n\nclass SignInWithGithubCommand(Command):\n    def execute(self):\n        return config.connection.sign_in_with_github(self.params)"}
{"repo_name":"nischalsheth/contrail-controller","ref":"refs/heads/master","path":"src/vnsw/provisioning/contrail_vrouter_provisioning/network.py","content":"#!/usr/bin/env python\n#\n# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.\n#\n\nimport os\nimport re\nimport glob\nimport struct\nimport socket\nimport logging\nimport netifaces\n\nfrom contrail_vrouter_provisioning import local\n\n\nlog = logging.getLogger('contrail_vrouter_provisioning.network')\n\n\nclass ComputeNetworkSetup(object):\n    def find_gateway(self, dev):\n        gateway = ''\n        cmd = \"sudo netstat -rn | sudo grep ^\\\"0.0.0.0\\\" | \"\n        cmd += \"sudo head -n 1 | sudo grep %s | sudo awk '{ print $2 }'\" % dev\n        gateway = local(cmd, capture=True).strip()\n        return gateway\n    # end find_gateway\n\n    def get_dns_servers(self, dev):\n        cmd = \"sudo grep \\\"^nameserver\\\\\u003e\\\" /etc/resolv.conf | \"\n        cmd += \"sudo awk  '{print $2}'\"\n        dns_list = local(cmd, capture=True)\n        return dns_list.split()\n    # end get_dns_servers\n\n    def get_domain_search_list(self):\n        domain_list = ''\n        cmd = \"sudo grep ^\\\"search\\\" /etc/resolv.conf | \"\n        cmd += \"sudo awk '{$1=\\\"\\\";print $0}'\"\n        domain_list = local(cmd, capture=True).strip()\n        if not domain_list:\n            cmd = \"sudo grep ^\\\"domain\\\" /etc/resolv.conf | \"\n            cmd += \"sudo awk '{$1=\\\"\\\"; print $0}'\"\n            domain_list = local(cmd, capture=True).strip()\n        return domain_list\n\n    def get_if_mtu(self, dev):\n        cmd = \"sudo ifconfig %s | sudo grep mtu | sudo awk '{ print $NF }'\" %\\\n               dev\n        mtu = local(cmd, capture=True).strip()\n        if not mtu:\n            # for debian\n            cmd = r\"sudo ifconfig %s | sudo grep MTU | \" % dev\n            cmd += r\"sudo sed 's/.*MTU.\\([0-9]\\+\\).*/\\1/g'\"\n            mtu = local(cmd, capture=True).strip()\n        if (mtu and mtu != '1500'):\n            return mtu\n        return ''\n    # end if_mtu\n\n    def get_device_by_ip(self, ip):\n        for i in netifaces.interfaces():\n            try:\n                if i == 'pkt1':\n                    continue\n                if netifaces.AF_INET in netifaces.ifaddresses(i):\n                    interfaces = netifaces.ifaddresses(i)[netifaces.AF_INET]\n                    for interface in interfaces:\n                        if ip == interface['addr']:\n                            if i == 'vhost0':\n                                log.info(\"vhost0 is already present!\")\n                            return i\n            except ValueError:\n                log.info(\"Skipping interface %s\", i)\n        raise RuntimeError('%s not configured, rerun w/ --physical_interface' %\n                           ip)\n    # end get_device_by_ip\n\n    def get_device_info(self, ip):\n        reprov = False\n        cfg_file = \"/etc/contrail/contrail-vrouter-agent.conf\"\n        try:\n            dev = self.get_device_by_ip(ip)\n            if dev == \"vhost0\":\n                dev = self.get_config(cfg_file,\n                                      \"VIRTUAL-HOST-INTERFACE\",\n                                      \"physical_interface\")\n                log.info(\"Re-provision. vhost0 present\")\n                reprov = True\n            else:\n                log.info(\"Fresh Install. vhost0 not present\")\n        except RuntimeError:\n            dev = self.get_config(cfg_file,\n                                  \"VIRTUAL-HOST-INTERFACE\",\n                                  \"physical_interface\")\n            if not dev.succeeded:\n                raise\n            log.info(\"vhost0 not present, vrouter not running\")\n            reprov = True\n        return (dev.strip(), reprov)\n    # end get_device_info\n\n    def get_secondary_device(self, primary):\n        for i in netifaces.interfaces():\n            try:\n                if i == 'pkt1':\n                    continue\n                if i == primary:\n                    continue\n                if i == 'vhost0':\n                    continue\n                if netifaces.AF_INET not in netifaces.ifaddresses(i):\n                    return i\n            except ValueError:\n                log.info(\"Skipping interface %s\" % i)\n        raise RuntimeError('Secondary interace  not configured,',\n                           'rerun w/ --physical_interface')\n    # end get_secondary_device\n\n    def get_if_mac(self, dev):\n        iface_addr = netifaces.ifaddresses(dev)\n        link_info = iface_addr[netifaces.AF_LINK]\n        mac_addr = link_info[0]['addr']\n        return mac_addr\n    # end get_if_mac\n\n    @staticmethod\n    def is_interface_vlan(interface):\n        iface = local(\"sudo ip link show %s | head -1\" % interface +\n                      \"| cut -f2 -d':' | grep '@'\",\n                      capture=True, warn_only=True)\n        if iface.succeeded:\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def get_physical_interface_of_vlan(interface):\n        iface = local(\"sudo ip link show %s | head -1 | cut -f2 -d':'\" %\n                      interface + \"| cut -f2 -d'@'\", capture=True)\n        return iface\n\n    def _rewrite_ifcfg_file(self, filename, dev, prsv_cfg):\n        bond = False\n        mac = ''\n        temp_dir_name = self._temp_dir_name\n\n        vlan = False\n        if os.path.isfile('/proc/net/vlan/%s' % dev):\n            vlan_info = open('/proc/net/vlan/config').readlines()\n            match = re.search('^%s.*\\|\\s+(\\S+)$' % dev, \"\\n\".join(vlan_info),\n                              flags=re.M | re.I)\n            if not match:\n                raise RuntimeError(\"Configured vlan %s is not found in\",\n                                   \"/proc/net/vlan/config\" % dev)\n            vlan = True\n\n        if os.path.isdir('/sys/class/net/%s/bonding' % dev):\n            bond = True\n        # end if os.path.isdir...\n\n        mac = netifaces.ifaddresses(dev)[netifaces.AF_LINK][0]['addr']\n        ifcfg_file = '/etc/sysconfig/network-scripts/ifcfg-%s' % dev\n        if not os.path.isfile(ifcfg_file):\n            ifcfg_file = temp_dir_name + 'ifcfg-' + dev\n            with open(ifcfg_file, 'w') as f:\n                f.write('''#Contrail %s\nTYPE=Ethernet\nONBOOT=yes\nDEVICE=\"%s\"\nUSERCTL=yes\nNM_CONTROLLED=no\nHWADDR=%s\n''' % (dev, dev, mac))\n                for dcfg in prsv_cfg:\n                    f.write(dcfg+'\\n')\n                if vlan:\n                    f.write('VLAN=yes\\n')\n        fd = open(ifcfg_file)\n        f_lines = fd.readlines()\n        fd.close()\n        local(\"sudo rm -f %s\" % ifcfg_file)\n        new_f_lines = []\n        remove_items = ['IPADDR', 'NETMASK', 'PREFIX', 'GATEWAY', 'HWADDR',\n                        'DNS1', 'DNS2', 'BOOTPROTO', 'NM_CONTROLLED',\n                        '#Contrail']\n\n        remove_items.append('DEVICE')\n        new_f_lines.append('#Contrail %s\\n' % dev)\n        new_f_lines.append('DEVICE=%s\\n' % dev)\n\n        for line in f_lines:\n            found = False\n            for text in remove_items:\n                if text in line:\n                    found = True\n            if not found:\n                new_f_lines.append(line)\n\n        new_f_lines.append('NM_CONTROLLED=no\\n')\n        if bond:\n            new_f_lines.append('SUBCHANNELS=1,2,3\\n')\n        elif not vlan:\n            new_f_lines.append('HWADDR=%s\\n' % mac)\n\n        fdw = open(filename, 'w')\n        fdw.writelines(new_f_lines)\n        fdw.close()\n\n    def migrate_routes(self, device):\n        '''\n        add route entries in /proc/net/route\n        '''\n        temp_dir_name = self._temp_dir_name\n        cfg_file = '/etc/sysconfig/network-scripts/route-vhost0'\n        tmp_file = '%s/route-vhost0' % temp_dir_name\n        with open(tmp_file, 'w') as route_cfg_file:\n            for route in open('/proc/net/route', 'r').readlines():\n                if route.startswith(device):\n                    route_fields = route.split()\n                    destination = int(route_fields[1], 16)\n                    gateway = int(route_fields[2], 16)\n                    flags = int(route_fields[3], 16)\n                    mask = int(route_fields[7], 16)\n                    if flags \u0026 0x2:\n                        if destination != 0:\n                            route_cfg_file.write(socket.inet_ntoa(\n                                struct.pack('I', destination)))\n                            route_cfg_file.write(\n                                    '/' + str(bin(mask).count('1')) + ' ')\n                            route_cfg_file.write('via ')\n                            route_cfg_file.write(socket.inet_ntoa(\n                                struct.pack('I', gateway)) + ' ')\n                            route_cfg_file.write('dev vhost0')\n                        # end if detination...\n                    # end if flags \u0026...\n                # end if route.startswith...\n            # end for route...\n        # end with open...\n        local(\"sudo mv -f %s %s\" % (tmp_file, cfg_file))\n        # delete the route-dev file\n        if os.path.isfile('/etc/sysconfig/network-scripts/route-%s' % device):\n            os.unlink('/etc/sysconfig/network-scripts/route-%s' % device)\n    # end def migrate_routes\n\n    def get_cfgfile_for_dev(self, iface, cfg_files):\n        if not cfg_files:\n            return None\n        mapped_intf_cfgfile = None\n        for file in cfg_files:\n            with open(file, 'r') as fd:\n                contents = fd.read()\n                regex = '(?:^|\\n)\\s*iface\\s+%s\\s+' % iface\n                if re.search(regex, contents):\n                    mapped_intf_cfgfile = file\n        return mapped_intf_cfgfile\n\n    def get_sourced_files(self):\n        '''Get all sourced config files'''\n        files = self.get_valid_files(self.get_source_entries())\n        files += self.get_source_directory_files()\n        return list(set(files))\n\n    def get_source_directory_files(self):\n        '''Get source-directory entry and make list of valid files'''\n        regex = '(?:^|\\n)\\s*source-directory\\s+(\\S+)'\n        files = list()\n        with open(self.default_cfg_file, 'r') as fd:\n            entries = re.findall(regex, fd.read())\n        dirs = [d for d in self.get_valid_files(entries) if os.path.isdir(d)]\n        for dir in dirs:\n            files.extend([os.path.join(dir, f) for f in os.listdir(dir)\n                          if os.path.isfile(os.path.join(dir, f)) and\n                          re.match('^[a-zA-Z0-9_-]+$', f)])\n        return files\n\n    def get_source_entries(self):\n        '''\n        Get entries matching source keyword from\n        /etc/network/interfaces file.\n        '''\n        regex = '(?:^|\\n)\\s*source\\s+(\\S+)'\n        with open(self.default_cfg_file, 'r') as fd:\n            return re.findall(regex, fd.read())\n\n    def get_valid_files(self, entries):\n        '''Provided a list of glob'd strings, return matching file names'''\n        files = list()\n        prepend = os.path.join(os.path.sep, 'etc', 'network') + os.path.sep\n        for entry in entries:\n            entry = entry.lstrip('./') if entry.startswith('./') else entry\n            if entry.startswith(os.path.sep):\n                entry = entry\n            else:\n                entry = prepend+entry\n            files.extend(glob.glob(entry))\n        return files\n\n    def _rewrite_net_interfaces_file(self, dev, mac, vhost_ip,\n                                     netmask, gateway_ip, esxi_vm,\n                                     vmpg_mtu, datapg_mtu):\n        self.default_cfg_file = '/etc/network/interfaces'\n        cfg_files = self.get_sourced_files()\n        cfg_files.append(self.default_cfg_file)\n        intf_cfgfile = self.get_cfgfile_for_dev('vhost0', cfg_files)\n        if intf_cfgfile:\n            log.info(\"Interface vhost0 is already present in\" +\n                     \"/etc/network/interfaces\")\n            log.info(\"Skipping rewrite of this file\")\n            return\n        # endif\n\n        vlan = False\n        if os.path.isfile('/proc/net/vlan/%s' % dev):\n            vlan_info = open('/proc/net/vlan/config').readlines()\n            match = re.search('^%s.*\\|\\s+(\\S+)$' % dev, \"\\n\".join(vlan_info),\n                              flags=re.M | re.I)\n            if not match:\n                raise RuntimeError('Configured vlan %s is not found in',\n                                   '/proc/net/vlan/config' % dev)\n            phydev = match.group(1)\n            vlan = True\n\n        # Replace strings matching dev to vhost0 in ifup and ifdown parts file\n        # Any changes to the file/logic with static routes has to be\n        # reflected in setup-vnc-static-routes.py too\n        ifup_parts_file = os.path.join(os.path.sep, 'etc', 'network',\n                                       'if-up.d', 'routes')\n        ifdown_parts_file = os.path.join(os.path.sep, 'etc', 'network',\n                                         'if-down.d', 'routes')\n\n        if (os.path.isfile(ifup_parts_file) and\n                os.path.isfile(ifdown_parts_file)):\n            local(\"sudo sed -i 's/%s/vhost0/g' %s\" % (dev, ifup_parts_file),\n                  warn_only=True)\n            local(\"sudo sed -i 's/%s/vhost0/g' %s\" % (dev, ifdown_parts_file),\n                  warn_only=True)\n\n        dev_cfgfile = self.get_cfgfile_for_dev(dev, cfg_files)\n        temp_intf_file = '%s/interfaces' % self._temp_dir_name\n        local(\"sudo cp %s %s\" % (dev_cfgfile, temp_intf_file))\n        with open(dev_cfgfile, 'r') as fd:\n            cfg_file = fd.read()\n\n        if not self._args.non_mgmt_ip:\n            # remove entry from auto \u003cdev\u003e to auto excluding these pattern\n            # then delete specifically auto \u003cdev\u003e\n            local(\"sudo sed -i '/auto %s/,/auto/{/auto/!d}' %s\" %\n                  (dev, temp_intf_file))\n            local(\"sudo sed -i '/auto %s/d' %s\" % (dev, temp_intf_file))\n            # add manual entry for dev\n            local(\"sudo echo 'auto %s' \u003e\u003e %s\" % (dev, temp_intf_file))\n            local(\"sudo echo 'iface %s inet manual' \u003e\u003e %s\" %\n                  (dev, temp_intf_file))\n            if vlan:\n                local(\"sudo echo '    post-up ifconfig %s up' \u003e\u003e %s\" %\n                      (dev, temp_intf_file))\n                local(\"sudo echo '    pre-down ifconfig %s down' \u003e\u003e %s\" %\n                      (dev, temp_intf_file))\n            else:\n                local(\"sudo echo '    pre-up ifconfig %s up' \u003e\u003e %s\" %\n                      (dev, temp_intf_file))\n                local(\"sudo echo '    post-down ifconfig %s down' \u003e\u003e %s\" %\n                      (dev, temp_intf_file))\n            if esxi_vm:\n                    local(\"sudo echo '    pre-up ifconfig %s up mtu %s' \u003e\u003e %s\"\n                          % (dev, datapg_mtu, temp_intf_file))\n                    cmd = \"sudo ethtool -i %s | grep driver | cut -f 2 -d ' '\"\\\n                          % dev\n                    device_driver = local(cmd, capture=True)\n                    if (device_driver == \"vmxnet3\"):\n                        cmd = \"sudo echo '    pre-up ethtool --offload \"\n                        rx_cmd = (cmd +\n                                  \"%s rx off' \u003e\u003e %s\" % (dev, temp_intf_file))\n                        tx_cmd = (cmd +\n                                  \"%s tx off' \u003e\u003e %s\" % (dev, temp_intf_file))\n                        local(rx_cmd)\n                        local(tx_cmd)\n            if vlan:\n                local(\"sudo echo '    vlan-raw-device %s' \u003e\u003e %s\" %\n                      (phydev, temp_intf_file))\n            if 'bond' in dev.lower():\n                iters = re.finditer('^\\s*auto\\s', cfg_file, re.M)\n                indices = [pat_match.start() for pat_match in iters]\n                matches = map(cfg_file.__getslice__, indices,\n                              indices[1:] + [len(cfg_file)])\n                for each in matches:\n                    each = each.strip()\n                    if re.match('^auto\\s+%s' % dev, each):\n                        string = ''\n                        for lines in each.splitlines():\n                            if 'bond-' in lines:\n                                string += lines+os.linesep\n                        local(\"sudo echo '%s' \u003e\u003e %s\" % (string,\n                                                        temp_intf_file))\n                    else:\n                        continue\n            local(\"sudo echo '' \u003e\u003e %s\" % temp_intf_file)\n        else:\n            # remove ip address and gateway\n            local(\"sudo sed -i '/iface %s inet static/, +2d' %s\" %\n                  (dev, temp_intf_file), warn_only=True)\n            if esxi_vm:\n                local(\"sudo echo '    pre-up ifconfig %s up mtu %s' \u003e\u003e %s\" %\n                      (dev, datapg_mtu, temp_intf_file), warn_only=True)\n                cmd = \"sudo ethtool -i %s | \" % dev\n                cmd += \"sudo grep driver | sudo cut -f 2 -d ' '\"\n                device_driver = local(cmd, capture=True, warn_only=True)\n                if (device_driver == \"vmxnet3\"):\n                    cmd = \"sudo echo '    pre-up ethtool --offload \"\n                    rx_cmd = cmd + \"%s rx off' \u003e\u003e %s\" % (dev, temp_intf_file)\n                    tx_cmd = cmd + \"%s tx off' \u003e\u003e %s\" % (dev, temp_intf_file)\n                    local(rx_cmd, warn_only=True)\n                    local(tx_cmd, warn_only=True)\n            if vlan:\n                cmd = \"sudo sed -i '/auto %s/ a\\iface %s inet manual\\\\n    \" %\\\n                       (dev, dev)\n                cmd += \"post-up ifconfig %s up\\\\n    \" % dev\n                cmd += \"pre-down ifconfig %s down\\' %s\" % (dev, temp_intf_file)\n                local(cmd)\n            else:\n                cmd = \"sudo sed -i '/auto %s/ a\\iface %s inet manual\\\\n    \" %\\\n                       (dev, dev)\n                cmd += \"pre-up ifconfig %s up\\\\n    \" % dev\n                cmd += \"post-down ifconfig %s down\\' %s\" %\\\n                       (dev, temp_intf_file)\n                local(cmd)\n        if esxi_vm and vmpg_mtu:\n            intf = self.get_secondary_device(self.dev)\n            mac_addr = self.get_if_mac(intf)\n            udev_net_file = '/etc/udev/rules.d/70-persistent-net.rules'\n            temp_udev_net_file = '%s/70-persistent-net.rules' %\\\n                                 (self._temp_dir_name)\n            local(\"sudo touch %s\" % temp_udev_net_file)\n            local(\"sudo cp %s %s\" % (udev_net_file, temp_udev_net_file))\n            cmd = \"sudo echo 'SUBSYSTEM==\\\"net\\\", ACTION==\\\"add\\\",\"\n            cmd += \" DRIVERS==\\\"?*\\\",\"\n            cmd += \" ATTR{address}==\\\"%s\\\", ATTR{dev_id}==\\\"0x0\\\", \" % mac_addr\n            cmd += \"ATTR{type}==\\\"1\\\", KERNEL==\\\"eth*\\\", NAME=\\\"%s\\\"' \u003e\u003e %s\" %\\\n                   (intf, temp_udev_net_file)\n            local(cmd)\n            local(\"sudo mv -f %s %s\" % (temp_udev_net_file, udev_net_file))\n            local(\"sudo sed -i '/auto %s/,/down/d' %s\" % (intf,\n                                                          temp_intf_file))\n            local(\"sudo echo '\\nauto %s' \u003e\u003e %s\" % (intf, temp_intf_file))\n            local(\"sudo echo 'iface %s inet manual' \u003e\u003e %s\" % (intf,\n                                                              temp_intf_file))\n            local(\"sudo echo '    pre-up ifconfig %s up mtu %s' \u003e\u003e %s\" %\n                  (intf, vmpg_mtu, temp_intf_file))\n            local(\"sudo echo '    post-down ifconfig %s down' \u003e\u003e %s\" %\n                  (intf, temp_intf_file))\n            local(\"sudo echo '    pre-up ethtool --offload %s lro off' \u003e\u003e %s\" %\n                  (intf, temp_intf_file))\n\n        # populte vhost0 as static\n        local(\"sudo echo '' \u003e\u003e %s\" % (temp_intf_file))\n        local(\"sudo echo 'auto vhost0' \u003e\u003e %s\" % (temp_intf_file))\n        local(\"sudo echo 'iface vhost0 inet static' \u003e\u003e %s\" % (temp_intf_file))\n        local(\"sudo echo '    pre-up %s/if-vhost0' \u003e\u003e %s\" %\n              (self.contrail_bin_dir, temp_intf_file))\n        local(\"sudo echo '    netmask %s' \u003e\u003e %s\" % (netmask, temp_intf_file))\n        local(\"sudo echo '    network_name application' \u003e\u003e %s\" %\n              temp_intf_file)\n        if esxi_vm and datapg_mtu:\n            local(\"sudo echo '    mtu %s' \u003e\u003e %s\" % (datapg_mtu,\n                                                    temp_intf_file))\n        if vhost_ip:\n            local(\"sudo echo '    address %s' \u003e\u003e %s\" % (vhost_ip,\n                                                        temp_intf_file))\n        if (not self._args.non_mgmt_ip) and gateway_ip:\n            local(\"sudo echo '    gateway %s' \u003e\u003e %s\" % (gateway_ip,\n                                                        temp_intf_file))\n\n        domain = self.get_domain_search_list()\n        if domain:\n            local(\"sudo echo '    dns-search %s' \u003e\u003e %s\" % (domain,\n                                                           temp_intf_file))\n        dns_list = self.get_dns_servers(dev)\n        if dns_list:\n            local(\"sudo echo -n '    dns-nameservers' \u003e\u003e %s\" %\n                  temp_intf_file)\n            for dns in dns_list:\n                local(\"sudo echo -n ' %s' \u003e\u003e %s\" % (dns, temp_intf_file))\n            local(\"sudo echo '' \u003e\u003e %s\" % temp_intf_file)\n        local(\"sudo echo '    post-up ip link set vhost0 address %s' \u003e\u003e %s\" %\n              (mac, temp_intf_file))\n\n        # move it to right place\n        local(\"sudo mv -f %s %s\" % (temp_intf_file, dev_cfgfile))\n"}
{"repo_name":"mcrowson/django","ref":"refs/heads/master","path":"tests/choices/tests.py","content":"from django.test import TestCase\n\nfrom .models import Person\n\n\nclass ChoicesTests(TestCase):\n    def test_display(self):\n        a = Person.objects.create(name='Adrian', gender='M')\n        s = Person.objects.create(name='Sara', gender='F')\n        self.assertEqual(a.gender, 'M')\n        self.assertEqual(s.gender, 'F')\n\n        self.assertEqual(a.get_gender_display(), 'Male')\n        self.assertEqual(s.get_gender_display(), 'Female')\n\n        # If the value for the field doesn't correspond to a valid choice,\n        # the value itself is provided as a display value.\n        a.gender = ''\n        self.assertEqual(a.get_gender_display(), '')\n\n        a.gender = 'U'\n        self.assertEqual(a.get_gender_display(), 'U')\n"}
{"repo_name":"RPi-Distro/python-gpiozero","ref":"refs/heads/master","path":"gpiozerocli/pinout.py","content":"# GPIO Zero: a library for controlling the Raspberry Pi's GPIO pins\n# Copyright (c) 2017-2019 Dave Jones \u003cdave@waveform.org.uk\u003e\n# Copyright (c) 2017 Ben Nuttall \u003cben@bennuttall.com\u003e\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice,\n#   this list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its contributors\n#   may be used to endorse or promote products derived from this software\n#   without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"\nA utility for querying Raspberry Pi GPIO pin-out information.\n\"\"\"\n\nfrom __future__ import (\n    unicode_literals,\n    absolute_import,\n    print_function,\n    division,\n)\n\nimport argparse\nimport sys\nimport textwrap\nimport warnings\nimport webbrowser\n\nfrom gpiozero import pi_info\n\n\nclass PinoutTool(object):\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(\n            description=__doc__\n        )\n        self.parser.add_argument(\n            '-r', '--revision',\n            dest='revision',\n            default='',\n            help='RPi revision. Default is to autodetect revision of current device'\n        )\n        self.parser.add_argument(\n            '-c', '--color',\n            action=\"store_true\",\n            default=None,\n            help='Force colored output (by default, the output will include ANSI'\n            'color codes if run in a color-capable terminal). See also --monochrome'\n        )\n        self.parser.add_argument(\n            '-m', '--monochrome',\n            dest='color',\n            action='store_false',\n            help='Force monochrome output. See also --color'\n        )\n        self.parser.add_argument(\n            '-x', '--xyz',\n            dest='xyz',\n            action='store_true',\n            help='Open pinout.xyz in the default web browser'\n        )\n\n    def __call__(self, args=None):\n        if args is None:\n            args = sys.argv[1:]\n        try:\n            return self.main(self.parser.parse_args(args)) or 0\n        except argparse.ArgumentError as e:\n            # argparse errors are already nicely formatted, print to stderr and\n            # exit with code 2\n            raise e\n        except Exception as e:\n            raise\n            # Output anything else nicely formatted on stderr and exit code 1\n            self.parser.exit(1, '{prog}: error: {message}\\n'.format(\n                prog=self.parser.prog, message=e))\n\n    def main(self, args):\n        warnings.simplefilter('ignore')\n        if args.xyz:\n            webbrowser.open('https://pinout.xyz')\n        else:\n            if args.revision == '':\n                try:\n                    pi_info().pprint(color=args.color)\n                except ImportError:\n                    formatter = self.parser._get_formatter()\n                    formatter.add_text(\n                        \"Unable to initialize GPIO Zero. This usually means \"\n                        \"that you are not running %(prog)s on a Raspberry Pi. \"\n                        \"If you still wish to run %(prog)s, set the \"\n                        \"GPIOZERO_PIN_FACTORY environment variable to 'mock' \"\n                        \"and retry, or refer to the Remote GPIO section of \"\n                        \"the manual* to configure your environment to \"\n                        \"remotely access your Pi.\"\n                    )\n                    formatter.add_text(\n                        \"* https://gpiozero.readthedocs.io/en/stable/\"\n                        \"remote_gpio.html\"\n                    )\n                    sys.stderr.write(formatter.format_help())\n                except IOError:\n                    raise IOError('This device is not a Raspberry Pi')\n            else:\n                pi_info(args.revision).pprint(color=args.color)\n            formatter = self.parser._get_formatter()\n            formatter.add_text(\n                \"For further information, please refer to \"\n                \"https://pinout.xyz/\"\n            )\n            sys.stdout.write('\\n')\n            sys.stdout.write(formatter.format_help())\n\n\nmain = PinoutTool()\n"}
{"repo_name":"lordmos/blink","ref":"refs/heads/master","path":"Tools/TestResultServer/model/jsonresults_unittest.py","content":"# Copyright (C) 2010 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\ntry:\n    import jsonresults\n    from jsonresults import *\nexcept ImportError:\n    print \"ERROR: Add the TestResultServer, google_appengine and yaml/lib directories to your PYTHONPATH\"\n    raise\n\nimport json\nimport logging\nimport unittest\n\nFULL_RESULT_EXAMPLE = \"\"\"ADD_RESULTS({\n    \"seconds_since_epoch\": 1368146629,\n    \"tests\": {\n        \"media\": {\n            \"encrypted-media\": {\n                \"encrypted-media-v2-events.html\": {\n                    \"bugs\": [\"crbug.com/1234\"],\n                    \"expected\": \"TIMEOUT\",\n                    \"actual\": \"TIMEOUT\",\n                    \"time\": 6.0\n                },\n                \"encrypted-media-v2-syntax.html\": {\n                    \"expected\": \"TIMEOUT\",\n                    \"actual\": \"TIMEOUT\"\n                }\n            },\n            \"progress-events-generated-correctly.html\": {\n                \"expected\": \"PASS FAIL IMAGE TIMEOUT CRASH MISSING\",\n                \"actual\": \"TIMEOUT\",\n                \"time\": 6.0\n            },\n            \"W3C\": {\n                \"audio\": {\n                    \"src\": {\n                        \"src_removal_does_not_trigger_loadstart.html\": {\n                            \"expected\": \"PASS\",\n                            \"actual\": \"PASS\",\n                            \"time\": 3.5\n                        }\n                    }\n                },\n                \"video\": {\n                    \"src\": {\n                        \"src_removal_does_not_trigger_loadstart.html\": {\n                            \"expected\": \"PASS\",\n                            \"actual\": \"PASS\",\n                            \"time\": 1.1\n                        },\n                        \"notrun.html\": {\n                            \"expected\": \"NOTRUN\",\n                            \"actual\": \"SKIP\",\n                            \"time\": 1.1\n                        }\n                    }\n                }\n            },\n            \"unexpected-skip.html\": {\n                \"expected\": \"PASS\",\n                \"actual\": \"SKIP\"\n            },\n            \"unexpected-fail.html\": {\n                \"expected\": \"PASS\",\n                \"actual\": \"FAIL\"\n            },\n            \"flaky-failed.html\": {\n                \"expected\": \"PASS FAIL\",\n                \"actual\": \"FAIL\"\n            },\n            \"media-document-audio-repaint.html\": {\n                \"expected\": \"IMAGE\",\n                \"actual\": \"IMAGE\",\n                \"time\": 0.1\n            }\n        }\n    },\n    \"skipped\": 2,\n    \"num_regressions\": 0,\n    \"build_number\": \"3\",\n    \"interrupted\": false,\n    \"layout_tests_dir\": \"\\/tmp\\/cr\\/src\\/third_party\\/WebKit\\/LayoutTests\",\n    \"version\": 3,\n    \"builder_name\": \"Webkit\",\n    \"num_passes\": 10,\n    \"pixel_tests_enabled\": true,\n    \"blink_revision\": \"1234\",\n    \"has_pretty_patch\": true,\n    \"fixable\": 25,\n    \"num_flaky\": 0,\n    \"num_failures_by_type\": {\n        \"CRASH\": 3,\n        \"MISSING\": 0,\n        \"TEXT\": 3,\n        \"IMAGE\": 1,\n        \"PASS\": 10,\n        \"SKIP\": 2,\n        \"TIMEOUT\": 16,\n        \"IMAGE+TEXT\": 0,\n        \"FAIL\": 2,\n        \"AUDIO\": 0\n    },\n    \"has_wdiff\": true,\n    \"chromium_revision\": \"5678\"\n});\"\"\"\n\nJSON_RESULTS_OLD_TEMPLATE = (\n    '{\"[BUILDER_NAME]\":{'\n    '\"allFixableCount\":[[TESTDATA_COUNT]],'\n    '\"blinkRevision\":[[TESTDATA_WEBKITREVISION]],'\n    '\"buildNumbers\":[[TESTDATA_BUILDNUMBERS]],'\n    '\"chromeRevision\":[[TESTDATA_CHROMEREVISION]],'\n    '\"failure_map\": %s,'\n    '\"fixableCount\":[[TESTDATA_COUNT]],'\n    '\"fixableCounts\":[[TESTDATA_COUNTS]],'\n    '\"secondsSinceEpoch\":[[TESTDATA_TIMES]],'\n    '\"tests\":{[TESTDATA_TESTS]}'\n    '},'\n    '\"version\":[VERSION]'\n    '}') % json.dumps(CHAR_TO_FAILURE)\n\nJSON_RESULTS_COUNTS = '{\"' + '\":[[TESTDATA_COUNT]],\"'.join([char for char in CHAR_TO_FAILURE.values()]) + '\":[[TESTDATA_COUNT]]}'\n\nJSON_RESULTS_TEMPLATE = (\n    '{\"[BUILDER_NAME]\":{'\n    '\"blinkRevision\":[[TESTDATA_WEBKITREVISION]],'\n    '\"buildNumbers\":[[TESTDATA_BUILDNUMBERS]],'\n    '\"chromeRevision\":[[TESTDATA_CHROMEREVISION]],'\n    '\"failure_map\": %s,'\n    '\"num_failures_by_type\":%s,'\n    '\"secondsSinceEpoch\":[[TESTDATA_TIMES]],'\n    '\"tests\":{[TESTDATA_TESTS]}'\n    '},'\n    '\"version\":[VERSION]'\n    '}') % (json.dumps(CHAR_TO_FAILURE), JSON_RESULTS_COUNTS)\n\nJSON_RESULTS_COUNTS_TEMPLATE = '{\"' + '\":[TESTDATA],\"'.join([char for char in CHAR_TO_FAILURE]) + '\":[TESTDATA]}'\n\nJSON_RESULTS_TEST_LIST_TEMPLATE = '{\"Webkit\":{\"tests\":{[TESTDATA_TESTS]}}}'\n\n\nclass MockFile(object):\n    def __init__(self, name='results.json', data=''):\n        self.master = 'MockMasterName'\n        self.builder = 'MockBuilderName'\n        self.test_type = 'MockTestType'\n        self.name = name\n        self.data = data\n\n    def save(self, data):\n        self.data = data\n        return True\n\n\nclass JsonResultsTest(unittest.TestCase):\n    def setUp(self):\n        self._builder = \"Webkit\"\n        self.old_log_level = logging.root.level\n        logging.root.setLevel(logging.ERROR)\n\n    def tearDown(self):\n        logging.root.setLevel(self.old_log_level)\n\n    # Use this to get better error messages than just string compare gives.\n    def assert_json_equal(self, a, b):\n        self.maxDiff = None\n        a = json.loads(a) if isinstance(a, str) else a\n        b = json.loads(b) if isinstance(b, str) else b\n        self.assertEqual(a, b)\n\n    def test_strip_prefix_suffix(self):\n        json = \"['contents']\"\n        self.assertEqual(JsonResults._strip_prefix_suffix(\"ADD_RESULTS(\" + json + \");\"), json)\n        self.assertEqual(JsonResults._strip_prefix_suffix(json), json)\n\n    def _make_test_json(self, test_data, json_string=JSON_RESULTS_TEMPLATE, builder_name=\"Webkit\"):\n        if not test_data:\n            return \"\"\n\n        builds = test_data[\"builds\"]\n        tests = test_data[\"tests\"]\n        if not builds or not tests:\n            return \"\"\n\n        counts = []\n        build_numbers = []\n        webkit_revision = []\n        chrome_revision = []\n        times = []\n        for build in builds:\n            counts.append(JSON_RESULTS_COUNTS_TEMPLATE.replace(\"[TESTDATA]\", build))\n            build_numbers.append(\"1000%s\" % build)\n            webkit_revision.append(\"2000%s\" % build)\n            chrome_revision.append(\"3000%s\" % build)\n            times.append(\"100000%s000\" % build)\n\n        json_string = json_string.replace(\"[BUILDER_NAME]\", builder_name)\n        json_string = json_string.replace(\"[TESTDATA_COUNTS]\", \",\".join(counts))\n        json_string = json_string.replace(\"[TESTDATA_COUNT]\", \",\".join(builds))\n        json_string = json_string.replace(\"[TESTDATA_BUILDNUMBERS]\", \",\".join(build_numbers))\n        json_string = json_string.replace(\"[TESTDATA_WEBKITREVISION]\", \",\".join(webkit_revision))\n        json_string = json_string.replace(\"[TESTDATA_CHROMEREVISION]\", \",\".join(chrome_revision))\n        json_string = json_string.replace(\"[TESTDATA_TIMES]\", \",\".join(times))\n\n        version = str(test_data[\"version\"]) if \"version\" in test_data else \"4\"\n        json_string = json_string.replace(\"[VERSION]\", version)\n        json_string = json_string.replace(\"{[TESTDATA_TESTS]}\", json.dumps(tests, separators=(',', ':'), sort_keys=True))\n        return json_string\n\n    def _test_merge(self, aggregated_data, incremental_data, expected_data, max_builds=jsonresults.JSON_RESULTS_MAX_BUILDS):\n        aggregated_results = self._make_test_json(aggregated_data, builder_name=self._builder)\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, self._make_test_json(incremental_data, builder_name=self._builder), is_full_results_format=False)\n        merged_results, status_code = JsonResults.merge(self._builder, aggregated_results, incremental_json, num_runs=max_builds, sort_keys=True)\n\n        if expected_data:\n            expected_results = self._make_test_json(expected_data, builder_name=self._builder)\n            self.assert_json_equal(merged_results, expected_results)\n            self.assertEqual(status_code, 200)\n        else:\n            self.assertTrue(status_code != 200)\n\n    def _test_get_test_list(self, input_data, expected_data):\n        input_results = self._make_test_json(input_data)\n        expected_results = JSON_RESULTS_TEST_LIST_TEMPLATE.replace(\"{[TESTDATA_TESTS]}\", json.dumps(expected_data, separators=(',', ':')))\n        actual_results = JsonResults.get_test_list(self._builder, input_results)\n        self.assert_json_equal(actual_results, expected_results)\n\n    def test_update_files_empty_aggregate_data(self):\n        small_file = MockFile(name='results-small.json')\n        large_file = MockFile(name='results.json')\n\n        incremental_data = {\n            \"builds\": [\"2\", \"1\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[200, TEXT]],\n                    \"times\": [[200, 0]],\n                }\n            }\n        }\n        incremental_string = self._make_test_json(incremental_data, builder_name=small_file.builder)\n\n        self.assertTrue(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False))\n        self.assert_json_equal(small_file.data, incremental_string)\n        self.assert_json_equal(large_file.data, incremental_string)\n\n    def test_update_files_null_incremental_data(self):\n        small_file = MockFile(name='results-small.json')\n        large_file = MockFile(name='results.json')\n\n        aggregated_data = {\n            \"builds\": [\"2\", \"1\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[200, TEXT]],\n                    \"times\": [[200, 0]],\n                }\n            }\n        }\n        aggregated_string = self._make_test_json(aggregated_data, builder_name=small_file.builder)\n\n        small_file.data = large_file.data = aggregated_string\n\n        incremental_string = \"\"\n\n        self.assertEqual(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False),\n            ('No incremental JSON data to merge.', 403))\n        self.assert_json_equal(small_file.data, aggregated_string)\n        self.assert_json_equal(large_file.data, aggregated_string)\n\n    def test_update_files_empty_incremental_data(self):\n        small_file = MockFile(name='results-small.json')\n        large_file = MockFile(name='results.json')\n\n        aggregated_data = {\n            \"builds\": [\"2\", \"1\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[200, TEXT]],\n                    \"times\": [[200, 0]],\n                }\n            }\n        }\n        aggregated_string = self._make_test_json(aggregated_data, builder_name=small_file.builder)\n\n        small_file.data = large_file.data = aggregated_string\n\n        incremental_data = {\n            \"builds\": [],\n            \"tests\": {}\n        }\n        incremental_string = self._make_test_json(incremental_data, builder_name=small_file.builder)\n\n        self.assertEqual(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False),\n            ('No incremental JSON data to merge.', 403))\n        self.assert_json_equal(small_file.data, aggregated_string)\n        self.assert_json_equal(large_file.data, aggregated_string)\n\n    def test_merge_with_empty_aggregated_results(self):\n        incremental_data = {\n            \"builds\": [\"2\", \"1\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[200, TEXT]],\n                    \"times\": [[200, 0]],\n                }\n            }\n        }\n        incremental_results, _ = JsonResults._get_incremental_json(self._builder, self._make_test_json(incremental_data), is_full_results_format=False)\n        aggregated_results = \"\"\n        merged_results, _ = JsonResults.merge(self._builder, aggregated_results, incremental_results, num_runs=jsonresults.JSON_RESULTS_MAX_BUILDS, sort_keys=True)\n        self.assert_json_equal(merged_results, incremental_results)\n\n    def test_failures_by_type_added(self):\n        aggregated_results = self._make_test_json({\n            \"builds\": [\"2\", \"1\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[100, TEXT], [100, FAIL]],\n                    \"times\": [[200, 0]],\n                }\n            }\n        }, json_string=JSON_RESULTS_OLD_TEMPLATE)\n        incremental_results = self._make_test_json({\n            \"builds\": [\"3\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[1, TEXT]],\n                    \"times\": [[1, 0]],\n                }\n            }\n        }, json_string=JSON_RESULTS_OLD_TEMPLATE)\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, incremental_results, is_full_results_format=False)\n        merged_results, _ = JsonResults.merge(self._builder, aggregated_results, incremental_json, num_runs=201, sort_keys=True)\n        self.assert_json_equal(merged_results, self._make_test_json({\n            \"builds\": [\"3\", \"2\", \"1\"],\n            \"tests\": {\n                \"001.html\": {\n                    \"results\": [[101, TEXT], [100, FAIL]],\n                    \"times\": [[201, 0]],\n                }\n            }\n        }))\n\n    def test_merge_full_results_format(self):\n        expected_incremental_results = {\n            \"Webkit\": {\n                \"blinkRevision\": [\"1234\"],\n                \"buildNumbers\": [\"3\"],\n                \"chromeRevision\": [\"5678\"],\n                \"failure_map\": CHAR_TO_FAILURE,\n                \"num_failures_by_type\": {\"AUDIO\": [0], \"CRASH\": [3], \"FAIL\": [2], \"IMAGE\": [1], \"IMAGE+TEXT\": [0], \"MISSING\": [0], \"PASS\": [10], \"SKIP\": [2], \"TEXT\": [3], \"TIMEOUT\": [16]},\n                \"secondsSinceEpoch\": [1368146629],\n                \"tests\": {\n                    \"media\": {\n                        \"W3C\": {\n                            \"audio\": {\n                                \"src\": {\n                                    \"src_removal_does_not_trigger_loadstart.html\": {\n                                        \"results\": [[1, PASS]],\n                                        \"times\": [[1, 4]],\n                                    }\n                                }\n                            }\n                        },\n                        \"encrypted-media\": {\n                            \"encrypted-media-v2-events.html\": {\n                                \"bugs\": [\"crbug.com/1234\"],\n                                \"expected\": \"TIMEOUT\",\n                                \"results\": [[1, TIMEOUT]],\n                                \"times\": [[1, 6]],\n                            },\n                            \"encrypted-media-v2-syntax.html\": {\n                                \"expected\": \"TIMEOUT\",\n                                \"results\": [[1, TIMEOUT]],\n                                \"times\": [[1, 0]],\n                            }\n                        },\n                        \"media-document-audio-repaint.html\": {\n                            \"expected\": \"IMAGE\",\n                            \"results\": [[1, IMAGE]],\n                            \"times\": [[1, 0]],\n                        },\n                        \"progress-events-generated-correctly.html\": {\n                            \"expected\": \"PASS FAIL IMAGE TIMEOUT CRASH MISSING\",\n                            \"results\": [[1, TIMEOUT]],\n                            \"times\": [[1, 6]],\n                        },\n                        \"flaky-failed.html\": {\n                            \"expected\": \"PASS FAIL\",\n                            \"results\": [[1, FAIL]],\n                            \"times\": [[1, 0]],\n                        },\n                        \"unexpected-fail.html\": {\n                            \"results\": [[1, FAIL]],\n                            \"times\": [[1, 0]],\n                        },\n                    }\n                }\n            },\n            \"version\": 4\n        }\n\n        aggregated_results = \"\"\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, FULL_RESULT_EXAMPLE, is_full_results_format=True)\n        merged_results, _ = JsonResults.merge(\"Webkit\", aggregated_results, incremental_json, num_runs=jsonresults.JSON_RESULTS_MAX_BUILDS, sort_keys=True)\n        self.assert_json_equal(merged_results, expected_incremental_results)\n\n    def test_merge_empty_aggregated_results(self):\n        # No existing aggregated results.\n        # Merged results == new incremental results.\n        self._test_merge(\n            # Aggregated results\n            None,\n            # Incremental results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]}}},\n            # Expected result\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]}}})\n\n    def test_merge_duplicate_build_number(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[100, TEXT]],\n                           \"times\": [[100, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"2\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]}}},\n            # Expected results\n            None)\n\n    def test_merge_incremental_single_test_single_run_same_result(self):\n        # Incremental results has the latest build and same test results for\n        # that run.\n        # Insert the incremental results at the first place and sum number\n        # of runs for TEXT (200 + 1) to get merged results.\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[201, TEXT]],\n                           \"times\": [[201, 0]]}}})\n\n    def test_merge_single_test_single_run_different_result(self):\n        # Incremental results has the latest build but different test results\n        # for that run.\n        # Insert the incremental results at the first place.\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, IMAGE]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, IMAGE], [200, TEXT]],\n                           \"times\": [[1, 1], [200, 0]]}}})\n\n    def test_merge_single_test_single_run_result_changed(self):\n        # Incremental results has the latest build but results which differ from\n        # the latest result (but are the same as an older result).\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT], [10, IMAGE]],\n                           \"times\": [[200, 0], [10, 1]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, IMAGE]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, IMAGE], [200, TEXT], [10, IMAGE]],\n                           \"times\": [[1, 1], [200, 0], [10, 1]]}}})\n\n    def test_merge_multiple_tests_single_run(self):\n        # All tests have incremental updates.\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]},\n                       \"002.html\": {\n                           \"results\": [[100, IMAGE]],\n                           \"times\": [[100, 1]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]},\n                       \"002.html\": {\n                           \"results\": [[1, IMAGE]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[201, TEXT]],\n                           \"times\": [[201, 0]]},\n                       \"002.html\": {\n                           \"results\": [[101, IMAGE]],\n                           \"times\": [[101, 1]]}}})\n\n    def test_merge_multiple_tests_single_run_one_no_result(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]},\n                       \"002.html\": {\n                           \"results\": [[100, IMAGE]],\n                           \"times\": [[100, 1]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"002.html\": {\n                           \"results\": [[1, IMAGE]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, NO_DATA], [200, TEXT]],\n                           \"times\": [[201, 0]]},\n                       \"002.html\": {\n                           \"results\": [[101, IMAGE]],\n                           \"times\": [[101, 1]]}}})\n\n    def test_merge_single_test_multiple_runs(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"4\", \"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[2, IMAGE], [1, FAIL]],\n                           \"times\": [[3, 2]]}}},\n            # Expected results\n            {\"builds\": [\"4\", \"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, FAIL], [2, IMAGE], [200, TEXT]],\n                           \"times\": [[3, 2], [200, 0]]}}})\n\n    def test_merge_multiple_tests_multiple_runs(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]},\n                       \"002.html\": {\n                           \"results\": [[10, IMAGE_PLUS_TEXT]],\n                           \"times\": [[10, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"4\", \"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[2, IMAGE]],\n                           \"times\": [[2, 2]]},\n                       \"002.html\": {\n                           \"results\": [[1, CRASH]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"4\", \"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[2, IMAGE], [200, TEXT]],\n                           \"times\": [[2, 2], [200, 0]]},\n                       \"002.html\": {\n                           \"results\": [[1, CRASH], [10, IMAGE_PLUS_TEXT]],\n                           \"times\": [[1, 1], [10, 0]]}}})\n\n    def test_merge_incremental_result_older_build(self):\n        # Test the build in incremental results is older than the most recent\n        # build in aggregated results.\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"3\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[5, TEXT]],\n                           \"times\": [[5, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"2\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]}}},\n            # Expected no merge happens.\n            {\"builds\": [\"2\", \"3\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[6, TEXT]],\n                           \"times\": [[6, 0]]}}})\n\n    def test_merge_incremental_result_same_build(self):\n        # Test the build in incremental results is same as the build in\n        # aggregated results.\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[5, TEXT]],\n                           \"times\": [[5, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"3\", \"2\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[2, TEXT]],\n                           \"times\": [[2, 0]]}}},\n            # Expected no merge happens.\n            {\"builds\": [\"3\", \"2\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[7, TEXT]],\n                           \"times\": [[7, 0]]}}})\n\n    def test_merge_remove_new_test(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[199, TEXT]],\n                           \"times\": [[199, 0]]},\n                       }},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]},\n                       \"002.html\": {\n                           \"results\": [[1, PASS]],\n                           \"times\": [[1, 0]]},\n                       \"notrun.html\": {\n                           \"results\": [[1, NOTRUN]],\n                           \"times\": [[1, 0]]},\n                       \"003.html\": {\n                           \"results\": [[1, NO_DATA]],\n                           \"times\": [[1, 0]]},\n                        }},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, TEXT]],\n                           \"times\": [[200, 0]]},\n                       }},\n            max_builds=200)\n\n    def test_merge_remove_test(self):\n        self._test_merge(\n            # Aggregated results\n            {\n                \"builds\": [\"2\", \"1\"],\n                \"tests\": {\n                    \"directory\": {\n                        \"directory\": {\n                            \"001.html\": {\n                                \"results\": [[200, PASS]],\n                                \"times\": [[200, 0]]\n                            }\n                        }\n                    },\n                    \"002.html\": {\n                        \"results\": [[10, TEXT]],\n                        \"times\": [[10, 0]]\n                    },\n                    \"003.html\": {\n                        \"results\": [[190, PASS], [9, NO_DATA], [1, TEXT]],\n                        \"times\": [[200, 0]]\n                    },\n                }\n            },\n            # Incremental results\n            {\n                \"builds\": [\"3\"],\n                \"tests\": {\n                    \"directory\": {\n                        \"directory\": {\n                            \"001.html\": {\n                                \"results\": [[1, PASS]],\n                                \"times\": [[1, 0]]\n                            }\n                        }\n                    },\n                    \"002.html\": {\n                        \"results\": [[1, PASS]],\n                        \"times\": [[1, 0]]\n                    },\n                    \"003.html\": {\n                        \"results\": [[1, PASS]],\n                        \"times\": [[1, 0]]\n                    },\n                }\n            },\n            # Expected results\n            {\n                \"builds\": [\"3\", \"2\", \"1\"],\n                \"tests\": {\n                    \"002.html\": {\n                        \"results\": [[1, PASS], [10, TEXT]],\n                        \"times\": [[11, 0]]\n                    }\n                }\n            },\n            max_builds=200)\n\n    def test_merge_updates_expected(self):\n        self._test_merge(\n            # Aggregated results\n            {\n                \"builds\": [\"2\", \"1\"],\n                \"tests\": {\n                    \"directory\": {\n                        \"directory\": {\n                            \"001.html\": {\n                                \"expected\": \"FAIL\",\n                                \"results\": [[200, PASS]],\n                                \"times\": [[200, 0]]\n                            }\n                        }\n                    },\n                    \"002.html\": {\n                        \"bugs\": [\"crbug.com/1234\"],\n                        \"expected\": \"FAIL\",\n                        \"results\": [[10, TEXT]],\n                        \"times\": [[10, 0]]\n                    },\n                    \"003.html\": {\n                        \"expected\": \"FAIL\",\n                        \"results\": [[190, PASS], [9, NO_DATA], [1, TEXT]],\n                        \"times\": [[200, 0]]\n                    },\n                    \"004.html\": {\n                        \"results\": [[199, PASS], [1, TEXT]],\n                        \"times\": [[200, 0]]\n                    },\n                }\n            },\n            # Incremental results\n            {\n                \"builds\": [\"3\"],\n                \"tests\": {\n                    \"002.html\": {\n                        \"expected\": \"PASS\",\n                        \"results\": [[1, PASS]],\n                        \"times\": [[1, 0]]\n                    },\n                    \"003.html\": {\n                        \"expected\": \"TIMEOUT\",\n                        \"results\": [[1, PASS]],\n                        \"times\": [[1, 0]]\n                    },\n                    \"004.html\": {\n                        \"bugs\": [\"crbug.com/1234\"],\n                        \"results\": [[1, PASS]],\n                        \"times\": [[1, 0]]\n                    },\n                }\n            },\n            # Expected results\n            {\n                \"builds\": [\"3\", \"2\", \"1\"],\n                \"tests\": {\n                    \"002.html\": {\n                        \"results\": [[1, PASS], [10, TEXT]],\n                        \"times\": [[11, 0]]\n                    },\n                    \"003.html\": {\n                        \"expected\": \"TIMEOUT\",\n                        \"results\": [[191, PASS], [9, NO_DATA]],\n                        \"times\": [[200, 0]]\n                    },\n                    \"004.html\": {\n                        \"bugs\": [\"crbug.com/1234\"],\n                        \"results\": [[200, PASS]],\n                        \"times\": [[200, 0]]\n                    },\n                }\n            },\n            max_builds=200)\n\n\n    def test_merge_keep_test_with_all_pass_but_slow_time(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, PASS]],\n                           \"times\": [[200, jsonresults.JSON_RESULTS_MIN_TIME]]},\n                       \"002.html\": {\n                           \"results\": [[10, TEXT]],\n                           \"times\": [[10, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, PASS]],\n                           \"times\": [[1, 1]]},\n                       \"002.html\": {\n                           \"results\": [[1, PASS]],\n                           \"times\": [[1, 0]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[201, PASS]],\n                           \"times\": [[1, 1], [200, jsonresults.JSON_RESULTS_MIN_TIME]]},\n                       \"002.html\": {\n                           \"results\": [[1, PASS], [10, TEXT]],\n                           \"times\": [[11, 0]]}}})\n\n    def test_merge_pruning_slow_tests_for_debug_builders(self):\n        self._builder = \"MockBuilder(dbg)\"\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[200, PASS]],\n                           \"times\": [[200, 3 * jsonresults.JSON_RESULTS_MIN_TIME]]},\n                       \"002.html\": {\n                           \"results\": [[10, TEXT]],\n                           \"times\": [[10, 0]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, PASS]],\n                           \"times\": [[1, 1]]},\n                       \"002.html\": {\n                           \"results\": [[1, PASS]],\n                           \"times\": [[1, 0]]},\n                       \"003.html\": {\n                           \"results\": [[1, PASS]],\n                           \"times\": [[1, jsonresults.JSON_RESULTS_MIN_TIME]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[201, PASS]],\n                           \"times\": [[1, 1], [200, 3 * jsonresults.JSON_RESULTS_MIN_TIME]]},\n                       \"002.html\": {\n                           \"results\": [[1, PASS], [10, TEXT]],\n                           \"times\": [[11, 0]]}}})\n\n    def test_merge_prune_extra_results(self):\n        # Remove items from test results and times that exceed the max number\n        # of builds to track.\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[max_builds, TEXT], [1, IMAGE]],\n                           \"times\": [[max_builds, 0], [1, 1]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TIMEOUT]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TIMEOUT], [max_builds, TEXT]],\n                           \"times\": [[1, 1], [max_builds, 0]]}}})\n\n    def test_merge_prune_extra_results_small(self):\n        # Remove items from test results and times that exceed the max number\n        # of builds to track, using smaller threshold.\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS_SMALL\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[max_builds, TEXT], [1, IMAGE]],\n                           \"times\": [[max_builds, 0], [1, 1]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TIMEOUT]],\n                           \"times\": [[1, 1]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TIMEOUT], [max_builds, TEXT]],\n                           \"times\": [[1, 1], [max_builds, 0]]}}},\n            int(max_builds))\n\n    def test_merge_prune_extra_results_with_new_result_of_same_type(self):\n        # Test that merging in a new result of the same type as the last result\n        # causes old results to fall off.\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS_SMALL\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[max_builds, TEXT], [1, NO_DATA]],\n                           \"times\": [[max_builds, 0], [1, 1]]}}},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]}}},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"001.html\": {\n                           \"results\": [[max_builds, TEXT]],\n                           \"times\": [[max_builds, 0]]}}},\n            int(max_builds))\n\n    def test_merge_build_directory_hierarchy(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"bar\": {\"baz\": {\n                           \"003.html\": {\n                                \"results\": [[25, TEXT]],\n                                \"times\": [[25, 0]]}}},\n                       \"foo\": {\n                           \"001.html\": {\n                                \"results\": [[50, TEXT]],\n                                \"times\": [[50, 0]]},\n                           \"002.html\": {\n                                \"results\": [[100, IMAGE]],\n                                \"times\": [[100, 0]]}}},\n              \"version\": 4},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"baz\": {\n                           \"004.html\": {\n                               \"results\": [[1, IMAGE]],\n                               \"times\": [[1, 0]]}},\n                       \"foo\": {\n                           \"001.html\": {\n                               \"results\": [[1, TEXT]],\n                               \"times\": [[1, 0]]},\n                           \"002.html\": {\n                               \"results\": [[1, IMAGE]],\n                               \"times\": [[1, 0]]}}},\n             \"version\": 4},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"bar\": {\"baz\": {\n                           \"003.html\": {\n                               \"results\": [[1, NO_DATA], [25, TEXT]],\n                               \"times\": [[26, 0]]}}},\n                       \"baz\": {\n                           \"004.html\": {\n                               \"results\": [[1, IMAGE]],\n                               \"times\": [[1, 0]]}},\n                       \"foo\": {\n                           \"001.html\": {\n                               \"results\": [[51, TEXT]],\n                               \"times\": [[51, 0]]},\n                           \"002.html\": {\n                               \"results\": [[101, IMAGE]],\n                               \"times\": [[101, 0]]}}},\n             \"version\": 4})\n\n    # FIXME(aboxhall): Add some tests for xhtml/svg test results.\n\n    def test_get_test_name_list(self):\n        # Get test name list only. Don't include non-test-list data and\n        # of test result details.\n        # FIXME: This also tests a temporary bug in the data where directory-level\n        # results have a results and times values. Once that bug is fixed,\n        # remove this test-case and assert we don't ever hit it.\n        self._test_get_test_list(\n            # Input results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"foo\": {\n                           \"001.html\": {\n                               \"results\": [[200, PASS]],\n                               \"times\": [[200, 0]]},\n                           \"results\": [[1, NO_DATA]],\n                           \"times\": [[1, 0]]},\n                       \"002.html\": {\n                           \"results\": [[10, TEXT]],\n                           \"times\": [[10, 0]]}}},\n            # Expected results\n            {\"foo\": {\"001.html\": {}}, \"002.html\": {}})\n\n    def test_gtest(self):\n        self._test_merge(\n            # Aggregated results\n            {\"builds\": [\"2\", \"1\"],\n             \"tests\": {\"foo.bar\": {\n                           \"results\": [[50, TEXT]],\n                           \"times\": [[50, 0]]},\n                       \"foo.bar2\": {\n                           \"results\": [[100, IMAGE]],\n                           \"times\": [[100, 0]]},\n                       \"test.failed\": {\n                           \"results\": [[5, FAIL]],\n                           \"times\": [[5, 0]]},\n                       },\n             \"version\": 3},\n            # Incremental results\n            {\"builds\": [\"3\"],\n             \"tests\": {\"foo.bar2\": {\n                           \"results\": [[1, IMAGE]],\n                           \"times\": [[1, 0]]},\n                       \"foo.bar3\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]},\n                       \"test.failed\": {\n                           \"results\": [[5, FAIL]],\n                           \"times\": [[5, 0]]},\n                       },\n             \"version\": 4},\n            # Expected results\n            {\"builds\": [\"3\", \"2\", \"1\"],\n             \"tests\": {\"foo.bar\": {\n                           \"results\": [[1, NO_DATA], [50, TEXT]],\n                           \"times\": [[51, 0]]},\n                       \"foo.bar2\": {\n                           \"results\": [[101, IMAGE]],\n                           \"times\": [[101, 0]]},\n                       \"foo.bar3\": {\n                           \"results\": [[1, TEXT]],\n                           \"times\": [[1, 0]]},\n                       \"test.failed\": {\n                           \"results\": [[10, FAIL]],\n                           \"times\": [[10, 0]]},\n                       },\n             \"version\": 4})\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"repo_name":"fabian4/trove","ref":"refs/heads/master","path":"trove/guestagent/datastore/experimental/redis/manager.py","content":"# Copyright (c) 2013 Rackspace\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom oslo_log import log as logging\nfrom oslo_service import periodic_task\n\nfrom trove.common import cfg\nfrom trove.common import exception\nfrom trove.common.i18n import _\nfrom trove.common import instance as rd_instance\nfrom trove.common import utils\nfrom trove.guestagent import backup\nfrom trove.guestagent.common import operating_system\nfrom trove.guestagent.datastore.experimental.redis import service\nfrom trove.guestagent import dbaas\nfrom trove.guestagent.strategies.replication import get_replication_strategy\nfrom trove.guestagent import volume\n\n\nLOG = logging.getLogger(__name__)\nCONF = cfg.CONF\nMANAGER = CONF.datastore_manager or 'redis'\nREPLICATION_STRATEGY = CONF.get(MANAGER).replication_strategy\nREPLICATION_NAMESPACE = CONF.get(MANAGER).replication_namespace\nREPLICATION_STRATEGY_CLASS = get_replication_strategy(REPLICATION_STRATEGY,\n                                                      REPLICATION_NAMESPACE)\n\n\nclass Manager(periodic_task.PeriodicTasks):\n    \"\"\"\n    This is the Redis manager class. It is dynamically loaded\n    based off of the service_type of the trove instance\n    \"\"\"\n\n    def __init__(self):\n        super(Manager, self).__init__(CONF)\n        self._app = service.RedisApp()\n\n    @periodic_task.periodic_task\n    def update_status(self, context):\n        \"\"\"\n        Updates the redis trove instance. It is decorated with\n        perodic task so it is automatically called every 3 ticks.\n        \"\"\"\n        LOG.debug(\"Update status called.\")\n        self._app.status.update()\n\n    def rpc_ping(self, context):\n        LOG.debug(\"Responding to RPC ping.\")\n        return True\n\n    def change_passwords(self, context, users):\n        \"\"\"\n        Changes the redis instance password,\n        it is currently not not implemented.\n        \"\"\"\n        LOG.debug(\"Change passwords called.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='change_passwords', datastore=MANAGER)\n\n    def reset_configuration(self, context, configuration):\n        \"\"\"\n        Resets to the default configuration,\n        currently this does nothing.\n        \"\"\"\n        LOG.debug(\"Reset configuration called.\")\n        self._app.reset_configuration(configuration)\n\n    def _perform_restore(self, backup_info, context, restore_location, app):\n        \"\"\"Perform a restore on this instance.\"\"\"\n        LOG.info(_(\"Restoring database from backup %s.\") % backup_info['id'])\n        try:\n            backup.restore(context, backup_info, restore_location)\n        except Exception:\n            LOG.exception(_(\"Error performing restore from backup %s.\") %\n                          backup_info['id'])\n            app.status.set_status(rd_instance.ServiceStatuses.FAILED)\n            raise\n        LOG.info(_(\"Restored database successfully.\"))\n\n    def prepare(self, context, packages, databases, memory_mb, users,\n                device_path=None, mount_point=None, backup_info=None,\n                config_contents=None, root_password=None, overrides=None,\n                cluster_config=None, snapshot=None):\n        \"\"\"\n        This is called when the trove instance first comes online.\n        It is the first rpc message passed from the task manager.\n        prepare handles all the base configuration of the redis instance.\n        \"\"\"\n        try:\n            self._app.status.begin_install()\n            if device_path:\n                device = volume.VolumeDevice(device_path)\n                # unmount if device is already mounted\n                device.unmount_device(device_path)\n                device.format()\n                device.mount(mount_point)\n                operating_system.chown(mount_point, 'redis', 'redis',\n                                       as_root=True)\n                LOG.debug('Mounted the volume.')\n            self._app.install_if_needed(packages)\n            LOG.info(_('Writing redis configuration.'))\n            if cluster_config:\n                config_contents = (config_contents + \"\\n\"\n                                   + \"cluster-enabled yes\\n\"\n                                   + \"cluster-config-file cluster.conf\\n\")\n            self._app.configuration_manager.save_configuration(config_contents)\n            self._app.apply_initial_guestagent_configuration()\n            if backup_info:\n                persistence_dir = self._app.get_working_dir()\n                self._perform_restore(backup_info, context, persistence_dir,\n                                      self._app)\n            if snapshot:\n                self.attach_replica(context, snapshot, snapshot['config'])\n            self._app.restart()\n            if cluster_config:\n                self._app.status.set_status(\n                    rd_instance.ServiceStatuses.BUILD_PENDING)\n            else:\n                self._app.complete_install_or_restart()\n            LOG.info(_('Redis instance has been setup and configured.'))\n        except Exception:\n            LOG.exception(_(\"Error setting up Redis instance.\"))\n            self._app.status.set_status(rd_instance.ServiceStatuses.FAILED)\n            raise\n\n    def restart(self, context):\n        \"\"\"\n        Restart this redis instance.\n        This method is called when the guest agent\n        gets a restart message from the taskmanager.\n        \"\"\"\n        LOG.debug(\"Restart called.\")\n        self._app.restart()\n\n    def start_db_with_conf_changes(self, context, config_contents):\n        \"\"\"\n        Start this redis instance with new conf changes.\n        \"\"\"\n        LOG.debug(\"Start DB with conf changes called.\")\n        self._app.start_db_with_conf_changes(config_contents)\n\n    def stop_db(self, context, do_not_start_on_reboot=False):\n        \"\"\"\n        Stop this redis instance.\n        This method is called when the guest agent\n        gets a stop message from the taskmanager.\n        \"\"\"\n        LOG.debug(\"Stop DB called.\")\n        self._app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)\n\n    def get_filesystem_stats(self, context, fs_path):\n        \"\"\"Gets the filesystem stats for the path given.\"\"\"\n        LOG.debug(\"Get Filesystem Stats.\")\n        mount_point = CONF.get(\n            'mysql' if not MANAGER else MANAGER).mount_point\n        return dbaas.get_filesystem_volume_stats(mount_point)\n\n    def create_backup(self, context, backup_info):\n        \"\"\"Create a backup of the database.\"\"\"\n        LOG.debug(\"Creating backup.\")\n        backup.backup(context, backup_info)\n\n    def mount_volume(self, context, device_path=None, mount_point=None):\n        device = volume.VolumeDevice(device_path)\n        device.mount(mount_point, write_to_fstab=False)\n        LOG.debug(\"Mounted the device %s at the mount point %s.\" %\n                  (device_path, mount_point))\n\n    def unmount_volume(self, context, device_path=None, mount_point=None):\n        device = volume.VolumeDevice(device_path)\n        device.unmount(mount_point)\n        LOG.debug(\"Unmounted the device %s from the mount point %s.\" %\n                  (device_path, mount_point))\n\n    def resize_fs(self, context, device_path=None, mount_point=None):\n        device = volume.VolumeDevice(device_path)\n        device.resize_fs(mount_point)\n        LOG.debug(\"Resized the filesystem at %s.\" % mount_point)\n\n    def update_overrides(self, context, overrides, remove=False):\n        LOG.debug(\"Updating overrides.\")\n        if remove:\n            self._app.remove_overrides()\n        else:\n            self._app.update_overrides(context, overrides, remove)\n\n    def apply_overrides(self, context, overrides):\n        LOG.debug(\"Applying overrides.\")\n        self._app.apply_overrides(self._app.admin, overrides)\n\n    def update_attributes(self, context, username, hostname, user_attrs):\n        LOG.debug(\"Updating attributes.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='update_attributes', datastore=MANAGER)\n\n    def create_database(self, context, databases):\n        LOG.debug(\"Creating database.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='create_database', datastore=MANAGER)\n\n    def create_user(self, context, users):\n        LOG.debug(\"Creating user.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='create_user', datastore=MANAGER)\n\n    def delete_database(self, context, database):\n        LOG.debug(\"Deleting database.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='delete_database', datastore=MANAGER)\n\n    def delete_user(self, context, user):\n        LOG.debug(\"Deleting user.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='delete_user', datastore=MANAGER)\n\n    def get_user(self, context, username, hostname):\n        LOG.debug(\"Getting user.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='get_user', datastore=MANAGER)\n\n    def grant_access(self, context, username, hostname, databases):\n        LOG.debug(\"Granting access.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='grant_access', datastore=MANAGER)\n\n    def revoke_access(self, context, username, hostname, database):\n        LOG.debug(\"Revoking access.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='revoke_access', datastore=MANAGER)\n\n    def list_access(self, context, username, hostname):\n        LOG.debug(\"Listing access.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='list_access', datastore=MANAGER)\n\n    def list_databases(self, context, limit=None, marker=None,\n                       include_marker=False):\n        LOG.debug(\"Listing databases.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='list_databases', datastore=MANAGER)\n\n    def list_users(self, context, limit=None, marker=None,\n                   include_marker=False):\n        LOG.debug(\"Listing users.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='list_users', datastore=MANAGER)\n\n    def enable_root(self, context):\n        LOG.debug(\"Enabling root.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='enable_root', datastore=MANAGER)\n\n    def enable_root_with_password(self, context, root_password=None):\n        LOG.debug(\"Enabling root with password.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='enable_root_with_password', datastore=MANAGER)\n\n    def is_root_enabled(self, context):\n        LOG.debug(\"Checking if root is enabled.\")\n        raise exception.DatastoreOperationNotSupported(\n            operation='is_root_enabled', datastore=MANAGER)\n\n    def backup_required_for_replication(self, context):\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        return replication.backup_required_for_replication()\n\n    def get_replication_snapshot(self, context, snapshot_info,\n                                 replica_source_config=None):\n        LOG.debug(\"Getting replication snapshot.\")\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        replication.enable_as_master(self._app, replica_source_config)\n\n        snapshot_id, log_position = (\n            replication.snapshot_for_replication(context, self._app, None,\n                                                 snapshot_info))\n\n        mount_point = CONF.get(MANAGER).mount_point\n        volume_stats = dbaas.get_filesystem_volume_stats(mount_point)\n\n        replication_snapshot = {\n            'dataset': {\n                'datastore_manager': MANAGER,\n                'dataset_size': volume_stats.get('used', 0.0),\n                'volume_size': volume_stats.get('total', 0.0),\n                'snapshot_id': snapshot_id\n            },\n            'replication_strategy': REPLICATION_STRATEGY,\n            'master': replication.get_master_ref(self._app, snapshot_info),\n            'log_position': log_position\n        }\n\n        return replication_snapshot\n\n    def enable_as_master(self, context, replica_source_config):\n        LOG.debug(\"Calling enable_as_master.\")\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        replication.enable_as_master(self._app, replica_source_config)\n\n    def detach_replica(self, context, for_failover=False):\n        LOG.debug(\"Detaching replica.\")\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        replica_info = replication.detach_slave(self._app, for_failover)\n        return replica_info\n\n    def get_replica_context(self, context):\n        LOG.debug(\"Getting replica context.\")\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        replica_info = replication.get_replica_context(self._app)\n        return replica_info\n\n    def _validate_slave_for_replication(self, context, replica_info):\n        if (replica_info['replication_strategy'] != REPLICATION_STRATEGY):\n            raise exception.IncompatibleReplicationStrategy(\n                replica_info.update({\n                    'guest_strategy': REPLICATION_STRATEGY\n                }))\n\n    def attach_replica(self, context, replica_info, slave_config):\n        LOG.debug(\"Attaching replica.\")\n        try:\n            if 'replication_strategy' in replica_info:\n                self._validate_slave_for_replication(context, replica_info)\n            replication = REPLICATION_STRATEGY_CLASS(context)\n            replication.enable_as_slave(self._app, replica_info,\n                                        slave_config)\n        except Exception:\n            LOG.exception(\"Error enabling replication.\")\n            self._app.status.set_status(rd_instance.ServiceStatuses.FAILED)\n            raise\n\n    def make_read_only(self, context, read_only):\n        LOG.debug(\"Executing make_read_only(%s)\" % read_only)\n        self._app.make_read_only(read_only)\n\n    def _get_repl_info(self):\n        return self._app.admin.get_info('replication')\n\n    def _get_master_host(self):\n        slave_info = self._get_repl_info()\n        return slave_info and slave_info['master_host'] or None\n\n    def _get_repl_offset(self):\n        repl_info = self._get_repl_info()\n        LOG.debug(\"Got repl info: %s\" % repl_info)\n        offset_key = '%s_repl_offset' % repl_info['role']\n        offset = repl_info[offset_key]\n        LOG.debug(\"Found offset %s for key %s.\" % (offset, offset_key))\n        return int(offset)\n\n    def get_last_txn(self, context):\n        master_host = self._get_master_host()\n        repl_offset = self._get_repl_offset()\n        return master_host, repl_offset\n\n    def get_latest_txn_id(self, context):\n        LOG.info(_(\"Retrieving latest repl offset.\"))\n        return self._get_repl_offset()\n\n    def wait_for_txn(self, context, txn):\n        LOG.info(_(\"Waiting on repl offset '%s'.\") % txn)\n\n        def _wait_for_txn():\n            current_offset = self._get_repl_offset()\n            LOG.debug(\"Current offset: %s.\" % current_offset)\n            return current_offset \u003e= txn\n\n        try:\n            utils.poll_until(_wait_for_txn, time_out=120)\n        except exception.PollTimeOut:\n            raise RuntimeError(_(\"Timeout occurred waiting for Redis repl \"\n                                 \"offset to change to '%s'.\") % txn)\n\n    def cleanup_source_on_replica_detach(self, context, replica_info):\n        LOG.debug(\"Cleaning up the source on the detach of a replica.\")\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        replication.cleanup_source_on_replica_detach(self._app, replica_info)\n\n    def demote_replication_master(self, context):\n        LOG.debug(\"Demoting replica source.\")\n        replication = REPLICATION_STRATEGY_CLASS(context)\n        replication.demote_master(self._app)\n\n    def cluster_meet(self, context, ip, port):\n        LOG.debug(\"Executing cluster_meet to join node to cluster.\")\n        self._app.cluster_meet(ip, port)\n\n    def get_node_ip(self, context):\n        LOG.debug(\"Retrieving cluster node ip address.\")\n        return self._app.get_node_ip()\n\n    def get_node_id_for_removal(self, context):\n        LOG.debug(\"Validating removal of node from cluster.\")\n        return self._app.get_node_id_for_removal()\n\n    def remove_nodes(self, context, node_ids):\n        LOG.debug(\"Removing nodes from cluster.\")\n        self._app.remove_nodes(node_ids)\n\n    def cluster_addslots(self, context, first_slot, last_slot):\n        LOG.debug(\"Executing cluster_addslots to assign hash slots %s-%s.\",\n                  first_slot, last_slot)\n        self._app.cluster_addslots(first_slot, last_slot)\n\n    def cluster_complete(self, context):\n        LOG.debug(\"Cluster creation complete, starting status checks.\")\n        self._app.complete_install_or_restart()\n"}
{"repo_name":"Hernanarce/pelisalacarta","ref":"refs/heads/master","path":"python/version-mediaserver/platformcode/platformtools.py","content":"# -*- coding: utf-8 -*-\r\n# ------------------------------------------------------------\r\n# pelisalacarta 4\r\n# Copyright 2015 tvalacarta@gmail.com\r\n# http://blog.tvalacarta.info/plugin-xbmc/pelisalacarta/\r\n#\r\n# Distributed under the terms of GNU General Public License v3 (GPLv3)\r\n# http://www.gnu.org/licenses/gpl-3.0.html\r\n# ------------------------------------------------------------\r\n# This file is part of pelisalacarta 4.\r\n#\r\n# pelisalacarta 4 is free software: you can redistribute it and/or modify\r\n# it under the terms of the GNU General Public License as published by\r\n# the Free Software Foundation, either version 3 of the License, or\r\n# (at your option) any later version.\r\n#\r\n# pelisalacarta 4 is distributed in the hope that it will be useful,\r\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\r\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\r\n# GNU General Public License for more details.\r\n#\r\n# You should have received a copy of the GNU General Public License\r\n# along with pelisalacarta 4.  If not, see \u003chttp://www.gnu.org/licenses/\u003e.\r\n# ------------------------------------------------------------\r\n# platformtools\r\n# ------------------------------------------------------------\r\n# Herramientas responsables de adaptar los diferentes \r\n# cuadros de dialogo a una plataforma en concreto,\r\n# en este caso Mediserver.\r\n# version 1.3\r\n# ------------------------------------------------------------\r\nimport os\r\nimport sys\r\nfrom core import config\r\nfrom core import logger\r\nimport threading\r\ncontrollers = {}\r\n\r\n\r\ndef dialog_ok(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_ok(*args, **kwargs)\r\n    \r\ndef dialog_notification(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_notification(*args, **kwargs)\r\n\r\ndef dialog_yesno(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_yesno(*args, **kwargs)\r\n    \r\ndef dialog_select(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_select(*args, **kwargs)\r\n\r\ndef dialog_progress(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_progress(*args, **kwargs)\r\n \r\ndef dialog_progress_bg(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_progress_bg(*args, **kwargs)\r\n\r\ndef dialog_input(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_input(*args, **kwargs)\r\n    \r\ndef dialog_numeric(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].dialog_numeric(*args, **kwargs)\r\n\r\ndef itemlist_refresh(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].itemlist_refresh(*args, **kwargs)\r\n\r\ndef itemlist_update(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].itemlist_update(*args, **kwargs)\r\n\r\ndef render_items(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].render_items(*args,**kwargs)\r\n\r\ndef is_playing(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].is_playing(*args, **kwargs)\r\n\r\ndef play_video(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].play_video(*args, **kwargs)\r\n\r\ndef open_settings(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].open_settings(*args, **kwargs)\r\n\r\ndef show_channel_settings(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].show_channel_settings(*args, **kwargs)\r\n\r\ndef show_video_info(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].show_video_info(*args, **kwargs)\r\n\r\ndef show_recaptcha(*args, **kwargs):\r\n    id = threading.current_thread().name\r\n    return controllers[id].show_recaptcha(*args, **kwargs)"}
{"repo_name":"hobson/pyexiv2","ref":"refs/heads/master","path":"test/usercomment.py","content":"# -*- coding: utf-8 -*-\n\n# ******************************************************************************\n#\n# Copyright (C) 2010 Olivier Tilloy \u003colivier@tilloy.net\u003e\n#\n# This file is part of the pyexiv2 distribution.\n#\n# pyexiv2 is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# pyexiv2 is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with pyexiv2; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, 5th Floor, Boston, MA 02110-1301 USA.\n#\n# Author: Olivier Tilloy \u003colivier@tilloy.net\u003e\n#\n# ******************************************************************************\n\nfrom pyexiv2.metadata import ImageMetadata\n\nimport unittest\nimport testutils\nimport os\nimport tempfile\nfrom testutils import EMPTY_JPG_DATA\n\n\nclass TestUserCommentReadWrite(unittest.TestCase):\n\n    checksums = {\n        'usercomment-ascii.jpg': 'ad29ac65fb6f63c8361aaed6cb02f8c7',\n        'usercomment-unicode-ii.jpg': '13b7cc09129a8677f2cf18634f5abd3c',\n        'usercomment-unicode-mm.jpg': '7addfed7823c556ba489cd4ab2037200',\n        }\n\n    def _read_image(self, filename):\n        filepath = testutils.get_absolute_file_path(os.path.join('data', filename))\n        self.assert_(testutils.CheckFileSum(filepath, self.checksums[filename]))\n        m = ImageMetadata(filepath)\n        m.read()\n        return m\n\n    def _expected_raw_value(self, endianness, value):\n        from pyexiv2 import __exiv2_version__\n        if __exiv2_version__ \u003e= '0.20':\n            return value\n        else:\n            encodings = {'ii': 'utf-16le', 'mm': 'utf-16be'}\n            return value.decode('utf-8').encode(encodings[endianness])\n\n    def test_read_ascii(self):\n        m = self._read_image('usercomment-ascii.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        self.assertEqual(tag.raw_value, 'charset=\"Ascii\" deja vu')\n        self.assertEqual(tag.value, u'deja vu')\n\n    def test_read_unicode_little_endian(self):\n        m = self._read_image('usercomment-unicode-ii.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        self.assertEqual(tag.raw_value, 'charset=\"Unicode\" %s' % self._expected_raw_value('ii', 'dj vu'))\n        self.assertEqual(tag.value, u'dj vu')\n\n    def test_read_unicode_big_endian(self):\n        m = self._read_image('usercomment-unicode-mm.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        self.assertEqual(tag.raw_value, 'charset=\"Unicode\" %s' % self._expected_raw_value('mm', 'dj vu'))\n        self.assertEqual(tag.value, u'dj vu')\n\n    def test_write_ascii(self):\n        m = self._read_image('usercomment-ascii.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        tag.value = 'foo bar'\n        self.assertEqual(tag.raw_value, 'charset=\"Ascii\" foo bar')\n        self.assertEqual(tag.value, u'foo bar')\n\n    def test_write_unicode_over_ascii(self):\n        m = self._read_image('usercomment-ascii.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        tag.value = u'dj vu'\n        self.assertEqual(tag.raw_value, 'dj vu')\n        self.assertEqual(tag.value, u'dj vu')\n\n    def test_write_unicode_little_endian(self):\n        m = self._read_image('usercomment-unicode-ii.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        tag.value = u'DJ VU'\n        self.assertEqual(tag.raw_value, 'charset=\"Unicode\" %s' % self._expected_raw_value('ii', 'DJ VU'))\n        self.assertEqual(tag.value, u'DJ VU')\n\n    def test_write_unicode_big_endian(self):\n        m = self._read_image('usercomment-unicode-mm.jpg')\n        tag = m['Exif.Photo.UserComment']\n        self.assertEqual(tag.type, 'Comment')\n        tag.value = u'DJ VU'\n        self.assertEqual(tag.raw_value, 'charset=\"Unicode\" %s' % self._expected_raw_value('mm', 'DJ VU'))\n        self.assertEqual(tag.value, u'DJ VU')\n\n\nclass TestUserCommentAdd(unittest.TestCase):\n\n    def setUp(self):\n        # Create an empty image file\n        fd, self.pathname = tempfile.mkstemp(suffix='.jpg')\n        os.write(fd, EMPTY_JPG_DATA)\n        os.close(fd)\n\n    def tearDown(self):\n        os.remove(self.pathname)\n\n    def _test_add_comment(self, value):\n        metadata = ImageMetadata(self.pathname)\n        metadata.read()\n        key = 'Exif.Photo.UserComment'\n        metadata[key] = value\n        metadata.write()\n\n        metadata = ImageMetadata(self.pathname)\n        metadata.read()\n        self.assert_(key in metadata.exif_keys)\n        tag = metadata[key]\n        self.assertEqual(tag.type, 'Comment')\n        self.assertEqual(tag.value, value)\n\n    def test_add_comment_ascii(self):\n        self._test_add_comment('deja vu')\n\n    def test_add_comment_unicode(self):\n        self._test_add_comment(u'dj vu')\n\n"}
{"repo_name":"andyzsf/edx","ref":"refs/heads/master","path":"common/djangoapps/student/migrations/0020_add_test_center_user.py","content":"# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        # Adding model 'TestCenterUser'\n        db.create_table('student_testcenteruser', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('user', self.gf('django.db.models.fields.related.ForeignKey')(default=None, to=orm['auth.User'], unique=True)),\n            ('created_at', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, db_index=True, blank=True)),\n            ('updated_at', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, db_index=True, blank=True)),\n            ('user_updated_at', self.gf('django.db.models.fields.DateTimeField')(db_index=True)),\n            ('candidate_id', self.gf('django.db.models.fields.IntegerField')(null=True, db_index=True)),\n            ('client_candidate_id', self.gf('django.db.models.fields.CharField')(max_length=50, db_index=True)),\n            ('first_name', self.gf('django.db.models.fields.CharField')(max_length=30, db_index=True)),\n            ('last_name', self.gf('django.db.models.fields.CharField')(max_length=50, db_index=True)),\n            ('middle_name', self.gf('django.db.models.fields.CharField')(max_length=30, blank=True)),\n            ('suffix', self.gf('django.db.models.fields.CharField')(max_length=255, blank=True)),\n            ('salutation', self.gf('django.db.models.fields.CharField')(max_length=50, blank=True)),\n            ('address_1', self.gf('django.db.models.fields.CharField')(max_length=40)),\n            ('address_2', self.gf('django.db.models.fields.CharField')(max_length=40, blank=True)),\n            ('address_3', self.gf('django.db.models.fields.CharField')(max_length=40, blank=True)),\n            ('city', self.gf('django.db.models.fields.CharField')(max_length=32, db_index=True)),\n            ('state', self.gf('django.db.models.fields.CharField')(db_index=True, max_length=20, blank=True)),\n            ('postal_code', self.gf('django.db.models.fields.CharField')(db_index=True, max_length=16, blank=True)),\n            ('country', self.gf('django.db.models.fields.CharField')(max_length=3, db_index=True)),\n            ('phone', self.gf('django.db.models.fields.CharField')(max_length=35)),\n            ('extension', self.gf('django.db.models.fields.CharField')(db_index=True, max_length=8, blank=True)),\n            ('phone_country_code', self.gf('django.db.models.fields.CharField')(max_length=3, db_index=True)),\n            ('fax', self.gf('django.db.models.fields.CharField')(max_length=35, blank=True)),\n            ('fax_country_code', self.gf('django.db.models.fields.CharField')(max_length=3, blank=True)),\n            ('company_name', self.gf('django.db.models.fields.CharField')(max_length=50, blank=True)),\n        ))\n        db.send_create_signal('student', ['TestCenterUser'])\n\n\n    def backwards(self, orm):\n        # Deleting model 'TestCenterUser'\n        db.delete_table('student_testcenteruser')\n\n\n    models = {\n        'auth.group': {\n            'Meta': {'object_name': 'Group'},\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '80'}),\n            'permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': \"orm['auth.Permission']\", 'symmetrical': 'False', 'blank': 'True'})\n        },\n        'auth.permission': {\n            'Meta': {'ordering': \"('content_type__app_label', 'content_type__model', 'codename')\", 'unique_together': \"(('content_type', 'codename'),)\", 'object_name': 'Permission'},\n            'codename': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': \"orm['contenttypes.ContentType']\"}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'})\n        },\n        'auth.user': {\n            'Meta': {'object_name': 'User'},\n            'about': ('django.db.models.fields.TextField', [], {'blank': 'True'}),\n            'avatar_type': ('django.db.models.fields.CharField', [], {'default': \"'n'\", 'max_length': '1'}),\n            'bronze': ('django.db.models.fields.SmallIntegerField', [], {'default': '0'}),\n            'consecutive_days_visit_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'country': ('django_countries.fields.CountryField', [], {'max_length': '2', 'blank': 'True'}),\n            'date_joined': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'date_of_birth': ('django.db.models.fields.DateField', [], {'null': 'True', 'blank': 'True'}),\n            'display_tag_filter_strategy': ('django.db.models.fields.SmallIntegerField', [], {'default': '0'}),\n            'email': ('django.db.models.fields.EmailField', [], {'max_length': '75', 'blank': 'True'}),\n            'email_isvalid': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'email_key': ('django.db.models.fields.CharField', [], {'max_length': '32', 'null': 'True'}),\n            'email_tag_filter_strategy': ('django.db.models.fields.SmallIntegerField', [], {'default': '1'}),\n            'first_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'gold': ('django.db.models.fields.SmallIntegerField', [], {'default': '0'}),\n            'gravatar': ('django.db.models.fields.CharField', [], {'max_length': '32'}),\n            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': \"orm['auth.Group']\", 'symmetrical': 'False', 'blank': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'ignored_tags': ('django.db.models.fields.TextField', [], {'blank': 'True'}),\n            'interesting_tags': ('django.db.models.fields.TextField', [], {'blank': 'True'}),\n            'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'is_staff': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'is_superuser': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'last_login': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'last_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'last_seen': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'location': ('django.db.models.fields.CharField', [], {'max_length': '100', 'blank': 'True'}),\n            'new_response_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'password': ('django.db.models.fields.CharField', [], {'max_length': '128'}),\n            'questions_per_page': ('django.db.models.fields.SmallIntegerField', [], {'default': '10'}),\n            'real_name': ('django.db.models.fields.CharField', [], {'max_length': '100', 'blank': 'True'}),\n            'reputation': ('django.db.models.fields.PositiveIntegerField', [], {'default': '1'}),\n            'seen_response_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),\n            'show_country': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'silver': ('django.db.models.fields.SmallIntegerField', [], {'default': '0'}),\n            'status': ('django.db.models.fields.CharField', [], {'default': \"'w'\", 'max_length': '2'}),\n            'user_permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': \"orm['auth.Permission']\", 'symmetrical': 'False', 'blank': 'True'}),\n            'username': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '30'}),\n            'website': ('django.db.models.fields.URLField', [], {'max_length': '200', 'blank': 'True'})\n        },\n        'contenttypes.contenttype': {\n            'Meta': {'ordering': \"('name',)\", 'unique_together': \"(('app_label', 'model'),)\", 'object_name': 'ContentType', 'db_table': \"'django_content_type'\"},\n            'app_label': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'model': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '100'})\n        },\n        'student.courseenrollment': {\n            'Meta': {'unique_together': \"(('user', 'course_id'),)\", 'object_name': 'CourseEnrollment'},\n            'course_id': ('django.db.models.fields.CharField', [], {'max_length': '255', 'db_index': 'True'}),\n            'created': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'null': 'True', 'db_index': 'True', 'blank': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'user': ('django.db.models.fields.related.ForeignKey', [], {'to': \"orm['auth.User']\"})\n        },\n        'student.pendingemailchange': {\n            'Meta': {'object_name': 'PendingEmailChange'},\n            'activation_key': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '32', 'db_index': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'new_email': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '255', 'blank': 'True'}),\n            'user': ('django.db.models.fields.related.OneToOneField', [], {'to': \"orm['auth.User']\", 'unique': 'True'})\n        },\n        'student.pendingnamechange': {\n            'Meta': {'object_name': 'PendingNameChange'},\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'new_name': ('django.db.models.fields.CharField', [], {'max_length': '255', 'blank': 'True'}),\n            'rationale': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),\n            'user': ('django.db.models.fields.related.OneToOneField', [], {'to': \"orm['auth.User']\", 'unique': 'True'})\n        },\n        'student.registration': {\n            'Meta': {'object_name': 'Registration', 'db_table': \"'auth_registration'\"},\n            'activation_key': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '32', 'db_index': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'user': ('django.db.models.fields.related.ForeignKey', [], {'to': \"orm['auth.User']\", 'unique': 'True'})\n        },\n        'student.testcenteruser': {\n            'Meta': {'object_name': 'TestCenterUser'},\n            'address_1': ('django.db.models.fields.CharField', [], {'max_length': '40'}),\n            'address_2': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),\n            'address_3': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),\n            'candidate_id': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'db_index': 'True'}),\n            'city': ('django.db.models.fields.CharField', [], {'max_length': '32', 'db_index': 'True'}),\n            'client_candidate_id': ('django.db.models.fields.CharField', [], {'max_length': '50', 'db_index': 'True'}),\n            'company_name': ('django.db.models.fields.CharField', [], {'max_length': '50', 'blank': 'True'}),\n            'country': ('django.db.models.fields.CharField', [], {'max_length': '3', 'db_index': 'True'}),\n            'created_at': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),\n            'extension': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '8', 'blank': 'True'}),\n            'fax': ('django.db.models.fields.CharField', [], {'max_length': '35', 'blank': 'True'}),\n            'fax_country_code': ('django.db.models.fields.CharField', [], {'max_length': '3', 'blank': 'True'}),\n            'first_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'db_index': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'last_name': ('django.db.models.fields.CharField', [], {'max_length': '50', 'db_index': 'True'}),\n            'middle_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'phone': ('django.db.models.fields.CharField', [], {'max_length': '35'}),\n            'phone_country_code': ('django.db.models.fields.CharField', [], {'max_length': '3', 'db_index': 'True'}),\n            'postal_code': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '16', 'blank': 'True'}),\n            'salutation': ('django.db.models.fields.CharField', [], {'max_length': '50', 'blank': 'True'}),\n            'state': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '20', 'blank': 'True'}),\n            'suffix': ('django.db.models.fields.CharField', [], {'max_length': '255', 'blank': 'True'}),\n            'updated_at': ('django.db.models.fields.DateTimeField', [], {'auto_now': 'True', 'db_index': 'True', 'blank': 'True'}),\n            'user': ('django.db.models.fields.related.ForeignKey', [], {'default': 'None', 'to': \"orm['auth.User']\", 'unique': 'True'}),\n            'user_updated_at': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'})\n        },\n        'student.userprofile': {\n            'Meta': {'object_name': 'UserProfile', 'db_table': \"'auth_userprofile'\"},\n            'courseware': ('django.db.models.fields.CharField', [], {'default': \"'course.xml'\", 'max_length': '255', 'blank': 'True'}),\n            'gender': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '6', 'null': 'True', 'blank': 'True'}),\n            'goals': ('django.db.models.fields.TextField', [], {'null': 'True', 'blank': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'language': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '255', 'blank': 'True'}),\n            'level_of_education': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '6', 'null': 'True', 'blank': 'True'}),\n            'location': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '255', 'blank': 'True'}),\n            'mailing_address': ('django.db.models.fields.TextField', [], {'null': 'True', 'blank': 'True'}),\n            'meta': ('django.db.models.fields.TextField', [], {'blank': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'db_index': 'True', 'max_length': '255', 'blank': 'True'}),\n            'user': ('django.db.models.fields.related.OneToOneField', [], {'related_name': \"'profile'\", 'unique': 'True', 'to': \"orm['auth.User']\"}),\n            'year_of_birth': ('django.db.models.fields.IntegerField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'})\n        },\n        'student.usertestgroup': {\n            'Meta': {'object_name': 'UserTestGroup'},\n            'description': ('django.db.models.fields.TextField', [], {'blank': 'True'}),\n            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '32', 'db_index': 'True'}),\n            'users': ('django.db.models.fields.related.ManyToManyField', [], {'to': \"orm['auth.User']\", 'db_index': 'True', 'symmetrical': 'False'})\n        }\n    }\n\n    complete_apps = ['student']\n"}
{"repo_name":"Syrcon/servo","ref":"refs/heads/master","path":"tests/wpt/web-platform-tests/tools/serve/serve.py","content":"# -*- coding: utf-8 -*-\nimport argparse\nimport json\nimport os\nimport signal\nimport socket\nimport sys\nimport threading\nimport time\nimport traceback\nimport urllib2\nimport uuid\nfrom collections import defaultdict, OrderedDict\nfrom multiprocessing import Process, Event\n\nfrom .. import localpaths\n\nimport sslutils\nfrom wptserve import server as wptserve, handlers\nfrom wptserve import stash\nfrom wptserve.logger import set_logger\nfrom mod_pywebsocket import standalone as pywebsocket\n\nrepo_root = localpaths.repo_root\n\nclass WorkersHandler(object):\n    def __init__(self):\n        self.handler = handlers.handler(self.handle_request)\n\n    def __call__(self, request, response):\n        return self.handler(request, response)\n\n    def handle_request(self, request, response):\n        worker_path = request.url_parts.path.replace(\".worker\", \".worker.js\")\n        return \"\"\"\u003c!doctype html\u003e\n\u003cmeta charset=utf-8\u003e\n\u003cscript src=\"/resources/testharness.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/resources/testharnessreport.js\"\u003e\u003c/script\u003e\n\u003cdiv id=log\u003e\u003c/div\u003e\n\u003cscript\u003e\nfetch_tests_from_worker(new Worker(\"%s\"));\n\u003c/script\u003e\n\"\"\" % (worker_path,)\n\nrewrites = [(\"GET\", \"/resources/WebIDLParser.js\", \"/resources/webidl2/lib/webidl2.js\")]\n\nsubdomains = [u\"www\",\n              u\"www1\",\n              u\"www2\",\n              u\"\",\n              u\"lve\"]\n\nclass RoutesBuilder(object):\n    def __init__(self):\n        self.forbidden_override = [(\"GET\", \"/tools/runner/*\", handlers.file_handler),\n                                   (\"POST\", \"/tools/runner/update_manifest.py\",\n                                    handlers.python_script_handler)]\n\n        self.forbidden = [(\"*\", \"/_certs/*\", handlers.ErrorHandler(404)),\n                          (\"*\", \"/tools/*\", handlers.ErrorHandler(404)),\n                          (\"*\", \"{spec}/tools/*\", handlers.ErrorHandler(404)),\n                          (\"*\", \"/serve.py\", handlers.ErrorHandler(404))]\n\n        self.static = [(\"GET\", \"*.worker\", WorkersHandler())]\n\n        self.mountpoint_routes = OrderedDict()\n\n        self.add_mount_point(\"/\", None)\n\n    def get_routes(self):\n        routes = self.forbidden_override + self.forbidden + self.static\n        # Using reversed here means that mount points that are added later\n        # get higher priority. This makes sense since / is typically added\n        # first.\n        for item in reversed(self.mountpoint_routes.values()):\n            routes.extend(item)\n        return routes\n\n    def add_static(self, path, format_args, content_type, route):\n        handler = handlers.StaticHandler(path, format_args, content_type)\n        self.static.append((b\"GET\", str(route), handler))\n\n    def add_mount_point(self, url_base, path):\n        url_base = \"/%s/\" % url_base.strip(\"/\") if url_base != \"/\" else \"/\"\n\n        self.mountpoint_routes[url_base] = []\n\n        routes = [(\"GET\", \"*.asis\", handlers.AsIsHandler),\n                  (\"*\", \"*.py\", handlers.PythonScriptHandler),\n                  (\"GET\", \"*\", handlers.FileHandler)]\n\n        for (method, suffix, handler_cls) in routes:\n            self.mountpoint_routes[url_base].append(\n                (method,\n                 b\"%s%s\" % (str(url_base) if url_base != \"/\" else \"\", str(suffix)),\n                 handler_cls(base_path=path, url_base=url_base)))\n\n\ndef default_routes():\n    return RoutesBuilder().get_routes()\n\n\ndef setup_logger(level):\n    import logging\n    global logger\n    logger = logging.getLogger(\"web-platform-tests\")\n    logging.basicConfig(level=getattr(logging, level.upper()))\n    set_logger(logger)\n\n\ndef open_socket(port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    if port != 0:\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind(('127.0.0.1', port))\n    sock.listen(5)\n    return sock\n\n\ndef get_port():\n    free_socket = open_socket(0)\n    port = free_socket.getsockname()[1]\n    logger.debug(\"Going to use port %s\" % port)\n    free_socket.close()\n    return port\n\n\nclass ServerProc(object):\n    def __init__(self):\n        self.proc = None\n        self.daemon = None\n        self.stop = Event()\n\n    def start(self, init_func, host, port, paths, routes, bind_hostname, external_config,\n              ssl_config, **kwargs):\n        self.proc = Process(target=self.create_daemon,\n                            args=(init_func, host, port, paths, routes, bind_hostname,\n                                  external_config, ssl_config))\n        self.proc.daemon = True\n        self.proc.start()\n\n    def create_daemon(self, init_func, host, port, paths, routes, bind_hostname,\n                      external_config, ssl_config, **kwargs):\n        try:\n            self.daemon = init_func(host, port, paths, routes, bind_hostname, external_config,\n                                    ssl_config, **kwargs)\n        except socket.error:\n            print \u003e\u003e sys.stderr, \"Socket error on port %s\" % port\n            raise\n        except:\n            print \u003e\u003e sys.stderr, traceback.format_exc()\n            raise\n\n        if self.daemon:\n            try:\n                self.daemon.start(block=False)\n                try:\n                    self.stop.wait()\n                except KeyboardInterrupt:\n                    pass\n            except:\n                print \u003e\u003e sys.stderr, traceback.format_exc()\n                raise\n\n    def wait(self):\n        self.stop.set()\n        self.proc.join()\n\n    def kill(self):\n        self.stop.set()\n        self.proc.terminate()\n        self.proc.join()\n\n    def is_alive(self):\n        return self.proc.is_alive()\n\n\ndef check_subdomains(host, paths, bind_hostname, ssl_config):\n    port = get_port()\n    subdomains = get_subdomains(host)\n\n    wrapper = ServerProc()\n    wrapper.start(start_http_server, host, port, paths, default_routes(), bind_hostname,\n                  None, ssl_config)\n\n    connected = False\n    for i in range(10):\n        try:\n            urllib2.urlopen(\"http://%s:%d/\" % (host, port))\n            connected = True\n            break\n        except urllib2.URLError:\n            time.sleep(1)\n\n    if not connected:\n        logger.critical(\"Failed to connect to test server on http://%s:%s You may need to edit /etc/hosts or similar\" % (host, port))\n        sys.exit(1)\n\n    for subdomain, (punycode, host) in subdomains.iteritems():\n        domain = \"%s.%s\" % (punycode, host)\n        try:\n            urllib2.urlopen(\"http://%s:%d/\" % (domain, port))\n        except Exception as e:\n            logger.critical(\"Failed probing domain %s. You may need to edit /etc/hosts or similar.\" % domain)\n            sys.exit(1)\n\n    wrapper.wait()\n\n\ndef get_subdomains(host):\n    #This assumes that the tld is ascii-only or already in punycode\n    return {subdomain: (subdomain.encode(\"idna\"), host)\n            for subdomain in subdomains}\n\n\ndef start_servers(host, ports, paths, routes, bind_hostname, external_config, ssl_config,\n                  **kwargs):\n    servers = defaultdict(list)\n    for scheme, ports in ports.iteritems():\n        assert len(ports) == {\"http\":2}.get(scheme, 1)\n\n        for port in ports:\n            if port is None:\n                continue\n            init_func = {\"http\":start_http_server,\n                         \"https\":start_https_server,\n                         \"ws\":start_ws_server,\n                         \"wss\":start_wss_server}[scheme]\n\n            server_proc = ServerProc()\n            server_proc.start(init_func, host, port, paths, routes, bind_hostname,\n                              external_config, ssl_config, **kwargs)\n            servers[scheme].append((port, server_proc))\n\n    return servers\n\n\ndef start_http_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\n                      **kwargs):\n    return wptserve.WebTestHttpd(host=host,\n                                 port=port,\n                                 doc_root=paths[\"doc_root\"],\n                                 routes=routes,\n                                 rewrites=rewrites,\n                                 bind_hostname=bind_hostname,\n                                 config=external_config,\n                                 use_ssl=False,\n                                 key_file=None,\n                                 certificate=None,\n                                 latency=kwargs.get(\"latency\"))\n\n\ndef start_https_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\n                       **kwargs):\n    return wptserve.WebTestHttpd(host=host,\n                                 port=port,\n                                 doc_root=paths[\"doc_root\"],\n                                 routes=routes,\n                                 rewrites=rewrites,\n                                 bind_hostname=bind_hostname,\n                                 config=external_config,\n                                 use_ssl=True,\n                                 key_file=ssl_config[\"key_path\"],\n                                 certificate=ssl_config[\"cert_path\"],\n                                 encrypt_after_connect=ssl_config[\"encrypt_after_connect\"],\n                                 latency=kwargs.get(\"latency\"))\n\n\nclass WebSocketDaemon(object):\n    def __init__(self, host, port, doc_root, handlers_root, log_level, bind_hostname,\n                 ssl_config):\n        self.host = host\n        cmd_args = [\"-p\", port,\n                    \"-d\", doc_root,\n                    \"-w\", handlers_root,\n                    \"--log-level\", log_level]\n\n        if ssl_config is not None:\n            # This is usually done through pywebsocket.main, however we're\n            # working around that to get the server instance and manually\n            # setup the wss server.\n            if pywebsocket._import_ssl():\n                tls_module = pywebsocket._TLS_BY_STANDARD_MODULE\n            elif pywebsocket._import_pyopenssl():\n                tls_module = pywebsocket._TLS_BY_PYOPENSSL\n            else:\n                print \"No SSL module available\"\n                sys.exit(1)\n\n            cmd_args += [\"--tls\",\n                         \"--private-key\", ssl_config[\"key_path\"],\n                         \"--certificate\", ssl_config[\"cert_path\"],\n                         \"--tls-module\", tls_module]\n\n        if (bind_hostname):\n            cmd_args = [\"-H\", host] + cmd_args\n        opts, args = pywebsocket._parse_args_and_config(cmd_args)\n        opts.cgi_directories = []\n        opts.is_executable_method = None\n        self.server = pywebsocket.WebSocketServer(opts)\n        ports = [item[0].getsockname()[1] for item in self.server._sockets]\n        assert all(item == ports[0] for item in ports)\n        self.port = ports[0]\n        self.started = False\n        self.server_thread = None\n\n    def start(self, block=False):\n        self.started = True\n        if block:\n            self.server.serve_forever()\n        else:\n            self.server_thread = threading.Thread(target=self.server.serve_forever)\n            self.server_thread.setDaemon(True)  # don't hang on exit\n            self.server_thread.start()\n\n    def stop(self):\n        \"\"\"\n        Stops the server.\n\n        If the server is not running, this method has no effect.\n        \"\"\"\n        if self.started:\n            try:\n                self.server.shutdown()\n                self.server.server_close()\n                self.server_thread.join()\n                self.server_thread = None\n            except AttributeError:\n                pass\n            self.started = False\n        self.server = None\n\n\ndef start_ws_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\n                    **kwargs):\n    return WebSocketDaemon(host,\n                           str(port),\n                           repo_root,\n                           paths[\"ws_doc_root\"],\n                           \"debug\",\n                           bind_hostname,\n                           ssl_config = None)\n\n\ndef start_wss_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\n                     **kwargs):\n    return WebSocketDaemon(host,\n                           str(port),\n                           repo_root,\n                           paths[\"ws_doc_root\"],\n                           \"debug\",\n                           bind_hostname,\n                           ssl_config)\n\n\ndef get_ports(config, ssl_environment):\n    rv = defaultdict(list)\n    for scheme, ports in config[\"ports\"].iteritems():\n        for i, port in enumerate(ports):\n            if scheme in [\"wss\", \"https\"] and not ssl_environment.ssl_enabled:\n                port = None\n            if port == \"auto\":\n                port = get_port()\n            else:\n                port = port\n            rv[scheme].append(port)\n    return rv\n\n\n\ndef normalise_config(config, ports):\n    host = config[\"external_host\"] if config[\"external_host\"] else config[\"host\"]\n    domains = get_subdomains(host)\n    ports_ = {}\n    for scheme, ports_used in ports.iteritems():\n        ports_[scheme] = ports_used\n\n    for key, value in domains.iteritems():\n        domains[key] = \".\".join(value)\n\n    domains[\"\"] = host\n\n    ports_ = {}\n    for scheme, ports_used in ports.iteritems():\n        ports_[scheme] = ports_used\n\n    return {\"host\": host,\n            \"domains\": domains,\n            \"ports\": ports_}\n\n\ndef get_ssl_config(config, external_domains, ssl_environment):\n    key_path, cert_path = ssl_environment.host_cert_path(external_domains)\n    return {\"key_path\": key_path,\n            \"cert_path\": cert_path,\n            \"encrypt_after_connect\": config[\"ssl\"][\"encrypt_after_connect\"]}\n\ndef start(config, ssl_environment, routes, **kwargs):\n    host = config[\"host\"]\n    domains = get_subdomains(host)\n    ports = get_ports(config, ssl_environment)\n    bind_hostname = config[\"bind_hostname\"]\n\n    paths = {\"doc_root\": config[\"doc_root\"],\n             \"ws_doc_root\": config[\"ws_doc_root\"]}\n\n    external_config = normalise_config(config, ports)\n\n    ssl_config = get_ssl_config(config, external_config[\"domains\"].values(), ssl_environment)\n\n    if config[\"check_subdomains\"]:\n        check_subdomains(host, paths, bind_hostname, ssl_config)\n\n    servers = start_servers(host, ports, paths, routes, bind_hostname, external_config,\n                            ssl_config, **kwargs)\n\n    return external_config, servers\n\n\ndef iter_procs(servers):\n    for servers in servers.values():\n        for port, server in servers:\n            yield server.proc\n\n\ndef value_set(config, key):\n    return key in config and config[key] is not None\n\n\ndef get_value_or_default(config, key, default=None):\n    return config[key] if value_set(config, key) else default\n\n\ndef set_computed_defaults(config):\n    if not value_set(config, \"doc_root\"):\n        config[\"doc_root\"] = repo_root\n\n    if not value_set(config, \"ws_doc_root\"):\n        root = get_value_or_default(config, \"doc_root\", default=repo_root)\n        config[\"ws_doc_root\"] = os.path.join(root, \"websockets\", \"handlers\")\n\n\ndef merge_json(base_obj, override_obj):\n    rv = {}\n    for key, value in base_obj.iteritems():\n        if key not in override_obj:\n            rv[key] = value\n        else:\n            if isinstance(value, dict):\n                rv[key] = merge_json(value, override_obj[key])\n            else:\n                rv[key] = override_obj[key]\n    return rv\n\n\ndef get_ssl_environment(config):\n    implementation_type = config[\"ssl\"][\"type\"]\n    cls = sslutils.environments[implementation_type]\n    try:\n        kwargs = config[\"ssl\"][implementation_type].copy()\n    except KeyError:\n        raise ValueError(\"%s is not a vaid ssl type.\" % implementation_type)\n    return cls(logger, **kwargs)\n\n\ndef load_config(default_path, override_path=None, **kwargs):\n    if os.path.exists(default_path):\n        with open(default_path) as f:\n            base_obj = json.load(f)\n    else:\n        raise ValueError(\"Config path %s does not exist\" % default_path)\n\n    if os.path.exists(override_path):\n        with open(override_path) as f:\n            override_obj = json.load(f)\n    else:\n        override_obj = {}\n    rv = merge_json(base_obj, override_obj)\n\n    if kwargs.get(\"config_path\"):\n        other_path = os.path.abspath(os.path.expanduser(kwargs.get(\"config_path\")))\n        if os.path.exists(other_path):\n            base_obj = rv\n            with open(other_path) as f:\n                override_obj = json.load(f)\n            rv = merge_json(base_obj, override_obj)\n        else:\n            raise ValueError(\"Config path %s does not exist\" % other_path)\n\n    overriding_path_args = [(\"doc_root\", \"Document root\"),\n                            (\"ws_doc_root\", \"WebSockets document root\")]\n    for key, title in overriding_path_args:\n        value = kwargs.get(key)\n        if value is None:\n            continue\n        value = os.path.abspath(os.path.expanduser(value))\n        if not os.path.exists(value):\n            raise ValueError(\"%s path %s does not exist\" % (title, value))\n        rv[key] = value\n\n    set_computed_defaults(rv)\n    return rv\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--latency\", type=int,\n                        help=\"Artificial latency to add before sending http responses, in ms\")\n    parser.add_argument(\"--config\", action=\"store\", dest=\"config_path\",\n                        help=\"Path to external config file\")\n    parser.add_argument(\"--doc_root\", action=\"store\", dest=\"doc_root\",\n                        help=\"Path to document root. Overrides config.\")\n    parser.add_argument(\"--ws_doc_root\", action=\"store\", dest=\"ws_doc_root\",\n                        help=\"Path to WebSockets document root. Overrides config.\")\n    return parser\n\n\ndef main():\n    kwargs = vars(get_parser().parse_args())\n    config = load_config(\"config.default.json\",\n                         \"config.json\",\n                         **kwargs)\n\n    setup_logger(config[\"log_level\"])\n\n    with stash.StashServer((config[\"host\"], get_port()), authkey=str(uuid.uuid4())):\n        with get_ssl_environment(config) as ssl_env:\n            config_, servers = start(config, ssl_env, default_routes(), **kwargs)\n\n            try:\n                while any(item.is_alive() for item in iter_procs(servers)):\n                    for item in iter_procs(servers):\n                        item.join(1)\n            except KeyboardInterrupt:\n                logger.info(\"Shutting down\")\n"}
{"repo_name":"xadahiya/django","ref":"refs/heads/master","path":"django/contrib/admin/actions.py","content":"\"\"\"\nBuilt-in, globally-available admin actions.\n\"\"\"\n\nfrom django.contrib import messages\nfrom django.contrib.admin import helpers\nfrom django.contrib.admin.utils import get_deleted_objects, model_ngettext\nfrom django.core.exceptions import PermissionDenied\nfrom django.db import router\nfrom django.template.response import TemplateResponse\nfrom django.utils.encoding import force_text\nfrom django.utils.translation import ugettext as _, ugettext_lazy\n\n\ndef delete_selected(modeladmin, request, queryset):\n    \"\"\"\n    Default action which deletes the selected objects.\n\n    This action first displays a confirmation page whichs shows all the\n    deleteable objects, or, if the user has no permission one of the related\n    childs (foreignkeys), a \"permission denied\" message.\n\n    Next, it deletes all selected objects and redirects back to the change list.\n    \"\"\"\n    opts = modeladmin.model._meta\n    app_label = opts.app_label\n\n    # Check that the user has delete permission for the actual model\n    if not modeladmin.has_delete_permission(request):\n        raise PermissionDenied\n\n    using = router.db_for_write(modeladmin.model)\n\n    # Populate deletable_objects, a data structure of all related objects that\n    # will also be deleted.\n    deletable_objects, model_count, perms_needed, protected = get_deleted_objects(\n        queryset, opts, request.user, modeladmin.admin_site, using)\n\n    # The user has already confirmed the deletion.\n    # Do the deletion and return a None to display the change list view again.\n    if request.POST.get('post'):\n        if perms_needed:\n            raise PermissionDenied\n        n = queryset.count()\n        if n:\n            for obj in queryset:\n                obj_display = force_text(obj)\n                modeladmin.log_deletion(request, obj, obj_display)\n            queryset.delete()\n            modeladmin.message_user(request, _(\"Successfully deleted %(count)d %(items)s.\") % {\n                \"count\": n, \"items\": model_ngettext(modeladmin.opts, n)\n            }, messages.SUCCESS)\n        # Return None to display the change list page again.\n        return None\n\n    if len(queryset) == 1:\n        objects_name = force_text(opts.verbose_name)\n    else:\n        objects_name = force_text(opts.verbose_name_plural)\n\n    if perms_needed or protected:\n        title = _(\"Cannot delete %(name)s\") % {\"name\": objects_name}\n    else:\n        title = _(\"Are you sure?\")\n\n    context = dict(\n        modeladmin.admin_site.each_context(request),\n        title=title,\n        objects_name=objects_name,\n        deletable_objects=[deletable_objects],\n        model_count=dict(model_count).items(),\n        queryset=queryset,\n        perms_lacking=perms_needed,\n        protected=protected,\n        opts=opts,\n        action_checkbox_name=helpers.ACTION_CHECKBOX_NAME,\n    )\n\n    request.current_app = modeladmin.admin_site.name\n\n    # Display the confirmation page\n    return TemplateResponse(request, modeladmin.delete_selected_confirmation_template or [\n        \"admin/%s/%s/delete_selected_confirmation.html\" % (app_label, opts.model_name),\n        \"admin/%s/delete_selected_confirmation.html\" % app_label,\n        \"admin/delete_selected_confirmation.html\"\n    ], context)\n\ndelete_selected.short_description = ugettext_lazy(\"Delete selected %(verbose_name_plural)s\")\n"}
{"repo_name":"wsmith323/django","ref":"refs/heads/master","path":"tests/save_delete_hooks/tests.py","content":"from __future__ import unicode_literals\n\nfrom django.test import TestCase\nfrom django.utils import six\n\nfrom .models import Person\n\n\nclass SaveDeleteHookTests(TestCase):\n    def test_basic(self):\n        p = Person(first_name=\"John\", last_name=\"Smith\")\n        self.assertEqual(p.data, [])\n        p.save()\n        self.assertEqual(p.data, [\n            \"Before save\",\n            \"After save\",\n        ])\n\n        self.assertQuerysetEqual(\n            Person.objects.all(), [\n                \"John Smith\",\n            ],\n            six.text_type\n        )\n\n        p.delete()\n        self.assertEqual(p.data, [\n            \"Before save\",\n            \"After save\",\n            \"Before deletion\",\n            \"After deletion\",\n        ])\n        self.assertQuerysetEqual(Person.objects.all(), [])\n"}
{"repo_name":"sparkslabs/kamaelia","ref":"refs/heads/master","path":"Sketches/CL/Topology/src/RelationTopology/Util/RelationAttributeParsing.py","content":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2010 British Broadcasting Corporation and Kamaelia Contributors(1)\n#\n# (1) Kamaelia Contributors are listed in the AUTHORS file and at\n#     http://www.kamaelia.org/AUTHORS - please extend this file,\n#     not this notice.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\\\n===============================================================\nParse entities, attributes and relations definition received\n===============================================================\n\nParse entities and relations definition received, one line one time.\n\n1. Definition format\n1.) Empty line (including any number of white spaces)\n2.) Line starting with # to comment\n3.) Entity definition\nExample:\n--------\nperson mum\nperson dad gender=male,shape=rect,width=80,height=80\nperson son gender=\"male\",photo=\"../Files/son.gif,width=60,height=60\"\nperson daughter radius=100\n4.) Relation definition\nExample: \n--------\nchildof(mum, son)\n\n2. NOTE:\n1.) Any number of spaces can exist before, after and between the above line\nExample:\n--------\n  person    mum  \n     childof  (  mum  , son  )  \n2.) Parse one line one time and then send out\n3.) Entity definition needs to come before relation definition \nif the relations definition uses the entity\n4.) When encountering repeated entity, it will update its attributes rather than\ncreate a new one.       \n\"\"\"\n\ndef parseEntity(entityLine):\n    \"\"\" parse entity line \"\"\"\n    result = entityLine.split()\n    #entity_ID = result[0]+'_'+result[1]\n    entity_name = result[1]\n    #particle = '-'\n    particle = 'GenericParticle'\n    if len(result) == 3:\n        attributes = result[2]\n        #attributes = attributes.lower()\n        attributes = attributes.replace('gender','color')\n        attributes = attributes.replace('female','pink')\n        attributes = attributes.replace('male','blue')\n        attributes = attributes.replace('photo','pic')\n        attributes = attributes + ',type=' + result[0]\n    else:\n        attributes = 'type=' + result[0]               \n    return \"ADD NODE %s %s auto %s %s\" % (entity_name,entity_name,particle,attributes)\n\ndef parseUpdatedEntity(entityLine):\n    \"\"\" parse entity line \"\"\"\n    result = entityLine.split()\n    #entity_ID = result[0]+'_'+result[1]\n    entity_name = result[1]\n    #particle = '-'\n    #particle = 'GenericParticle'\n    if len(result) == 3:\n        attributes = result[2]\n        #attributes = attributes.lower()\n        attributes = attributes.replace('gender','color')\n        attributes = attributes.replace('female','pink')\n        attributes = attributes.replace('male','blue')\n        attributes = attributes.replace('photo','pic')\n        attributes = attributes.replace('name','label')\n    else:\n        attributes = 'label=' + entity_name              \n    return \"UPDATE NODE %s %s\" % (entity_name,attributes)\n\ndef parseRelation(relationLine):\n    \"\"\" parse relation line \"\"\"\n    result = relationLine.split('(')\n    relation = result[0].strip()\n    entities_str = result[1].rstrip(')')\n    entities_list = entities_str.split(',')\n    src = entities_list[0].strip()\n    dst = entities_list[1].strip()\n    return \"ADD LINK %s %s %s\" % (src,dst,relation)\n\n\n        \nimport re\n\nimport Axon\nfrom Axon.Ipc import producerFinished, shutdownMicroprocess\n\nclass RelationAttributeParser(Axon.Component.component):\n    \"\"\"\\\n======================================================================\nA component to parse entities, attributes and relations definition\n======================================================================\n\"\"\"\n    def shutdown(self):\n        \"\"\" shutdown method: define when to shun down\"\"\"\n        while self.dataReady(\"control\"):\n            data = self.recv(\"control\")\n            if isinstance(data, producerFinished) or isinstance(data, shutdownMicroprocess):\n                self.shutdown_mess = data\n                return True\n        return False\n      \n    def main(self):\n        \"\"\" main method: do stuff \"\"\"\n        \n        previousNodes = []  \n        \n        # Put all codes within the loop, so that others can be run even it doesn't shut down\n        while not self.shutdown():\n            X = []\n            links = []\n            nodes = []\n            updatedNodes = []\n            while not self.anyReady():\n                self.pause()\n                yield 1\n    \n            while self.dataReady(\"inbox\"):\n                L = self.recv(\"inbox\")\n                if L.strip() == \"\": continue # empty line\n                if L.lstrip()[0] == \"#\": continue # comment\n                X.append(L.strip())\n            #yield 1\n\n            for item in X:            \n                if re.match('(.+)\\((.+),(.+)\\)',item): # relation\n                    command = parseRelation(item)\n                    links.append(command)\n                else:\n                    isRepeated = False\n                    for node in previousNodes:\n                        if item.split()[1] == node.split()[2]:\n                            isRepeated = True\n                    if not isRepeated: # new entity\n                        command = parseEntity(item)\n                        nodes.append(command)        \n                        previousNodes.append(command)\n                    else: # old entity\n                        command = parseUpdatedEntity(item)\n                        updatedNodes.append(command)\n            #yield 1\n            for node in nodes:\n                self.send(node, \"outbox\")\n            for updatedNode in updatedNodes:\n                self.send(updatedNode, \"outbox\")\n            for link in links:\n                self.send(link, \"outbox\")\n            yield 1\n            \n        \n        self.send(self.shutdown_mess,\"signal\")\n        \nif __name__ == \"__main__\":\n    from Kamaelia.Util.DataSource import DataSource\n    from Kamaelia.Visualisation.PhysicsGraph.lines_to_tokenlists import lines_to_tokenlists\n    from Kamaelia.Util.Console import ConsoleReader,ConsoleEchoer\n    from GenericTopologyViewer import GenericTopologyViewer\n    from Kamaelia.Chassis.Graphline import Graphline\n    \n    # Data can be from both DataSource and console inputs\n    Graphline(\n        CONSOLEREADER = ConsoleReader(),\n        DATASOURCE = DataSource([\"  person  mum   gender=female,photo=../Files/mum.jpg,width=80,height=80 \", '  ', \"\"\"   \n                    \"\"\", 'person dad gender=male,shape=rect,width=80,height=80', \n                    '  person  son   gender=male,photo=../Files/son.gif,width=60,height=60',\n                    'person son photo=../Files/son1.gif',\n                     'person daughter radius=20', 'person daughter radius=100',\n                     ' childof  (  mum  , son  ) ', 'childof(mum, daughter)',\n                     'childof(dad, son)', 'childof(dad, daughter)']),\n        PARSER = RelationAttributeParser(),\n        TOKENS = lines_to_tokenlists(),\n        VIEWER = GenericTopologyViewer(),\n        CONSOLEECHOER = ConsoleEchoer(),\n    linkages = {\n        (\"CONSOLEREADER\",\"outbox\") : (\"PARSER\",\"inbox\"),\n        (\"DATASOURCE\",\"outbox\") : (\"PARSER\",\"inbox\"),\n        (\"PARSER\",\"outbox\") : (\"TOKENS\",\"inbox\"),\n        (\"TOKENS\",\"outbox\")   : (\"VIEWER\",\"inbox\"),\n        (\"VIEWER\",\"outbox\")  : (\"CONSOLEECHOER\",\"inbox\"),\n        \n    }\n).run()"}
{"repo_name":"dssg/wikienergy","ref":"refs/heads/master","path":"disaggregator/build/pandas/pandas/io/tests/__init__.py","content":"\ndef setUp():\n    import socket\n    socket.setdefaulttimeout(5)\n"}
{"repo_name":"soulxu/libvirt-xuhj","ref":"refs/heads/master","path":"src/esx/esx_vi_generator.py","content":"#!/usr/bin/env python\n\n#\n# esx_vi_generator.py: generates most of the SOAP type mapping code\n#\n# Copyright (C) 2010-2011 Matthias Bolte \u003cmatthias.bolte@googlemail.com\u003e\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307  USA\n#\n\nimport sys\nimport os\nimport os.path\n\n\n\nOCCURRENCE__REQUIRED_ITEM = \"r\"\nOCCURRENCE__REQUIRED_LIST = \"rl\"\nOCCURRENCE__OPTIONAL_ITEM = \"o\"\nOCCURRENCE__OPTIONAL_LIST = \"ol\"\nOCCURRENCE__IGNORED = \"i\"\n\nvalid_occurrences = [OCCURRENCE__REQUIRED_ITEM,\n                     OCCURRENCE__REQUIRED_LIST,\n                     OCCURRENCE__OPTIONAL_ITEM,\n                     OCCURRENCE__OPTIONAL_LIST,\n                     OCCURRENCE__IGNORED]\n\nautobind_names = set()\n\nseparator = \"/* \" + (\"* \" * 37) + \"*\\n\"\n\n\n\ndef aligned(left, right, length=59):\n    while len(left) \u003c length:\n        left += \" \"\n\n    return left + right\n\n\n\nclass Member:\n    def __init__(self, type, occurrence):\n        self.type = type\n        self.occurrence = occurrence\n\n\n    def is_enum(self):\n        return self.type in predefined_enums or self.type in enums_by_name\n\n\n    def is_object(self):\n        return self.type in predefined_objects or self.type in objects_by_name\n\n\n    def is_type_generated(self):\n        return self.type in enums_by_name or self.type in objects_by_name\n\n\n    def get_occurrence_comment(self):\n        if self.occurrence == OCCURRENCE__REQUIRED_ITEM:\n            return \"/* required */\"\n        elif self.occurrence == OCCURRENCE__REQUIRED_LIST:\n            return \"/* required, list */\"\n        elif self.occurrence == OCCURRENCE__OPTIONAL_ITEM:\n            return \"/* optional */\"\n        elif self.occurrence == OCCURRENCE__OPTIONAL_LIST:\n            return \"/* optional, list */\"\n\n        raise ValueError(\"unknown occurrence value '%s'\" % self.occurrence)\n\n\n\nclass Parameter(Member):\n    def __init__(self, type, name, occurrence):\n        Member.__init__(self, type, occurrence)\n\n        if ':' in name and name.startswith(\"_this\"):\n            self.name, self.autobind_name = name.split(\":\")\n        else:\n            self.name = name\n            self.autobind_name = None\n\n\n    def generate_parameter(self, is_last=False, is_header=True, offset=0):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            raise ValueError(\"invalid function parameter occurrence value '%s'\"\n                             % self.occurrence)\n        elif self.autobind_name is not None:\n            return \"\"\n        else:\n            string = \"       \"\n            string += \" \" * offset\n            string += \"%s%s\" % (self.get_type_string(), self.name)\n\n            if is_last:\n                if is_header:\n                    string += \"); \"\n                else:\n                    string += \"), \"\n            else:\n                string += \", \"\n\n            return aligned(string, self.get_occurrence_comment() + \"\\n\")\n\n\n    def generate_return(self, offset = 0, end_of_line = \";\"):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            raise ValueError(\"invalid function parameter occurrence value '%s'\"\n                             % self.occurrence)\n        else:\n            string = \"       \"\n            string += \" \" * offset\n            string += \"%s%s)%s\" \\\n                      % (self.get_type_string(True), self.name, end_of_line)\n\n            return aligned(string, self.get_occurrence_comment() + \"\\n\")\n\n\n    def generate_require_code(self):\n        if self.occurrence in [OCCURRENCE__REQUIRED_ITEM,\n                               OCCURRENCE__REQUIRED_LIST]:\n            return \"    ESX_VI__METHOD__PARAMETER__REQUIRE(%s)\\n\" % self.name\n        else:\n            return \"\"\n\n\n    def generate_serialize_code(self):\n        if self.occurrence in [OCCURRENCE__REQUIRED_LIST,\n                               OCCURRENCE__OPTIONAL_LIST]:\n            return \"    ESX_VI__METHOD__PARAMETER__SERIALIZE_LIST(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n        elif self.type == \"String\":\n            return \"    ESX_VI__METHOD__PARAMETER__SERIALIZE_VALUE(String, %s)\\n\" \\\n                   % self.name\n        else:\n            return \"    ESX_VI__METHOD__PARAMETER__SERIALIZE(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n\n\n    def get_type_string(self, as_return_value=False):\n        string = \"\"\n\n        if self.type == \"String\" and \\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\n                                   OCCURRENCE__OPTIONAL_LIST]:\n            if as_return_value:\n                string += \"char *\"\n            else:\n                string += \"const char *\"\n        elif self.is_enum():\n            string += \"esxVI_%s \" % self.type\n        else:\n            string += \"esxVI_%s *\" % self.type\n\n        if as_return_value:\n            string += \"*\"\n\n        return string\n\n\n    def get_occurrence_short_enum(self):\n        if self.occurrence == OCCURRENCE__REQUIRED_ITEM:\n            return \"RequiredItem\"\n        elif self.occurrence == OCCURRENCE__REQUIRED_LIST:\n            return \"RequiredList\"\n        elif self.occurrence == OCCURRENCE__OPTIONAL_ITEM:\n            return \"OptionalItem\"\n        elif self.occurrence == OCCURRENCE__OPTIONAL_LIST:\n            return \"OptionalList\"\n\n        raise ValueError(\"unknown occurrence value '%s'\" % self.occurrence)\n\n\n\nclass Method:\n    def __init__(self, name, parameters, returns):\n        self.name = name\n        self.parameters = []\n        self.autobind_parameter = None\n        self.returns = returns\n\n        for parameter in parameters:\n            if parameter.autobind_name is None:\n                self.parameters.append(parameter)\n            else:\n                self.autobind_parameter = parameter\n\n\n    def generate_header(self):\n        header = \"int esxVI_%s\\n\" % self.name\n        header += \"      (esxVI_Context *ctx\"\n\n        if len(self.parameters) \u003e 0 or self.returns is not None:\n            header += \",\\n\"\n\n            for parameter in self.parameters[:-1]:\n                header += parameter.generate_parameter()\n\n            if self.returns is None:\n                header += self.parameters[-1].generate_parameter(is_last=True)\n            else:\n                header += self.parameters[-1].generate_parameter()\n                header += self.returns.generate_return()\n        else:\n            header += \");\\n\"\n\n        header += \"\\n\"\n\n        return header\n\n\n    def generate_source(self):\n        source = \"/* esxVI_%s */\\n\" % self.name\n        source += \"ESX_VI__METHOD(%s,\" % self.name\n\n        if self.autobind_parameter is not None:\n            autobind_names.add(self.autobind_parameter.autobind_name)\n            source += \" %s,\\n\" % self.autobind_parameter.autobind_name\n        else:\n            source += \" /* explicit _this */,\\n\"\n\n        source += \"               (esxVI_Context *ctx\"\n\n        if len(self.parameters) \u003e 0 or self.returns is not None:\n            source += \",\\n\"\n\n            for parameter in self.parameters[:-1]:\n                source += parameter.generate_parameter(is_header=False,\n                                                       offset=9)\n\n            if self.returns is None:\n                source += self.parameters[-1].generate_parameter(is_last=True,\n                                                                 is_header=False,\n                                                                 offset=9)\n            else:\n                source += self.parameters[-1].generate_parameter(is_header=False,\n                                                                 offset=9)\n                source += self.returns.generate_return(offset=9,\n                                                       end_of_line=\",\")\n        else:\n            source += \"),\\n\"\n\n        if self.returns is None:\n            source += \"               void, /* nothing */, None,\\n\"\n        elif self.returns.type == \"String\":\n            source += \"               String, Value, %s,\\n\" \\\n                      % self.returns.get_occurrence_short_enum()\n        else:\n            source += \"               %s, /* nothing */, %s,\\n\" \\\n                      % (self.returns.type,\n                         self.returns.get_occurrence_short_enum())\n\n        source += \"{\\n\"\n\n        if self.autobind_parameter is not None:\n            source += self.autobind_parameter.generate_require_code()\n\n        for parameter in self.parameters:\n            source += parameter.generate_require_code()\n\n        source += \"},\\n\"\n        source += \"{\\n\"\n\n        if self.autobind_parameter is not None:\n            source += self.autobind_parameter.generate_serialize_code()\n\n        for parameter in self.parameters:\n            source += parameter.generate_serialize_code()\n\n        source += \"})\\n\\n\\n\\n\"\n\n        return source\n\n\n\nclass Property(Member):\n    def __init__(self, type, name, occurrence):\n        Member.__init__(self, type, occurrence)\n\n        self.name = name\n\n\n    def generate_struct_member(self):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            return \"    /* FIXME: %s is currently ignored */\\n\" % self.name\n        else:\n            string = \"    %s%s; \" % (self.get_type_string(), self.name)\n\n            return aligned(string, self.get_occurrence_comment() + \"\\n\")\n\n\n    def generate_free_code(self):\n        if self.type == \"String\" and \\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\n                                   OCCURRENCE__OPTIONAL_LIST,\n                                   OCCURRENCE__IGNORED]:\n            return \"    VIR_FREE(item-\u003e%s);\\n\" % self.name\n        elif self.is_enum():\n            return \"\"\n        else:\n            if self.occurrence == OCCURRENCE__IGNORED:\n                return \"    /* FIXME: %s is currently ignored */\\n\" % self.name\n            else:\n                return \"    esxVI_%s_Free(\u0026item-\u003e%s);\\n\" % (self.type, self.name)\n\n\n    def generate_validate_code(self, managed=False):\n        if managed:\n            macro = \"ESX_VI__TEMPLATE__PROPERTY__MANAGED_REQUIRE\"\n        else:\n            macro = \"ESX_VI__TEMPLATE__PROPERTY__REQUIRE\"\n\n        if self.occurrence in [OCCURRENCE__REQUIRED_ITEM,\n                               OCCURRENCE__REQUIRED_LIST]:\n            return \"    %s(%s)\\n\" % (macro, self.name)\n        elif self.occurrence == OCCURRENCE__IGNORED:\n            return \"    /* FIXME: %s is currently ignored */\\n\" % self.name\n        else:\n            return \"\"\n\n\n    def generate_deep_copy_code(self):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            return \"    /* FIXME: %s is currently ignored */\\n\" % self.name\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\n                                 OCCURRENCE__OPTIONAL_LIST]:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY_LIST(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n        elif self.type == \"String\":\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY_VALUE(String, %s)\\n\" \\\n                   % self.name\n        elif self.is_enum():\n            return \"    (*dest)-\u003e%s = src-\u003e%s;\\n\" % (self.name, self.name)\n        else:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n\n\n    def generate_serialize_code(self):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            return \"    /* FIXME: %s is currently ignored */\\n\" % self.name\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\n                                 OCCURRENCE__OPTIONAL_LIST]:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE_LIST(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n        elif self.type == \"String\":\n            return \"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE_VALUE(String, %s)\\n\" \\\n                   % self.name\n        else:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n\n\n    def generate_deserialize_code(self):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_IGNORE(%s) /* FIXME */\\n\" \\\n                   % self.name\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\n                                 OCCURRENCE__OPTIONAL_LIST]:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_LIST(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n        elif self.type == \"String\":\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_VALUE(String, %s)\\n\" \\\n                   % self.name\n        else:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n\n\n    def generate_lookup_code(self):\n        if self.occurrence == OCCURRENCE__IGNORED:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__CAST_FROM_ANY_TYPE_IGNORE(%s) /* FIXME */\\n\" \\\n                   % self.name\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\n                                 OCCURRENCE__OPTIONAL_LIST]:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__CAST_LIST_FROM_ANY_TYPE(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n        elif self.type == \"String\":\n            return \"    ESX_VI__TEMPLATE__PROPERTY__CAST_VALUE_FROM_ANY_TYPE(String, %s)\\n\" \\\n                   % self.name\n        else:\n            return \"    ESX_VI__TEMPLATE__PROPERTY__CAST_FROM_ANY_TYPE(%s, %s)\\n\" \\\n                   % (self.type, self.name)\n\n\n    def get_type_string(self):\n        if self.type == \"String\" and \\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\n                                   OCCURRENCE__OPTIONAL_LIST]:\n            return \"char *\"\n        elif self.is_enum():\n            return \"esxVI_%s \" % self.type\n        else:\n            return \"esxVI_%s *\" % self.type\n\n\n\nclass Type:\n    def __init__(self, kind, name):\n        self.kind = kind\n        self.name = name\n\n\n    def generate_typedef(self):\n        return \"typedef %s _esxVI_%s esxVI_%s;\\n\" \\\n               % (self.kind, self.name, self.name)\n\n\n    def generate_typeenum(self):\n        return \"    esxVI_Type_%s,\\n\" % self.name\n\n\n    def generate_typetostring(self):\n        string = \"          case esxVI_Type_%s:\\n\" % self.name\n        string += \"            return \\\"%s\\\";\\n\\n\" % self.name\n\n        return string\n\n\n    def generate_typefromstring(self):\n        string =  \"           else if (STREQ(type, \\\"%s\\\")) {\\n\" % self.name\n        string += \"               return esxVI_Type_%s;\\n\" % self.name\n        string += \"           }\\n\"\n\n        return string\n\n\n\nclass Object(Type):\n    FEATURE__DYNAMIC_CAST = (1 \u003c\u003c 1)\n    FEATURE__LIST         = (1 \u003c\u003c 2)\n    FEATURE__DEEP_COPY    = (1 \u003c\u003c 3)\n    FEATURE__ANY_TYPE     = (1 \u003c\u003c 4)\n    FEATURE__SERIALIZE    = (1 \u003c\u003c 5)\n    FEATURE__DESERIALIZE  = (1 \u003c\u003c 6)\n\n\n    def __init__(self, name, extends, properties, features=0, extended_by=None):\n        Type.__init__(self, \"struct\", name)\n        self.extends = extends\n        self.features = features\n        self.properties = properties\n        self.extended_by = extended_by\n        self.candidate_for_dynamic_cast = False\n\n        if self.extended_by is not None:\n            self.extended_by.sort()\n\n\n    def generate_struct_members(self, add_banner=False, struct_gap=False):\n        members = \"\"\n\n        if struct_gap:\n            members += \"\\n\"\n\n        if self.extends is not None:\n            members += objects_by_name[self.extends] \\\n                       .generate_struct_members(add_banner=True,\n                                                struct_gap=False) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            members += \"    /* %s */\\n\" % self.name\n\n        for property in self.properties:\n            members += property.generate_struct_member()\n\n        if len(self.properties) \u003c 1:\n            members += \"    /* no properties */\\n\"\n\n        return members\n\n\n    def generate_free_code(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += objects_by_name[self.extends] \\\n                      .generate_free_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += property.generate_free_code()\n\n            if len(string) \u003c 1:\n                source += \"    /* no properties to be freed */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_validate_code(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += objects_by_name[self.extends] \\\n                      .generate_validate_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += property.generate_validate_code()\n\n            if len(string) \u003c 1:\n                source += \"    /* no required properties */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_dynamic_cast_code(self, is_first=True):\n        source = \"\"\n\n        if self.extended_by is not None:\n            if not is_first:\n                source += \"\\n\"\n\n            source += \"    /* %s */\\n\" % self.name\n\n            for extended_by in self.extended_by:\n                source += \"    ESX_VI__TEMPLATE__DYNAMIC_CAST__ACCEPT(%s)\\n\" \\\n                          % extended_by\n\n            for extended_by in self.extended_by:\n                source += objects_by_name[extended_by] \\\n                          .generate_dynamic_cast_code(False)\n\n        return source\n\n\n    def generate_deep_copy_code(self, add_banner = False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += objects_by_name[self.extends] \\\n                      .generate_deep_copy_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += property.generate_deep_copy_code()\n\n            if len(string) \u003c 1:\n                source += \"    /* no properties to be deep copied */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_serialize_code(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += objects_by_name[self.extends] \\\n                      .generate_serialize_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            for property in self.properties:\n                source += property.generate_serialize_code()\n\n        return source\n\n\n    def generate_deserialize_code(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += objects_by_name[self.extends] \\\n                      .generate_deserialize_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            for property in self.properties:\n                source += property.generate_deserialize_code()\n\n        return source\n\n\n    def generate_header(self):\n        header = separator\n        header += \" * VI Object: %s\\n\" % self.name\n\n        if self.extends is not None:\n            header += \" *            extends %s\\n\" % self.extends\n\n        first = True\n\n        if self.extended_by is not None:\n            for extended_by in self.extended_by:\n                if first:\n                    header += \" *            extended by %s\\n\" % extended_by\n                    first = False\n                else:\n                    header += \" *                        %s\\n\" % extended_by\n\n        header += \" */\\n\\n\"\n\n        # struct\n        header += \"struct _esxVI_%s {\\n\" % self.name\n\n        if self.features \u0026 Object.FEATURE__LIST:\n            header += aligned(\"    esxVI_%s *_next; \" % self.name,\n                              \"/* optional */\\n\")\n        else:\n            header += aligned(\"    esxVI_%s *_unused; \" % self.name,\n                              \"/* optional */\\n\")\n\n        header += aligned(\"    esxVI_Type _type; \", \"/* required */\\n\")\n        header += self.generate_struct_members(struct_gap=True)\n        header += \"};\\n\\n\"\n\n        # functions\n        header += \"int esxVI_%s_Alloc(esxVI_%s **item);\\n\" \\\n                  % (self.name, self.name)\n        header += \"void esxVI_%s_Free(esxVI_%s **item);\\n\" \\\n                  % (self.name, self.name)\n        header += \"int esxVI_%s_Validate(esxVI_%s *item);\\n\" \\\n                  % (self.name, self.name)\n\n        if self.features \u0026 Object.FEATURE__DYNAMIC_CAST:\n            if self.extended_by is not None or self.extends is not None:\n                header += \"esxVI_%s *esxVI_%s_DynamicCast(void *item);\\n\" \\\n                          % (self.name, self.name)\n            else:\n                report_error(\"cannot add dynamic cast support for an untyped object\")\n\n        if self.features \u0026 Object.FEATURE__LIST:\n            header += \"int esxVI_%s_AppendToList(esxVI_%s **list, esxVI_%s *item);\\n\" \\\n                      % (self.name, self.name, self.name)\n\n        if self.features \u0026 Object.FEATURE__DEEP_COPY:\n            header += \"int esxVI_%s_DeepCopy(esxVI_%s **dst, esxVI_%s *src);\\n\" \\\n                      % (self.name, self.name, self.name)\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                header += (\"int esxVI_%s_DeepCopyList(esxVI_%s **dstList, \"\n                                                     \"esxVI_%s *srcList);\\n\") \\\n                          % (self.name, self.name, self.name)\n\n        if self.features \u0026 Object.FEATURE__ANY_TYPE:\n            header += (\"int esxVI_%s_CastFromAnyType(esxVI_AnyType *anyType, \"\n                                                    \"esxVI_%s **item);\\n\") \\\n                      % (self.name, self.name)\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                header += (\"int esxVI_%s_CastListFromAnyType(esxVI_AnyType *anyType, \"\n                                                            \"esxVI_%s **list);\\n\") \\\n                          % (self.name, self.name)\n\n        if self.features \u0026 Object.FEATURE__SERIALIZE:\n            header += (\"int esxVI_%s_Serialize(esxVI_%s *item, \"\n                                              \"const char *element, \"\n                                              \"virBufferPtr output);\\n\") \\\n                      % (self.name, self.name)\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                header += (\"int esxVI_%s_SerializeList(esxVI_%s *list, \"\n                                                      \"const char *element, \"\n                                                      \"virBufferPtr output);\\n\") \\\n                          % (self.name, self.name)\n\n        if self.features \u0026 Object.FEATURE__DESERIALIZE:\n            header += \"int esxVI_%s_Deserialize(xmlNodePtr node, esxVI_%s **item);\\n\" \\\n                      % (self.name, self.name)\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                header += (\"int esxVI_%s_DeserializeList(xmlNodePtr node, \"\n                                                        \"esxVI_%s **list);\\n\") \\\n                          % (self.name, self.name)\n\n        header += \"\\n\\n\\n\"\n\n        return header\n\n\n    def generate_source(self):\n        source = separator\n        source += \" * VI Object: %s\\n\" % self.name\n\n        if self.extends is not None:\n            source += \" *            extends %s\\n\" % self.extends\n\n        first = True\n\n        if self.extended_by is not None:\n            for extended_by in self.extended_by:\n                if first:\n                    source += \" *            extended by %s\\n\" % extended_by\n                    first = False\n                else:\n                    source += \" *                        %s\\n\" % extended_by\n\n        source += \" */\\n\\n\"\n\n        # functions\n        source += \"/* esxVI_%s_Alloc */\\n\" % self.name\n        source += \"ESX_VI__TEMPLATE__ALLOC(%s)\\n\\n\" % self.name\n\n        # free\n        if self.extended_by is None:\n            source += \"/* esxVI_%s_Free */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__FREE(%s,\\n\" % self.name\n            source += \"{\\n\"\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                if self.extends is not None:\n                    # avoid \"dereferencing type-punned pointer will break\n                    # strict-aliasing rules\" warnings\n                    source += \"    esxVI_%s *next = (esxVI_%s *)item-\u003e_next;\\n\\n\" \\\n                              % (self.extends, self.extends)\n                    source += \"    esxVI_%s_Free(\u0026next);\\n\" % self.extends\n                    source += \"    item-\u003e_next = (esxVI_%s *)next;\\n\\n\" % self.name\n                else:\n                    source += \"    esxVI_%s_Free(\u0026item-\u003e_next);\\n\\n\" % self.name\n\n            source += self.generate_free_code()\n\n            source += \"})\\n\\n\"\n        else:\n            source += \"/* esxVI_%s_Free */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__DYNAMIC_FREE(%s,\\n\" % self.name\n            source += \"{\\n\"\n\n            for extended_by in self.extended_by:\n                source += \"    ESX_VI__TEMPLATE__DISPATCH__FREE(%s)\\n\" \\\n                          % extended_by\n\n            source += \"},\\n\"\n            source += \"{\\n\"\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                if self.extends is not None:\n                    # avoid \"dereferencing type-punned pointer will brea\n                    # strict-aliasing rules\" warnings\n                    source += \"    esxVI_%s *next = (esxVI_%s *)item-\u003e_next;\\n\\n\" \\\n                              % (self.extends, self.extends)\n                    source += \"    esxVI_%s_Free(\u0026next);\\n\" % self.extends\n                    source += \"    item-\u003e_next = (esxVI_%s *)next;\\n\\n\" % self.name\n                else:\n                    source += \"    esxVI_%s_Free(\u0026item-\u003e_next);\\n\\n\" % self.name\n\n            source += self.generate_free_code()\n\n            source += \"})\\n\\n\"\n\n        # validate\n        source += \"/* esxVI_%s_Validate */\\n\" % self.name\n        source += \"ESX_VI__TEMPLATE__VALIDATE(%s,\\n\" % self.name\n        source += \"{\\n\"\n\n        source += self.generate_validate_code()\n\n        source += \"})\\n\\n\"\n\n        # dynamic cast\n        if self.features \u0026 Object.FEATURE__DYNAMIC_CAST:\n            if self.extended_by is not None or self.extends is not None:\n                source += \"/* esxVI_%s_DynamicCast */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__DYNAMIC_CAST(%s,\\n\" % self.name\n                source += \"{\\n\"\n\n                source += self.generate_dynamic_cast_code()\n\n                source += \"})\\n\\n\"\n            else:\n                report_error(\"cannot add dynamic cast support for an untyped object\")\n\n        # append to list\n        if self.features \u0026 Object.FEATURE__LIST:\n            source += \"/* esxVI_%s_AppendToList */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__LIST__APPEND(%s)\\n\\n\" % self.name\n\n        # deep copy\n        if self.extended_by is None:\n            if self.features \u0026 Object.FEATURE__DEEP_COPY:\n                source += \"/* esxVI_%s_DeepCopy */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__DEEP_COPY(%s,\\n\" % self.name\n                source += \"{\\n\"\n\n                source += self.generate_deep_copy_code()\n\n                source += \"})\\n\\n\"\n\n                if self.features \u0026 Object.FEATURE__LIST:\n                    source += \"/* esxVI_%s_DeepCopyList */\\n\" % self.name\n                    source += \"ESX_VI__TEMPLATE__LIST__DEEP_COPY(%s)\\n\\n\" \\\n                              % self.name\n        else:\n            if self.features \u0026 Object.FEATURE__DEEP_COPY:\n                source += \"/* esxVI_%s_DeepCopy */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__DYNAMIC_DEEP_COPY(%s)\\n\" % self.name\n                source += \"{\\n\"\n\n                for extended_by in self.extended_by:\n                    source += \"    ESX_VI__TEMPLATE__DISPATCH__DEEP_COPY(%s)\\n\" \\\n                              % extended_by\n\n                source += \"},\\n\"\n                source += \"{\\n\"\n\n                source += self.generate_deep_copy_code()\n\n                source += \"})\\n\\n\"\n\n                if self.features \u0026 Object.FEATURE__LIST:\n                    source += \"/* esxVI_%s_DeepCopyList */\\n\" % self.name\n                    source += \"ESX_VI__TEMPLATE__LIST__DEEP_COPY(%s)\\n\\n\" \\\n                              % self.name\n\n        # cast from any type\n        if self.features \u0026 Object.FEATURE__ANY_TYPE:\n            source += \"/* esxVI_%s_CastFromAnyType */\\n\" % self.name\n\n            if self.extended_by is None:\n                source += \"ESX_VI__TEMPLATE__CAST_FROM_ANY_TYPE(%s)\\n\\n\" \\\n                          % self.name\n            else:\n                source += \"ESX_VI__TEMPLATE__DYNAMIC_CAST_FROM_ANY_TYPE(%s,\\n\" \\\n                          % self.name\n                source += \"{\\n\"\n\n                for extended_by in self.extended_by:\n                    source += \"    ESX_VI__TEMPLATE__DISPATCH__CAST_FROM_ANY_TYPE(%s)\\n\" \\\n                              % extended_by\n\n                source += \"})\\n\\n\"\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                source += \"/* esxVI_%s_CastListFromAnyType */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__LIST__CAST_FROM_ANY_TYPE(%s)\\n\\n\" \\\n                          % self.name\n\n        # serialize\n        if self.extended_by is None:\n            if self.features \u0026 Object.FEATURE__SERIALIZE:\n                source += \"/* esxVI_%s_Serialize */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__SERIALIZE(%s,\\n\" % self.name\n                source += \"{\\n\"\n\n                source += self.generate_serialize_code()\n\n                source += \"})\\n\\n\"\n\n                if self.features \u0026 Object.FEATURE__LIST:\n                    source += \"/* esxVI_%s_SerializeList */\\n\" % self.name\n                    source += \"ESX_VI__TEMPLATE__LIST__SERIALIZE(%s)\\n\\n\" \\\n                              % self.name\n        else:\n            if self.features \u0026 Object.FEATURE__SERIALIZE:\n                source += \"/* esxVI_%s_Serialize */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__DYNAMIC_SERIALIZE(%s,\\n\" % self.name\n                source += \"{\\n\"\n\n                for extended_by in self.extended_by:\n                    source += \"    ESX_VI__TEMPLATE__DISPATCH__SERIALIZE(%s)\\n\" \\\n                              % extended_by\n\n                source += \"},\\n\"\n                source += \"{\\n\"\n\n                source += self.generate_serialize_code()\n\n                source += \"})\\n\\n\"\n\n                if self.features \u0026 Object.FEATURE__LIST:\n                    source += \"/* esxVI_%s_SerializeList */\\n\" % self.name\n                    source += \"ESX_VI__TEMPLATE__LIST__SERIALIZE(%s)\\n\\n\" \\\n                              % self.name\n\n        # deserialize\n        if self.extended_by is None:\n            if self.features \u0026 Object.FEATURE__DESERIALIZE:\n                source += \"/* esxVI_%s_Deserialize */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__DESERIALIZE(%s,\\n\" % self.name\n                source += \"{\\n\"\n\n                source += self.generate_deserialize_code()\n\n                source += \"})\\n\\n\"\n\n                if self.features \u0026 Object.FEATURE__LIST:\n                    source += \"/* esxVI_%s_DeserializeList */\\n\" % self.name\n                    source += \"ESX_VI__TEMPLATE__LIST__DESERIALIZE(%s)\\n\\n\" \\\n                              % self.name\n        else:\n            if self.features \u0026 Object.FEATURE__DESERIALIZE:\n                source += \"/* esxVI_%s_Deserialize */\\n\" % self.name\n                source += \"ESX_VI__TEMPLATE__DYNAMIC_DESERIALIZE(%s,\\n\" \\\n                          % self.name\n                source += \"{\\n\"\n\n                for extended_by in self.extended_by:\n                    source += \"    ESX_VI__TEMPLATE__DISPATCH__DESERIALIZE(%s)\\n\" \\\n                              % extended_by\n\n                source += \"},\\n\"\n                source += \"{\\n\"\n\n                source += self.generate_deserialize_code()\n\n                source += \"})\\n\\n\"\n\n                if self.features \u0026 Object.FEATURE__LIST:\n                    source += \"/* esxVI_%s_DeserializeList */\\n\" % self.name\n                    source += \"ESX_VI__TEMPLATE__LIST__DESERIALIZE(%s)\\n\\n\" \\\n                              % self.name\n\n        source += \"\\n\\n\"\n\n        return source\n\n\n\nclass ManagedObject(Type):\n    FEATURE__LIST = (1 \u003c\u003c 2)\n\n\n    def __init__(self, name, extends, properties, features=0, extended_by=None):\n        Type.__init__(self, \"struct\", name)\n        self.extends = extends\n        self.features = features\n        self.properties = properties\n        self.extended_by = extended_by\n\n        if self.extended_by is not None:\n            self.extended_by.sort()\n\n\n    def generate_struct_members(self, add_banner=False, struct_gap=False):\n        members = \"\"\n\n        if struct_gap:\n            members += \"\\n\"\n\n        if self.extends is not None:\n            members += managed_objects_by_name[self.extends] \\\n                       .generate_struct_members(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            members += \"    /* %s */\\n\" % self.name\n\n        for property in self.properties:\n            members += property.generate_struct_member()\n\n        if len(self.properties) \u003c 1:\n            members += \"    /* no properties */\\n\"\n\n        return members\n\n\n    def generate_free_code(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += managed_objects_by_name[self.extends] \\\n                      .generate_free_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += property.generate_free_code()\n\n            if len(string) \u003c 1:\n                source += \"    /* no properties to be freed */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_validate_code(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += managed_objects_by_name[self.extends] \\\n                      .generate_validate_code(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += property.generate_validate_code(managed=True)\n\n            if len(string) \u003c 1:\n                source += \"    /* no required properties */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_lookup_code1(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += managed_objects_by_name[self.extends] \\\n                      .generate_lookup_code1(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += \"    \\\"%s\\\\0\\\"\\n\" % property.name\n\n            if len(string) \u003c 1:\n                source += \"    /* no properties */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_lookup_code2(self, add_banner=False):\n        source = \"\"\n\n        if self.extends is not None:\n            source += managed_objects_by_name[self.extends] \\\n                      .generate_lookup_code2(add_banner=True) + \"\\n\"\n\n        if self.extends is not None or add_banner:\n            source += \"    /* %s */\\n\" % self.name\n\n        if len(self.properties) \u003c 1:\n            source += \"    /* no properties */\\n\"\n        else:\n            string = \"\"\n\n            for property in self.properties:\n                string += property.generate_lookup_code()\n\n            if len(string) \u003c 1:\n                source += \"    /* no properties */\\n\"\n            else:\n                source += string\n\n        return source\n\n\n    def generate_comment(self):\n        comment = separator\n        comment += \" * VI Managed Object: %s\\n\" % self.name\n\n        if self.extends is not None:\n            comment += \" *                    extends %s\\n\" % self.extends\n\n        first = True\n\n        if self.extended_by is not None:\n            for extended_by in self.extended_by:\n                if first:\n                    comment += \" *                    extended by %s\\n\" \\\n                               % extended_by\n                    first = False\n                else:\n                    comment += \" *                                %s\\n\" \\\n                               % extended_by\n\n        comment += \" */\\n\\n\"\n\n        return comment\n\n\n    def generate_header(self):\n        header = self.generate_comment()\n\n        # struct\n        header += \"struct _esxVI_%s {\\n\" % self.name\n\n        if self.features \u0026 Object.FEATURE__LIST:\n            header += aligned(\"    esxVI_%s *_next; \" % self.name,\n                              \"/* optional */\\n\")\n        else:\n            header += aligned(\"    esxVI_%s *_unused; \" % self.name,\n                              \"/* optional */\\n\")\n\n        header += aligned(\"    esxVI_Type _type; \", \"/* required */\\n\")\n        header += aligned(\"    esxVI_ManagedObjectReference *_reference; \",\n                          \"/* required */\\n\")\n        header += \"\\n\"\n        header += self.generate_struct_members()\n\n        header += \"};\\n\\n\"\n\n        # functions\n        header += \"int esxVI_%s_Alloc(esxVI_%s **item);\\n\" % (self.name, self.name)\n        header += \"void esxVI_%s_Free(esxVI_%s **item);\\n\" % (self.name, self.name)\n        header += (\"int esxVI_%s_Validate(esxVI_%s *item, \"\n                                         \"esxVI_String *selectedPropertyNameList);\\n\") \\\n                  % (self.name, self.name)\n\n        if self.features \u0026 Object.FEATURE__LIST:\n            header += \"int esxVI_%s_AppendToList(esxVI_%s **list, esxVI_%s *item);\\n\" \\\n                      % (self.name, self.name, self.name)\n\n        header += \"\\n\\n\\n\"\n\n        return header\n\n\n    def generate_helper_header(self):\n        header = \"\"\n\n        # functions\n        header += (\"int esxVI_Lookup%s(esxVI_Context *ctx, \"\n                                      \"const char *name, \"\n                                      \"esxVI_ManagedObjectReference *root, \"\n                                      \"esxVI_String *selectedPropertyNameList, \"\n                                      \"esxVI_%s **item, \"\n                                      \"esxVI_Occurrence occurrence);\\n\") \\\n                  % (self.name, self.name)\n\n        header += \"\\n\"\n\n        return header\n\n\n    def generate_source(self):\n        source = self.generate_comment()\n\n        # functions\n        source += \"/* esxVI_%s_Alloc */\\n\" % self.name\n        source += \"ESX_VI__TEMPLATE__ALLOC(%s)\\n\\n\" % self.name\n\n        # free\n        if self.extended_by is None:\n            source += \"/* esxVI_%s_Free */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__FREE(%s,\\n\" % self.name\n            source += \"{\\n\"\n\n            if self.features \u0026 ManagedObject.FEATURE__LIST:\n                if self.extends is not None:\n                    # avoid \"dereferencing type-punned pointer will break\n                    # strict-aliasing rules\" warnings\n                    source += \"    esxVI_%s *next = (esxVI_%s *)item-\u003e_next;\\n\\n\" \\\n                              % (self.extends, self.extends)\n                    source += \"    esxVI_%s_Free(\u0026next);\\n\" % self.extends\n                    source += \"    item-\u003e_next = (esxVI_%s *)next;\\n\\n\" % self.name\n                else:\n                    source += \"    esxVI_%s_Free(\u0026item-\u003e_next);\\n\" % self.name\n\n            source += \"    esxVI_ManagedObjectReference_Free(\u0026item-\u003e_reference);\\n\\n\"\n\n            source += self.generate_free_code()\n\n            source += \"})\\n\\n\"\n        else:\n            source += \"/* esxVI_%s_Free */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__DYNAMIC_FREE(%s,\\n\" % self.name\n            source += \"{\\n\"\n\n            for extended_by in self.extended_by:\n                source += \"    ESX_VI__TEMPLATE__DISPATCH__FREE(%s)\\n\" % extended_by\n\n            source += \"},\\n\"\n            source += \"{\\n\"\n\n            if self.features \u0026 Object.FEATURE__LIST:\n                if self.extends is not None:\n                    # avoid \"dereferencing type-punned pointer will break\n                    # strict-aliasing rules\" warnings\n                    source += \"    esxVI_%s *next = (esxVI_%s *)item-\u003e_next;\\n\\n\" \\\n                              % (self.extends, self.extends)\n                    source += \"    esxVI_%s_Free(\u0026next);\\n\" % self.extends\n                    source += \"    item-\u003e_next = (esxVI_%s *)next;\\n\\n\" % self.name\n                else:\n                    source += \"    esxVI_%s_Free(\u0026item-\u003e_next);\\n\" % self.name\n\n            source += \"    esxVI_ManagedObjectReference_Free(\u0026item-\u003e_reference);\\n\\n\"\n\n            source += self.generate_free_code()\n\n            source += \"})\\n\\n\"\n\n        # validate\n        source += \"/* esxVI_%s_Validate */\\n\" % self.name\n        source += \"ESX_VI__TEMPLATE__MANAGED_VALIDATE(%s,\\n\" % self.name\n        source += \"{\\n\"\n\n        source += self.generate_validate_code()\n\n        source += \"})\\n\\n\"\n\n        # append to list\n        if self.features \u0026 ManagedObject.FEATURE__LIST:\n            source += \"/* esxVI_%s_AppendToList */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__LIST__APPEND(%s)\\n\\n\" % self.name\n\n        source += \"\\n\\n\"\n\n        return source\n\n\n    def generate_helper_source(self):\n        source = \"\"\n\n        # lookup\n        source += \"/* esxVI_Lookup%s */\\n\" % self.name\n        source += \"ESX_VI__TEMPLATE__LOOKUP(%s,\\n\" % self.name\n        source += \"{\\n\"\n\n        source += self.generate_lookup_code1()\n\n        source += \"},\\n\"\n        source += \"{\\n\"\n\n        source += self.generate_lookup_code2()\n\n        source += \"})\\n\\n\"\n\n        source += \"\\n\\n\"\n\n        return source\n\n\n\nclass Enum(Type):\n    FEATURE__ANY_TYPE = (1 \u003c\u003c 1)\n    FEATURE__SERIALIZE = (1 \u003c\u003c 2)\n    FEATURE__DESERIALIZE = (1 \u003c\u003c 3)\n\n\n    def __init__(self, name, values, features=0):\n        Type.__init__(self, \"enum\", name)\n        self.values = values\n        self.features = features\n\n\n    def generate_header(self):\n        header = separator\n        header += \" * VI Enum: %s\\n\" % self.name\n        header += \" */\\n\\n\"\n\n        # enum\n        header += \"enum _esxVI_%s {\\n\" % self.name\n        header += \"    esxVI_%s_Undefined = 0,\\n\" % self.name\n\n        for value in self.values:\n            header += \"    esxVI_%s_%s,\\n\" % (self.name, capitalize_first(value))\n\n        header += \"};\\n\\n\"\n\n        # functions\n        if self.features \u0026 Enum.FEATURE__ANY_TYPE:\n            header += (\"int esxVI_%s_CastFromAnyType(esxVI_AnyType *anyType, \"\n                                                    \"esxVI_%s *item);\\n\") \\\n                      % (self.name, self.name)\n\n        if self.features \u0026 Enum.FEATURE__SERIALIZE:\n            header += (\"int esxVI_%s_Serialize(esxVI_%s item, const char *element, \"\n                                              \"virBufferPtr output);\\n\") \\\n                      % (self.name, self.name)\n\n        if self.features \u0026 Enum.FEATURE__DESERIALIZE:\n            header += (\"int esxVI_%s_Deserialize(xmlNodePtr node, \"\n                                                \"esxVI_%s *item);\\n\") \\\n                      % (self.name, self.name)\n\n        header += \"\\n\\n\\n\"\n\n        return header\n\n\n    def generate_source(self):\n        source = separator\n        source += \" * VI Enum: %s\\n\" % self.name\n        source += \" */\\n\\n\"\n\n        source += \"static const esxVI_Enumeration _esxVI_%s_Enumeration = {\\n\" \\\n                  % self.name\n        source += \"    esxVI_Type_%s, {\\n\" % self.name\n\n        for value in self.values:\n            source += \"        { \\\"%s\\\", esxVI_%s_%s },\\n\" \\\n                      % (value, self.name, capitalize_first(value))\n\n        source += \"        { NULL, -1 },\\n\"\n        source += \"    },\\n\"\n        source += \"};\\n\\n\"\n\n        # functions\n        if self.features \u0026 Enum.FEATURE__ANY_TYPE:\n            source += \"/* esxVI_%s_CastFromAnyType */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__ENUMERATION__CAST_FROM_ANY_TYPE(%s)\\n\\n\" \\\n                      % self.name\n\n        if self.features \u0026 Enum.FEATURE__SERIALIZE:\n            source += \"/* esxVI_%s_Serialize */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__ENUMERATION__SERIALIZE(%s)\\n\\n\" \\\n                      % self.name\n\n        if self.features \u0026 Enum.FEATURE__DESERIALIZE:\n            source += \"/* esxVI_%s_Deserialize */\\n\" % self.name\n            source += \"ESX_VI__TEMPLATE__ENUMERATION__DESERIALIZE(%s)\\n\\n\" \\\n                      % self.name\n\n        source += \"\\n\\n\"\n\n        return source\n\n\n\ndef report_error(message):\n    print \"error: \" + message\n    sys.exit(1)\n\n\n\ndef capitalize_first(string):\n    return string[:1].upper() + string[1:]\n\n\n\ndef parse_object(block):\n    # expected format: [managed] object \u003cname\u003e [extends \u003cname\u003e]\n    header_items = block[0][1].split()\n    managed = False\n\n    if header_items[0] == \"managed\":\n        managed = True\n        del header_items[0]\n\n    if len(header_items) \u003c 2:\n        report_error(\"line %d: invalid block header\" % (number))\n\n    assert header_items[0] == \"object\"\n\n    name = header_items[1]\n    extends = None\n\n    if len(header_items) \u003e 2:\n        if header_items[2] != \"extends\":\n            report_error(\"line %d: invalid block header\" % (number))\n        else:\n            extends = header_items[3]\n\n    properties = []\n\n    for line in block[1:]:\n        # expected format: \u003ctype\u003e \u003cname\u003e \u003coccurrence\u003e\n        items = line[1].split()\n\n        if len(items) != 3:\n            report_error(\"line %d: invalid property\" % line[0])\n\n        if items[2] not in valid_occurrences:\n            report_error(\"line %d: invalid occurrence\" % line[0])\n\n        properties.append(Property(type=items[0], name=items[1],\n                                   occurrence=items[2]))\n\n    if managed:\n        return ManagedObject(name=name, extends=extends, properties=properties)\n    else:\n        return Object(name=name, extends=extends, properties=properties)\n\n\n\ndef parse_enum(block):\n    # expected format: enum \u003cname\u003e\n    header_items = block[0][1].split()\n\n    if len(header_items) \u003c 2:\n        report_error(\"line %d: invalid block header\" % (number))\n\n    assert header_items[0] == \"enum\"\n\n    name = header_items[1]\n\n    values = []\n\n    for line in block[1:]:\n        # expected format: \u003cvalue\u003e\n        values.append(line[1])\n\n    return Enum(name=name, values=values)\n\n\n\ndef parse_method(block):\n    # expected format: method \u003cname\u003e [returns \u003ctype\u003e \u003coccurrence\u003e]\n    header_items = block[0][1].split()\n\n    if len(header_items) \u003c 2:\n        report_error(\"line %d: invalid block header\" % (number))\n\n    assert header_items[0] == \"method\"\n\n    name = header_items[1]\n    returns = None\n\n    if len(header_items) \u003e 2:\n        if header_items[2] != \"returns\":\n            report_error(\"line %d: invalid block header\" % (number))\n        else:\n            returns = Parameter(type=header_items[3], name=\"output\",\n                                occurrence=header_items[4])\n\n    parameters = []\n\n    for line in block[1:]:\n        # expected format: \u003ctype\u003e \u003cname\u003e \u003coccurrence\u003e\n        items = line[1].split()\n\n        if len(items) != 3:\n            report_error(\"line %d: invalid property\" % line[0])\n\n        if items[2] not in valid_occurrences:\n            report_error(\"line %d: invalid occurrence\" % line[0])\n\n        parameters.append(Parameter(type=items[0], name=items[1],\n                                    occurrence=items[2]))\n\n    return Method(name=name, parameters=parameters, returns=returns)\n\n\n\ndef is_known_type(type):\n    return type in predefined_objects or \\\n           type in predefined_enums or \\\n           type in objects_by_name or \\\n           type in managed_objects_by_name or \\\n           type in enums_by_name\n\n\n\ndef open_and_print(filename):\n    if filename.startswith(\"./\"):\n        print \"  GEN    \" + filename[2:]\n    else:\n        print \"  GEN    \" + filename\n\n    return open(filename, \"wb\")\n\n\n\npredefined_enums = [\"Boolean\"]\n\npredefined_objects = [\"AnyType\",\n                      \"Int\",\n                      \"Long\",\n                      \"String\",\n                      \"DateTime\",\n                      \"MethodFault\",\n                      \"ManagedObjectReference\"]\n\nadditional_enum_features = { \"ManagedEntityStatus\"      : Enum.FEATURE__ANY_TYPE,\n                             \"TaskInfoState\"            : Enum.FEATURE__ANY_TYPE,\n                             \"VirtualMachinePowerState\" : Enum.FEATURE__ANY_TYPE }\n\nadditional_object_features = { \"AutoStartDefaults\"          : Object.FEATURE__ANY_TYPE,\n                               \"AutoStartPowerInfo\"         : Object.FEATURE__ANY_TYPE,\n                               \"DatastoreHostMount\"         : Object.FEATURE__DEEP_COPY |\n                                                              Object.FEATURE__LIST |\n                                                              Object.FEATURE__ANY_TYPE,\n                               \"DatastoreInfo\"              : Object.FEATURE__ANY_TYPE |\n                                                              Object.FEATURE__DYNAMIC_CAST,\n                               \"HostConfigManager\"          : Object.FEATURE__ANY_TYPE,\n                               \"HostCpuIdInfo\"              : Object.FEATURE__LIST |\n                                                              Object.FEATURE__ANY_TYPE,\n                               \"HostDatastoreBrowserSearchResults\" : Object.FEATURE__LIST |\n                                                              Object.FEATURE__ANY_TYPE,\n                               \"ManagedObjectReference\"     : Object.FEATURE__ANY_TYPE,\n                               \"ObjectContent\"              : Object.FEATURE__DEEP_COPY,\n                               \"ResourcePoolResourceUsage\"  : Object.FEATURE__ANY_TYPE,\n                               \"ServiceContent\"             : Object.FEATURE__DESERIALIZE,\n                               \"SharesInfo\"                 : Object.FEATURE__ANY_TYPE,\n                               \"TaskInfo\"                   : Object.FEATURE__LIST |\n                                                              Object.FEATURE__ANY_TYPE,\n                               \"UserSession\"                : Object.FEATURE__ANY_TYPE,\n                               \"VirtualMachineQuestionInfo\" : Object.FEATURE__ANY_TYPE,\n                               \"VirtualMachineSnapshotTree\" : Object.FEATURE__DEEP_COPY |\n                                                              Object.FEATURE__ANY_TYPE,\n                               \"VmEventArgument\"            : Object.FEATURE__DESERIALIZE }\n\nremoved_object_features = {}\n\n\n\nif \"srcdir\" in os.environ:\n    input_filename = os.path.join(os.environ[\"srcdir\"], \"esx/esx_vi_generator.input\")\n    output_dirname = os.path.join(os.environ[\"srcdir\"], \"esx\")\nelse:\n    input_filename = os.path.join(os.getcwd(), \"esx_vi_generator.input\")\n    output_dirname = os.getcwd()\n\n\n\ntypes_typedef = open_and_print(os.path.join(output_dirname, \"esx_vi_types.generated.typedef\"))\ntypes_typeenum = open_and_print(os.path.join(output_dirname, \"esx_vi_types.generated.typeenum\"))\ntypes_typetostring = open_and_print(os.path.join(output_dirname, \"esx_vi_types.generated.typetostring\"))\ntypes_typefromstring = open_and_print(os.path.join(output_dirname, \"esx_vi_types.generated.typefromstring\"))\ntypes_header = open_and_print(os.path.join(output_dirname, \"esx_vi_types.generated.h\"))\ntypes_source = open_and_print(os.path.join(output_dirname, \"esx_vi_types.generated.c\"))\nmethods_header = open_and_print(os.path.join(output_dirname, \"esx_vi_methods.generated.h\"))\nmethods_source = open_and_print(os.path.join(output_dirname, \"esx_vi_methods.generated.c\"))\nmethods_macro = open_and_print(os.path.join(output_dirname, \"esx_vi_methods.generated.macro\"))\nhelpers_header = open_and_print(os.path.join(output_dirname, \"esx_vi.generated.h\"))\nhelpers_source = open_and_print(os.path.join(output_dirname, \"esx_vi.generated.c\"))\n\n\n\nnumber = 0\nobjects_by_name = {}\nmanaged_objects_by_name = {}\nenums_by_name = {}\nmethods_by_name = {}\nblock = None\n\n\n\n# parse input file\nfor line in file(input_filename, \"rb\").readlines():\n    number += 1\n\n    if \"#\" in line:\n        line = line[:line.index(\"#\")]\n\n    line = line.lstrip().rstrip()\n\n    if len(line) \u003c 1:\n        continue\n\n    if line.startswith(\"object\") or line.startswith(\"managed object\") or \\\n       line.startswith(\"enum\") or line.startswith(\"method\"):\n        if block is not None:\n            report_error(\"line %d: nested block found\" % (number))\n        else:\n            block = []\n\n    if block is not None:\n        if line == \"end\":\n            if block[0][1].startswith(\"object\"):\n                obj = parse_object(block)\n                objects_by_name[obj.name] = obj\n            elif block[0][1].startswith(\"managed object\"):\n                obj = parse_object(block)\n                managed_objects_by_name[obj.name] = obj\n            elif block[0][1].startswith(\"enum\"):\n                enum = parse_enum(block)\n                enums_by_name[enum.name] = enum\n            else:\n                method = parse_method(block)\n                methods_by_name[method.name] = method\n\n            block = None\n        else:\n            block.append((number, line))\n\n\n\nfor method in methods_by_name.values():\n    # method parameter types must be serializable\n    for parameter in method.parameters:\n        if not parameter.is_type_generated():\n            continue\n\n        if parameter.is_enum():\n            enums_by_name[parameter.type].features |= Enum.FEATURE__SERIALIZE\n        else:\n            objects_by_name[parameter.type].features |= Object.FEATURE__SERIALIZE\n            objects_by_name[parameter.type].candidate_for_dynamic_cast = True\n\n        # detect list usage\n        if parameter.occurrence == OCCURRENCE__REQUIRED_LIST or \\\n           parameter.occurrence == OCCURRENCE__OPTIONAL_LIST:\n            if parameter.is_enum():\n                report_error(\"unsupported usage of enum '%s' as list in '%s'\"\n                             % (parameter.type, method.name))\n            else:\n                objects_by_name[parameter.type].features |= Object.FEATURE__LIST\n\n    # method return types must be deserializable\n    if method.returns and method.returns.is_type_generated():\n        if method.returns.is_enum():\n            enums_by_name[method.returns.type].features |= Enum.FEATURE__DESERIALIZE\n        else:\n            objects_by_name[method.returns.type].features |= Object.FEATURE__DESERIALIZE\n            objects_by_name[method.returns.type].candidate_for_dynamic_cast = True\n\n        # detect list usage\n        if method.returns.occurrence == OCCURRENCE__REQUIRED_LIST or \\\n           method.returns.occurrence == OCCURRENCE__OPTIONAL_LIST:\n            if method.returns.is_enum():\n                report_error(\"unsupported usage of enum '%s' as list in '%s'\"\n                             % (method.returns.type, method.name))\n            else:\n                objects_by_name[method.returns.type].features |= Object.FEATURE__LIST\n\n\n\nfor enum in enums_by_name.values():\n    # apply additional features\n    if enum.name in additional_enum_features:\n        enum.features |= additional_enum_features[enum.name]\n\n        if additional_enum_features[enum.name] \u0026 Enum.FEATURE__ANY_TYPE:\n            enum.features |= Enum.FEATURE__DESERIALIZE\n\n\n\nfor obj in objects_by_name.values():\n    for property in obj.properties:\n        if property.occurrence != OCCURRENCE__IGNORED and \\\n           not is_known_type(property.type):\n            report_error(\"object '%s' contains unknown property type '%s'\"\n                         % (obj.name, property.type))\n\n    if obj.extends is not None:\n        if not is_known_type(obj.extends):\n            report_error(\"object '%s' extends unknown object '%s'\"\n                         % (obj.name, obj.extends))\n\n    for property in obj.properties:\n        if not property.is_type_generated():\n            continue\n\n        if property.is_enum():\n            enums_by_name[property.type].candidate_for_dynamic_cast = True\n        else:\n            objects_by_name[property.type].candidate_for_dynamic_cast = True\n\n        # detect list usage\n        if property.occurrence == OCCURRENCE__REQUIRED_LIST or \\\n           property.occurrence == OCCURRENCE__OPTIONAL_LIST:\n            if property.is_enum():\n                report_error(\"unsupported usage of enum '%s' as list in '%s'\"\n                             % (property.type, obj.type))\n            else:\n                objects_by_name[property.type].features |= Object.FEATURE__LIST\n\n    # apply/remove additional features\n    if obj.name in additional_object_features:\n        obj.features |= additional_object_features[obj.name]\n\n        if additional_object_features[obj.name] \u0026 Object.FEATURE__ANY_TYPE:\n            obj.features |= Object.FEATURE__DESERIALIZE\n\n    if obj.name in removed_object_features:\n        obj.features \u0026= ~removed_object_features[obj.name]\n\n    # detect extended_by relation\n    if obj.extends is not None:\n        extended_obj = objects_by_name[obj.extends]\n\n        if extended_obj.extended_by is None:\n            extended_obj.extended_by = [obj.name]\n        else:\n            extended_obj.extended_by.append(obj.name)\n            extended_obj.extended_by.sort()\n\n\n\nfor obj in objects_by_name.values():\n    # if an object is a candidate (it is used directly as parameter or return\n    # type or is a member of another object) and it is extended by another\n    # object then this type needs the dynamic cast feature\n    if obj.candidate_for_dynamic_cast and obj.extended_by:\n        obj.features |= Object.FEATURE__DYNAMIC_CAST\n\n\n\ndef propagate_feature(obj, feature):\n    global features_have_changed\n\n    if not (obj.features \u0026 feature):\n        return\n\n    for property in obj.properties:\n        if property.occurrence == OCCURRENCE__IGNORED or \\\n           not property.is_type_generated():\n            continue\n\n        if property.is_enum():\n            if feature == Object.FEATURE__SERIALIZE and \\\n               not (enums_by_name[property.type].features \u0026 Enum.FEATURE__SERIALIZE):\n                enums_by_name[property.type].features |= Enum.FEATURE__SERIALIZE\n                features_have_changed = True\n            elif feature == Object.FEATURE__DESERIALIZE and \\\n               not (enums_by_name[property.type].features \u0026 Enum.FEATURE__DESERIALIZE):\n                enums_by_name[property.type].features |= Enum.FEATURE__DESERIALIZE\n                features_have_changed = True\n        elif property.is_object():\n            if not (objects_by_name[property.type].features \u0026 feature):\n                objects_by_name[property.type].features |= feature\n                features_have_changed = True\n\n            if obj.name != property.type:\n                propagate_feature(objects_by_name[property.type], feature)\n\n\n\ndef inherit_features(obj):\n    global features_have_changed\n\n    if obj.extended_by is not None:\n        for extended_by in obj.extended_by:\n            previous = objects_by_name[extended_by].features\n            objects_by_name[extended_by].features |= obj.features\n\n            if objects_by_name[extended_by].features != previous:\n                features_have_changed = True\n\n    if obj.extends is not None:\n        previous = objects_by_name[obj.extends].features\n        objects_by_name[obj.extends].features |= obj.features\n\n        if objects_by_name[obj.extends].features != previous:\n            features_have_changed = True\n\n    if obj.extended_by is not None:\n        for extended_by in obj.extended_by:\n            inherit_features(objects_by_name[extended_by])\n\n\n\n# there are two directions to spread features:\n# 1) up and down the inheritance chain\n# 2) from object types to their member property types\n# spreading needs to be done alternating on both directions because they can\n# affect each other\nfeatures_have_changed = True\n\nwhile features_have_changed:\n    features_have_changed = False\n\n    for obj in objects_by_name.values():\n        propagate_feature(obj, Object.FEATURE__DEEP_COPY)\n        propagate_feature(obj, Object.FEATURE__SERIALIZE)\n        propagate_feature(obj, Object.FEATURE__DESERIALIZE)\n\n    for obj in objects_by_name.values():\n        inherit_features(obj)\n\n\n\nfor obj in managed_objects_by_name.values():\n    for property in obj.properties:\n        if property.occurrence != OCCURRENCE__IGNORED and \\\n           not is_known_type(property.type):\n            report_error(\"object '%s' contains unknown property type '%s'\"\n                         % (obj.name, property.type))\n\n    if obj.extends is not None:\n        if not is_known_type(obj.extends):\n            report_error(\"object '%s' extends unknown object '%s'\"\n                         % (obj.name, obj.extends))\n\n    # detect extended_by relation\n    if obj.extends is not None:\n        extended_obj = managed_objects_by_name[obj.extends]\n\n        if extended_obj.extended_by is None:\n            extended_obj.extended_by = [obj.name]\n        else:\n            extended_obj.extended_by.append(obj.name)\n            extended_obj.extended_by.sort()\n\n\n\nnotice = \"/* Generated by esx_vi_generator.py */\\n\\n\\n\\n\"\n\ntypes_typedef.write(notice)\ntypes_typeenum.write(notice)\ntypes_typetostring.write(notice)\ntypes_typefromstring.write(notice)\ntypes_header.write(notice)\ntypes_source.write(notice)\nmethods_header.write(notice)\nmethods_source.write(notice)\nmethods_macro.write(notice)\nhelpers_header.write(notice)\nhelpers_source.write(notice)\n\n\n\n# output enums\ntypes_typedef.write(separator +\n                    \" * VI Enums\\n\" +\n                    \" */\\n\\n\")\n\nnames = enums_by_name.keys()\nnames.sort()\n\nfor name in names:\n    types_typedef.write(enums_by_name[name].generate_typedef())\n    types_typeenum.write(enums_by_name[name].generate_typeenum())\n    types_typetostring.write(enums_by_name[name].generate_typetostring())\n    types_typefromstring.write(enums_by_name[name].generate_typefromstring())\n    types_header.write(enums_by_name[name].generate_header())\n    types_source.write(enums_by_name[name].generate_source())\n\n\n\n# output objects\ntypes_typedef.write(\"\\n\\n\\n\" +\n                    separator +\n                    \" * VI Objects\\n\" +\n                    \" */\\n\\n\")\ntypes_typeenum.write(\"\\n\")\ntypes_typetostring.write(\"\\n\")\ntypes_typefromstring.write(\"\\n\")\n\nnames = objects_by_name.keys()\nnames.sort()\n\nfor name in names:\n    types_typedef.write(objects_by_name[name].generate_typedef())\n    types_typeenum.write(objects_by_name[name].generate_typeenum())\n    types_typetostring.write(objects_by_name[name].generate_typetostring())\n    types_typefromstring.write(objects_by_name[name].generate_typefromstring())\n    types_header.write(objects_by_name[name].generate_header())\n    types_source.write(objects_by_name[name].generate_source())\n\n\n\n# output managed objects\ntypes_typedef.write(\"\\n\\n\\n\" +\n                    separator +\n                    \" * VI Managed Objects\\n\" +\n                    \" */\\n\\n\")\ntypes_typeenum.write(\"\\n\")\ntypes_typetostring.write(\"\\n\")\ntypes_typefromstring.write(\"\\n\")\n\nnames = managed_objects_by_name.keys()\nnames.sort()\n\nfor name in names:\n    types_typedef.write(managed_objects_by_name[name].generate_typedef())\n    types_typeenum.write(managed_objects_by_name[name].generate_typeenum())\n    types_typetostring.write(managed_objects_by_name[name].generate_typetostring())\n    types_typefromstring.write(managed_objects_by_name[name].generate_typefromstring())\n    types_header.write(managed_objects_by_name[name].generate_header())\n    types_source.write(managed_objects_by_name[name].generate_source())\n\n\n\n# output methods\nnames = methods_by_name.keys()\nnames.sort()\n\nfor name in names:\n    methods_header.write(methods_by_name[name].generate_header())\n    methods_source.write(methods_by_name[name].generate_source())\n\nnames = list(autobind_names)\nnames.sort()\n\nfor name in names:\n    string = aligned(\"#define ESX_VI__METHOD__PARAMETER__THIS__%s \" % name, \"\\\\\\n\", 78)\n    string += \"    ESX_VI__METHOD__PARAMETER__THIS_FROM_SERVICE(ManagedObjectReference,      \\\\\\n\"\n    string += aligned(\"\", \"%s)\\n\\n\\n\\n\" % name, 49)\n\n    methods_macro.write(string)\n\n\n\n# output helpers\nnames = managed_objects_by_name.keys()\nnames.sort()\n\nfor name in names:\n    helpers_header.write(managed_objects_by_name[name].generate_helper_header())\n    helpers_source.write(managed_objects_by_name[name].generate_helper_source())\n"}
{"repo_name":"patrickstocklin/chattR","ref":"refs/heads/master","path":"lib/python2.7/site-packages/pip/index.py","content":"\"\"\"Routines related to PyPI, indexes\"\"\"\nfrom __future__ import absolute_import\n\nimport logging\nimport cgi\nfrom collections import namedtuple\nimport itertools\nimport sys\nimport os\nimport re\nimport mimetypes\nimport posixpath\nimport warnings\n\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\nfrom pip._vendor.six.moves.urllib import request as urllib_request\n\nfrom pip.compat import ipaddress\nfrom pip.utils import (\n    Inf, cached_property, normalize_name, splitext, normalize_path,\n    ARCHIVE_EXTENSIONS, SUPPORTED_EXTENSIONS)\nfrom pip.utils.deprecation import RemovedInPip8Warning\nfrom pip.utils.logging import indent_log\nfrom pip.exceptions import (\n    DistributionNotFound, BestVersionAlreadyInstalled, InvalidWheelFilename,\n    UnsupportedWheel,\n)\nfrom pip.download import HAS_TLS, url_to_path, path_to_url\nfrom pip.models import PyPI\nfrom pip.wheel import Wheel, wheel_ext\nfrom pip.pep425tags import supported_tags, supported_tags_noarch, get_platform\nfrom pip._vendor import html5lib, requests, pkg_resources, six\nfrom pip._vendor.packaging.version import parse as parse_version\nfrom pip._vendor.requests.exceptions import SSLError\n\n\n__all__ = ['FormatControl', 'fmt_ctl_handle_mutual_exclude', 'PackageFinder']\n\n\n# Taken from Chrome's list of secure origins (See: http://bit.ly/1qrySKC)\nSECURE_ORIGINS = [\n    # protocol, hostname, port\n    (\"https\", \"*\", \"*\"),\n    (\"*\", \"localhost\", \"*\"),\n    (\"*\", \"127.0.0.0/8\", \"*\"),\n    (\"*\", \"::1/128\", \"*\"),\n    (\"file\", \"*\", None),\n]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass InstallationCandidate(object):\n\n    def __init__(self, project, version, location):\n        self.project = project\n        self.version = parse_version(version)\n        self.location = location\n        self._key = (self.project, self.version, self.location)\n\n    def __repr__(self):\n        return \"\u003cInstallationCandidate({0!r}, {1!r}, {2!r})\u003e\".format(\n            self.project, self.version, self.location,\n        )\n\n    def __hash__(self):\n        return hash(self._key)\n\n    def __lt__(self, other):\n        return self._compare(other, lambda s, o: s \u003c o)\n\n    def __le__(self, other):\n        return self._compare(other, lambda s, o: s \u003c= o)\n\n    def __eq__(self, other):\n        return self._compare(other, lambda s, o: s == o)\n\n    def __ge__(self, other):\n        return self._compare(other, lambda s, o: s \u003e= o)\n\n    def __gt__(self, other):\n        return self._compare(other, lambda s, o: s \u003e o)\n\n    def __ne__(self, other):\n        return self._compare(other, lambda s, o: s != o)\n\n    def _compare(self, other, method):\n        if not isinstance(other, InstallationCandidate):\n            return NotImplemented\n\n        return method(self._key, other._key)\n\n\nclass PackageFinder(object):\n    \"\"\"This finds packages.\n\n    This is meant to match easy_install's technique for looking for\n    packages, by reading pages and looking for appropriate links.\n    \"\"\"\n\n    def __init__(self, find_links, index_urls,\n                 allow_external=(), allow_unverified=(),\n                 allow_all_external=False, allow_all_prereleases=False,\n                 trusted_hosts=None, process_dependency_links=False,\n                 session=None, format_control=None):\n        \"\"\"Create a PackageFinder.\n\n        :param format_control: A FormatControl object or None. Used to control\n            the selection of source packages / binary packages when consulting\n            the index and links.\n        \"\"\"\n        if session is None:\n            raise TypeError(\n                \"PackageFinder() missing 1 required keyword argument: \"\n                \"'session'\"\n            )\n\n        # Build find_links. If an argument starts with ~, it may be\n        # a local file relative to a home directory. So try normalizing\n        # it and if it exists, use the normalized version.\n        # This is deliberately conservative - it might be fine just to\n        # blindly normalize anything starting with a ~...\n        self.find_links = []\n        for link in find_links:\n            if link.startswith('~'):\n                new_link = normalize_path(link)\n                if os.path.exists(new_link):\n                    link = new_link\n            self.find_links.append(link)\n\n        self.index_urls = index_urls\n        self.dependency_links = []\n\n        # These are boring links that have already been logged somehow:\n        self.logged_links = set()\n\n        self.format_control = format_control or FormatControl(set(), set())\n\n        # Do we allow (safe and verifiable) externally hosted files?\n        self.allow_external = set(normalize_name(n) for n in allow_external)\n\n        # Which names are allowed to install insecure and unverifiable files?\n        self.allow_unverified = set(\n            normalize_name(n) for n in allow_unverified\n        )\n\n        # Anything that is allowed unverified is also allowed external\n        self.allow_external |= self.allow_unverified\n\n        # Do we allow all (safe and verifiable) externally hosted files?\n        self.allow_all_external = allow_all_external\n\n        # Domains that we won't emit warnings for when not using HTTPS\n        self.secure_origins = [\n            (\"*\", host, \"*\")\n            for host in (trusted_hosts if trusted_hosts else [])\n        ]\n\n        # Stores if we ignored any external links so that we can instruct\n        #   end users how to install them if no distributions are available\n        self.need_warn_external = False\n\n        # Stores if we ignored any unsafe links so that we can instruct\n        #   end users how to install them if no distributions are available\n        self.need_warn_unverified = False\n\n        # Do we want to allow _all_ pre-releases?\n        self.allow_all_prereleases = allow_all_prereleases\n\n        # Do we process dependency links?\n        self.process_dependency_links = process_dependency_links\n\n        # The Session we'll use to make requests\n        self.session = session\n\n        # If we don't have TLS enabled, then WARN if anyplace we're looking\n        # relies on TLS.\n        if not HAS_TLS:\n            for link in itertools.chain(self.index_urls, self.find_links):\n                parsed = urllib_parse.urlparse(link)\n                if parsed.scheme == \"https\":\n                    logger.warning(\n                        \"pip is configured with locations that require \"\n                        \"TLS/SSL, however the ssl module in Python is not \"\n                        \"available.\"\n                    )\n                    break\n\n    def add_dependency_links(self, links):\n        # # FIXME: this shouldn't be global list this, it should only\n        # # apply to requirements of the package that specifies the\n        # # dependency_links value\n        # # FIXME: also, we should track comes_from (i.e., use Link)\n        if self.process_dependency_links:\n            warnings.warn(\n                \"Dependency Links processing has been deprecated and will be \"\n                \"removed in a future release.\",\n                RemovedInPip8Warning,\n            )\n            self.dependency_links.extend(links)\n\n    @staticmethod\n    def _sort_locations(locations, expand_dir=False):\n        \"\"\"\n        Sort locations into \"files\" (archives) and \"urls\", and return\n        a pair of lists (files,urls)\n        \"\"\"\n        files = []\n        urls = []\n\n        # puts the url for the given file path into the appropriate list\n        def sort_path(path):\n            url = path_to_url(path)\n            if mimetypes.guess_type(url, strict=False)[0] == 'text/html':\n                urls.append(url)\n            else:\n                files.append(url)\n\n        for url in locations:\n\n            is_local_path = os.path.exists(url)\n            is_file_url = url.startswith('file:')\n\n            if is_local_path or is_file_url:\n                if is_local_path:\n                    path = url\n                else:\n                    path = url_to_path(url)\n                if os.path.isdir(path):\n                    if expand_dir:\n                        path = os.path.realpath(path)\n                        for item in os.listdir(path):\n                            sort_path(os.path.join(path, item))\n                    elif is_file_url:\n                        urls.append(url)\n                elif os.path.isfile(path):\n                    sort_path(path)\n            else:\n                urls.append(url)\n\n        return files, urls\n\n    def _candidate_sort_key(self, candidate):\n        \"\"\"\n        Function used to generate link sort key for link tuples.\n        The greater the return value, the more preferred it is.\n        If not finding wheels, then sorted by version only.\n        If finding wheels, then the sort order is by version, then:\n          1. existing installs\n          2. wheels ordered via Wheel.support_index_min()\n          3. source archives\n        Note: it was considered to embed this logic into the Link\n              comparison operators, but then different sdist links\n              with the same version, would have to be considered equal\n        \"\"\"\n        support_num = len(supported_tags)\n        if candidate.location == INSTALLED_VERSION:\n            pri = 1\n        elif candidate.location.is_wheel:\n            # can raise InvalidWheelFilename\n            wheel = Wheel(candidate.location.filename)\n            if not wheel.supported():\n                raise UnsupportedWheel(\n                    \"%s is not a supported wheel for this platform. It \"\n                    \"can't be sorted.\" % wheel.filename\n                )\n            pri = -(wheel.support_index_min())\n        else:  # sdist\n            pri = -(support_num)\n        return (candidate.version, pri)\n\n    def _sort_versions(self, applicable_versions):\n        \"\"\"\n        Bring the latest version (and wheels) to the front, but maintain the\n        existing ordering as secondary. See the docstring for `_link_sort_key`\n        for details. This function is isolated for easier unit testing.\n        \"\"\"\n        return sorted(\n            applicable_versions,\n            key=self._candidate_sort_key,\n            reverse=True\n        )\n\n    def _validate_secure_origin(self, logger, location):\n        # Determine if this url used a secure transport mechanism\n        parsed = urllib_parse.urlparse(str(location))\n        origin = (parsed.scheme, parsed.hostname, parsed.port)\n\n        # Determine if our origin is a secure origin by looking through our\n        # hardcoded list of secure origins, as well as any additional ones\n        # configured on this PackageFinder instance.\n        for secure_origin in (SECURE_ORIGINS + self.secure_origins):\n            # Check to see if the protocol matches\n            if origin[0] != secure_origin[0] and secure_origin[0] != \"*\":\n                continue\n\n            try:\n                # We need to do this decode dance to ensure that we have a\n                # unicode object, even on Python 2.x.\n                addr = ipaddress.ip_address(\n                    origin[1]\n                    if (\n                        isinstance(origin[1], six.text_type) or\n                        origin[1] is None\n                    )\n                    else origin[1].decode(\"utf8\")\n                )\n                network = ipaddress.ip_network(\n                    secure_origin[1]\n                    if isinstance(secure_origin[1], six.text_type)\n                    else secure_origin[1].decode(\"utf8\")\n                )\n            except ValueError:\n                # We don't have both a valid address or a valid network, so\n                # we'll check this origin against hostnames.\n                if origin[1] != secure_origin[1] and secure_origin[1] != \"*\":\n                    continue\n            else:\n                # We have a valid address and network, so see if the address\n                # is contained within the network.\n                if addr not in network:\n                    continue\n\n            # Check to see if the port patches\n            if (origin[2] != secure_origin[2] and\n                    secure_origin[2] != \"*\" and\n                    secure_origin[2] is not None):\n                continue\n\n            # If we've gotten here, then this origin matches the current\n            # secure origin and we should return True\n            return True\n\n        # If we've gotten to this point, then the origin isn't secure and we\n        # will not accept it as a valid location to search. We will however\n        # log a warning that we are ignoring it.\n        logger.warning(\n            \"The repository located at %s is not a trusted or secure host and \"\n            \"is being ignored. If this repository is available via HTTPS it \"\n            \"is recommended to use HTTPS instead, otherwise you may silence \"\n            \"this warning and allow it anyways with '--trusted-host %s'.\",\n            parsed.hostname,\n            parsed.hostname,\n        )\n\n        return False\n\n    def _get_index_urls_locations(self, project_name):\n        \"\"\"Returns the locations found via self.index_urls\n\n        Checks the url_name on the main (first in the list) index and\n        use this url_name to produce all locations\n        \"\"\"\n\n        def mkurl_pypi_url(url):\n            loc = posixpath.join(url, project_url_name)\n            # For maximum compatibility with easy_install, ensure the path\n            # ends in a trailing slash.  Although this isn't in the spec\n            # (and PyPI can handle it without the slash) some other index\n            # implementations might break if they relied on easy_install's\n            # behavior.\n            if not loc.endswith('/'):\n                loc = loc + '/'\n            return loc\n\n        project_url_name = urllib_parse.quote(project_name.lower())\n\n        if self.index_urls:\n            # Check that we have the url_name correctly spelled:\n\n            # Only check main index if index URL is given\n            main_index_url = Link(\n                mkurl_pypi_url(self.index_urls[0]),\n                trusted=True,\n            )\n\n            page = self._get_page(main_index_url)\n            if page is None and PyPI.netloc not in str(main_index_url):\n                warnings.warn(\n                    \"Failed to find %r at %s. It is suggested to upgrade \"\n                    \"your index to support normalized names as the name in \"\n                    \"/simple/{name}.\" % (project_name, main_index_url),\n                    RemovedInPip8Warning,\n                )\n\n                project_url_name = self._find_url_name(\n                    Link(self.index_urls[0], trusted=True),\n                    project_url_name,\n                ) or project_url_name\n\n        if project_url_name is not None:\n            return [mkurl_pypi_url(url) for url in self.index_urls]\n        return []\n\n    def _find_all_versions(self, project_name):\n        \"\"\"Find all available versions for project_name\n\n        This checks index_urls, find_links and dependency_links\n        All versions found are returned\n\n        See _link_package_versions for details on which files are accepted\n        \"\"\"\n        index_locations = self._get_index_urls_locations(project_name)\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\n        fl_file_loc, fl_url_loc = self._sort_locations(\n            self.find_links, expand_dir=True)\n        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)\n\n        file_locations = (\n            Link(url) for url in itertools.chain(\n                index_file_loc, fl_file_loc, dep_file_loc)\n        )\n\n        # We trust every url that the user has given us whether it was given\n        #   via --index-url or --find-links\n        # We explicitly do not trust links that came from dependency_links\n        # We want to filter out any thing which does not have a secure origin.\n        url_locations = [\n            link for link in itertools.chain(\n                (Link(url, trusted=True) for url in index_url_loc),\n                (Link(url, trusted=True) for url in fl_url_loc),\n                (Link(url) for url in dep_url_loc),\n            )\n            if self._validate_secure_origin(logger, link)\n        ]\n\n        logger.debug('%d location(s) to search for versions of %s:',\n                     len(url_locations), project_name)\n\n        for location in url_locations:\n            logger.debug('* %s', location)\n\n        canonical_name = pkg_resources.safe_name(project_name).lower()\n        formats = fmt_ctl_formats(self.format_control, canonical_name)\n        search = Search(project_name.lower(), canonical_name, formats)\n        find_links_versions = self._package_versions(\n            # We trust every directly linked archive in find_links\n            (Link(url, '-f', trusted=True) for url in self.find_links),\n            search\n        )\n\n        page_versions = []\n        for page in self._get_pages(url_locations, project_name):\n            logger.debug('Analyzing links from page %s', page.url)\n            with indent_log():\n                page_versions.extend(\n                    self._package_versions(page.links, search)\n                )\n\n        dependency_versions = self._package_versions(\n            (Link(url) for url in self.dependency_links), search\n        )\n        if dependency_versions:\n            logger.debug(\n                'dependency_links found: %s',\n                ', '.join([\n                    version.location.url for version in dependency_versions\n                ])\n            )\n\n        file_versions = self._package_versions(file_locations, search)\n        if file_versions:\n            file_versions.sort(reverse=True)\n            logger.debug(\n                'Local files found: %s',\n                ', '.join([\n                    url_to_path(candidate.location.url)\n                    for candidate in file_versions\n                ])\n            )\n\n        # This is an intentional priority ordering\n        return (\n            file_versions + find_links_versions + page_versions +\n            dependency_versions\n        )\n\n    def find_requirement(self, req, upgrade):\n        \"\"\"Try to find an InstallationCandidate for req\n\n        Expects req, an InstallRequirement and upgrade, a boolean\n        Returns an InstallationCandidate or None\n        May raise DistributionNotFound or BestVersionAlreadyInstalled\n        \"\"\"\n        all_versions = self._find_all_versions(req.name)\n\n        # Filter out anything which doesn't match our specifier\n        _versions = set(\n            req.specifier.filter(\n                # We turn the version object into a str here because otherwise\n                # when we're debundled but setuptools isn't, Python will see\n                # packaging.version.Version and\n                # pkg_resources._vendor.packaging.version.Version as different\n                # types. This way we'll use a str as a common data interchange\n                # format. If we stop using the pkg_resources provided specifier\n                # and start using our own, we can drop the cast to str().\n                [str(x.version) for x in all_versions],\n                prereleases=(\n                    self.allow_all_prereleases\n                    if self.allow_all_prereleases else None\n                ),\n            )\n        )\n        applicable_versions = [\n            # Again, converting to str to deal with debundling.\n            x for x in all_versions if str(x.version) in _versions\n        ]\n\n        if req.satisfied_by is not None:\n            # Finally add our existing versions to the front of our versions.\n            applicable_versions.insert(\n                0,\n                InstallationCandidate(\n                    req.name,\n                    req.satisfied_by.version,\n                    INSTALLED_VERSION,\n                )\n            )\n            existing_applicable = True\n        else:\n            existing_applicable = False\n\n        applicable_versions = self._sort_versions(applicable_versions)\n\n        if not upgrade and existing_applicable:\n            if applicable_versions[0].location is INSTALLED_VERSION:\n                logger.debug(\n                    'Existing installed version (%s) is most up-to-date and '\n                    'satisfies requirement',\n                    req.satisfied_by.version,\n                )\n            else:\n                logger.debug(\n                    'Existing installed version (%s) satisfies requirement '\n                    '(most up-to-date version is %s)',\n                    req.satisfied_by.version,\n                    applicable_versions[0][2],\n                )\n            return None\n\n        if not applicable_versions:\n            logger.critical(\n                'Could not find a version that satisfies the requirement %s '\n                '(from versions: %s)',\n                req,\n                ', '.join(\n                    sorted(\n                        set(str(i.version) for i in all_versions),\n                        key=parse_version,\n                    )\n                )\n            )\n\n            if self.need_warn_external:\n                logger.warning(\n                    \"Some externally hosted files were ignored as access to \"\n                    \"them may be unreliable (use --allow-external %s to \"\n                    \"allow).\",\n                    req.name,\n                )\n\n            if self.need_warn_unverified:\n                logger.warning(\n                    \"Some insecure and unverifiable files were ignored\"\n                    \" (use --allow-unverified %s to allow).\",\n                    req.name,\n                )\n\n            raise DistributionNotFound(\n                'No matching distribution found for %s' % req\n            )\n\n        if applicable_versions[0].location is INSTALLED_VERSION:\n            # We have an existing version, and its the best version\n            logger.debug(\n                'Installed version (%s) is most up-to-date (past versions: '\n                '%s)',\n                req.satisfied_by.version,\n                ', '.join(str(i.version) for i in applicable_versions[1:]) or\n                \"none\",\n            )\n            raise BestVersionAlreadyInstalled\n\n        if len(applicable_versions) \u003e 1:\n            logger.debug(\n                'Using version %s (newest of versions: %s)',\n                applicable_versions[0].version,\n                ', '.join(str(i.version) for i in applicable_versions)\n            )\n\n        selected_version = applicable_versions[0].location\n\n        if (selected_version.verifiable is not None and not\n                selected_version.verifiable):\n            logger.warning(\n                \"%s is potentially insecure and unverifiable.\", req.name,\n            )\n\n        return selected_version\n\n    def _find_url_name(self, index_url, url_name):\n        \"\"\"\n        Finds the true URL name of a package, when the given name isn't quite\n        correct.\n        This is usually used to implement case-insensitivity.\n        \"\"\"\n        if not index_url.url.endswith('/'):\n            # Vaguely part of the PyPI API... weird but true.\n            # FIXME: bad to modify this?\n            index_url.url += '/'\n        page = self._get_page(index_url)\n        if page is None:\n            logger.critical('Cannot fetch index base URL %s', index_url)\n            return\n        norm_name = normalize_name(url_name)\n        for link in page.links:\n            base = posixpath.basename(link.path.rstrip('/'))\n            if norm_name == normalize_name(base):\n                logger.debug(\n                    'Real name of requirement %s is %s', url_name, base,\n                )\n                return base\n        return None\n\n    def _get_pages(self, locations, project_name):\n        \"\"\"\n        Yields (page, page_url) from the given locations, skipping\n        locations that have errors, and adding download/homepage links\n        \"\"\"\n        all_locations = list(locations)\n        seen = set()\n        normalized = normalize_name(project_name)\n\n        while all_locations:\n            location = all_locations.pop(0)\n            if location in seen:\n                continue\n            seen.add(location)\n\n            page = self._get_page(location)\n            if page is None:\n                continue\n\n            yield page\n\n            for link in page.rel_links():\n\n                if (normalized not in self.allow_external and not\n                        self.allow_all_external):\n                    self.need_warn_external = True\n                    logger.debug(\n                        \"Not searching %s for files because external \"\n                        \"urls are disallowed.\",\n                        link,\n                    )\n                    continue\n\n                if (link.trusted is not None and not\n                        link.trusted and\n                        normalized not in self.allow_unverified):\n                    logger.debug(\n                        \"Not searching %s for urls, it is an \"\n                        \"untrusted link and cannot produce safe or \"\n                        \"verifiable files.\",\n                        link,\n                    )\n                    self.need_warn_unverified = True\n                    continue\n\n                all_locations.append(link)\n\n    _py_version_re = re.compile(r'-py([123]\\.?[0-9]?)$')\n\n    def _sort_links(self, links):\n        \"\"\"\n        Returns elements of links in order, non-egg links first, egg links\n        second, while eliminating duplicates\n        \"\"\"\n        eggs, no_eggs = [], []\n        seen = set()\n        for link in links:\n            if link not in seen:\n                seen.add(link)\n                if link.egg_fragment:\n                    eggs.append(link)\n                else:\n                    no_eggs.append(link)\n        return no_eggs + eggs\n\n    def _package_versions(self, links, search):\n        result = []\n        for link in self._sort_links(links):\n            v = self._link_package_versions(link, search)\n            if v is not None:\n                result.append(v)\n        return result\n\n    def _log_skipped_link(self, link, reason):\n        if link not in self.logged_links:\n            logger.debug('Skipping link %s; %s', link, reason)\n            self.logged_links.add(link)\n\n    def _link_package_versions(self, link, search):\n        \"\"\"Return an InstallationCandidate or None\"\"\"\n        platform = get_platform()\n\n        version = None\n        if link.egg_fragment:\n            egg_info = link.egg_fragment\n            ext = link.ext\n        else:\n            egg_info, ext = link.splitext()\n            if not ext:\n                self._log_skipped_link(link, 'not a file')\n                return\n            if ext not in SUPPORTED_EXTENSIONS:\n                self._log_skipped_link(\n                    link, 'unsupported archive format: %s' % ext)\n                return\n            if \"binary\" not in search.formats and ext == wheel_ext:\n                self._log_skipped_link(\n                    link, 'No binaries permitted for %s' % search.supplied)\n                return\n            if \"macosx10\" in link.path and ext == '.zip':\n                self._log_skipped_link(link, 'macosx10 one')\n                return\n            if ext == wheel_ext:\n                try:\n                    wheel = Wheel(link.filename)\n                except InvalidWheelFilename:\n                    self._log_skipped_link(link, 'invalid wheel filename')\n                    return\n                if (pkg_resources.safe_name(wheel.name).lower() !=\n                        search.canonical):\n                    self._log_skipped_link(\n                        link, 'wrong project name (not %s)' % search.supplied)\n                    return\n                if not wheel.supported():\n                    self._log_skipped_link(\n                        link, 'it is not compatible with this Python')\n                    return\n                # This is a dirty hack to prevent installing Binary Wheels from\n                # PyPI unless it is a Windows or Mac Binary Wheel. This is\n                # paired with a change to PyPI disabling uploads for the\n                # same. Once we have a mechanism for enabling support for\n                # binary wheels on linux that deals with the inherent problems\n                # of binary distribution this can be removed.\n                comes_from = getattr(link, \"comes_from\", None)\n                if (\n                        (\n                            not platform.startswith('win') and not\n                            platform.startswith('macosx') and not\n                            platform == 'cli'\n                        ) and\n                        comes_from is not None and\n                        urllib_parse.urlparse(\n                            comes_from.url\n                        ).netloc.endswith(PyPI.netloc)):\n                    if not wheel.supported(tags=supported_tags_noarch):\n                        self._log_skipped_link(\n                            link,\n                            \"it is a pypi-hosted binary \"\n                            \"Wheel on an unsupported platform\",\n                        )\n                        return\n                version = wheel.version\n\n        # This should be up by the search.ok_binary check, but see issue 2700.\n        if \"source\" not in search.formats and ext != wheel_ext:\n            self._log_skipped_link(\n                link, 'No sources permitted for %s' % search.supplied)\n            return\n\n        if not version:\n            version = egg_info_matches(egg_info, search.supplied, link)\n        if version is None:\n            self._log_skipped_link(\n                link, 'wrong project name (not %s)' % search.supplied)\n            return\n\n        if (link.internal is not None and not\n                link.internal and not\n                normalize_name(search.supplied).lower()\n                in self.allow_external and not\n                self.allow_all_external):\n            # We have a link that we are sure is external, so we should skip\n            #   it unless we are allowing externals\n            self._log_skipped_link(link, 'it is externally hosted')\n            self.need_warn_external = True\n            return\n\n        if (link.verifiable is not None and not\n                link.verifiable and not\n                (normalize_name(search.supplied).lower()\n                    in self.allow_unverified)):\n            # We have a link that we are sure we cannot verify its integrity,\n            #   so we should skip it unless we are allowing unsafe installs\n            #   for this requirement.\n            self._log_skipped_link(\n                link, 'it is an insecure and unverifiable file')\n            self.need_warn_unverified = True\n            return\n\n        match = self._py_version_re.search(version)\n        if match:\n            version = version[:match.start()]\n            py_version = match.group(1)\n            if py_version != sys.version[:3]:\n                self._log_skipped_link(\n                    link, 'Python version is incorrect')\n                return\n        logger.debug('Found link %s, version: %s', link, version)\n\n        return InstallationCandidate(search.supplied, version, link)\n\n    def _get_page(self, link):\n        return HTMLPage.get_page(link, session=self.session)\n\n\ndef egg_info_matches(\n        egg_info, search_name, link,\n        _egg_info_re=re.compile(r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)', re.I)):\n    \"\"\"Pull the version part out of a string.\n\n    :param egg_info: The string to parse. E.g. foo-2.1\n    :param search_name: The name of the package this belongs to. None to\n        infer the name. Note that this cannot unambiguously parse strings\n        like foo-2-2 which might be foo, 2-2 or foo-2, 2.\n    :param link: The link the string came from, for logging on failure.\n    \"\"\"\n    match = _egg_info_re.search(egg_info)\n    if not match:\n        logger.debug('Could not parse version from link: %s', link)\n        return None\n    if search_name is None:\n        full_match = match.group(0)\n        return full_match[full_match.index('-'):]\n    name = match.group(0).lower()\n    # To match the \"safe\" name that pkg_resources creates:\n    name = name.replace('_', '-')\n    # project name and version must be separated by a dash\n    look_for = search_name.lower() + \"-\"\n    if name.startswith(look_for):\n        return match.group(0)[len(look_for):]\n    else:\n        return None\n\n\nclass HTMLPage(object):\n    \"\"\"Represents one page, along with its URL\"\"\"\n\n    def __init__(self, content, url, headers=None, trusted=None):\n        # Determine if we have any encoding information in our headers\n        encoding = None\n        if headers and \"Content-Type\" in headers:\n            content_type, params = cgi.parse_header(headers[\"Content-Type\"])\n\n            if \"charset\" in params:\n                encoding = params['charset']\n\n        self.content = content\n        self.parsed = html5lib.parse(\n            self.content,\n            encoding=encoding,\n            namespaceHTMLElements=False,\n        )\n        self.url = url\n        self.headers = headers\n        self.trusted = trusted\n\n    def __str__(self):\n        return self.url\n\n    @classmethod\n    def get_page(cls, link, skip_archives=True, session=None):\n        if session is None:\n            raise TypeError(\n                \"get_page() missing 1 required keyword argument: 'session'\"\n            )\n\n        url = link.url\n        url = url.split('#', 1)[0]\n\n        # Check for VCS schemes that do not support lookup as web pages.\n        from pip.vcs import VcsSupport\n        for scheme in VcsSupport.schemes:\n            if url.lower().startswith(scheme) and url[len(scheme)] in '+:':\n                logger.debug('Cannot look at %s URL %s', scheme, link)\n                return None\n\n        try:\n            if skip_archives:\n                filename = link.filename\n                for bad_ext in ARCHIVE_EXTENSIONS:\n                    if filename.endswith(bad_ext):\n                        content_type = cls._get_content_type(\n                            url, session=session,\n                        )\n                        if content_type.lower().startswith('text/html'):\n                            break\n                        else:\n                            logger.debug(\n                                'Skipping page %s because of Content-Type: %s',\n                                link,\n                                content_type,\n                            )\n                            return\n\n            logger.debug('Getting page %s', url)\n\n            # Tack index.html onto file:// URLs that point to directories\n            (scheme, netloc, path, params, query, fragment) = \\\n                urllib_parse.urlparse(url)\n            if (scheme == 'file' and\n                    os.path.isdir(urllib_request.url2pathname(path))):\n                # add trailing slash if not present so urljoin doesn't trim\n                # final segment\n                if not url.endswith('/'):\n                    url += '/'\n                url = urllib_parse.urljoin(url, 'index.html')\n                logger.debug(' file: URL is directory, getting %s', url)\n\n            resp = session.get(\n                url,\n                headers={\n                    \"Accept\": \"text/html\",\n                    \"Cache-Control\": \"max-age=600\",\n                },\n            )\n            resp.raise_for_status()\n\n            # The check for archives above only works if the url ends with\n            #   something that looks like an archive. However that is not a\n            #   requirement of an url. Unless we issue a HEAD request on every\n            #   url we cannot know ahead of time for sure if something is HTML\n            #   or not. However we can check after we've downloaded it.\n            content_type = resp.headers.get('Content-Type', 'unknown')\n            if not content_type.lower().startswith(\"text/html\"):\n                logger.debug(\n                    'Skipping page %s because of Content-Type: %s',\n                    link,\n                    content_type,\n                )\n                return\n\n            inst = cls(\n                resp.content, resp.url, resp.headers,\n                trusted=link.trusted,\n            )\n        except requests.HTTPError as exc:\n            level = 2 if exc.response.status_code == 404 else 1\n            cls._handle_fail(link, exc, url, level=level)\n        except requests.ConnectionError as exc:\n            cls._handle_fail(link, \"connection error: %s\" % exc, url)\n        except requests.Timeout:\n            cls._handle_fail(link, \"timed out\", url)\n        except SSLError as exc:\n            reason = (\"There was a problem confirming the ssl certificate: \"\n                      \"%s\" % exc)\n            cls._handle_fail(link, reason, url, level=2, meth=logger.info)\n        else:\n            return inst\n\n    @staticmethod\n    def _handle_fail(link, reason, url, level=1, meth=None):\n        if meth is None:\n            meth = logger.debug\n\n        meth(\"Could not fetch URL %s: %s - skipping\", link, reason)\n\n    @staticmethod\n    def _get_content_type(url, session):\n        \"\"\"Get the Content-Type of the given url, using a HEAD request\"\"\"\n        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)\n        if scheme not in ('http', 'https'):\n            # FIXME: some warning or something?\n            # assertion error?\n            return ''\n\n        resp = session.head(url, allow_redirects=True)\n        resp.raise_for_status()\n\n        return resp.headers.get(\"Content-Type\", \"\")\n\n    @cached_property\n    def api_version(self):\n        metas = [\n            x for x in self.parsed.findall(\".//meta\")\n            if x.get(\"name\", \"\").lower() == \"api-version\"\n        ]\n        if metas:\n            try:\n                return int(metas[0].get(\"value\", None))\n            except (TypeError, ValueError):\n                pass\n\n        return None\n\n    @cached_property\n    def base_url(self):\n        bases = [\n            x for x in self.parsed.findall(\".//base\")\n            if x.get(\"href\") is not None\n        ]\n        if bases and bases[0].get(\"href\"):\n            return bases[0].get(\"href\")\n        else:\n            return self.url\n\n    @property\n    def links(self):\n        \"\"\"Yields all links in the page\"\"\"\n        for anchor in self.parsed.findall(\".//a\"):\n            if anchor.get(\"href\"):\n                href = anchor.get(\"href\")\n                url = self.clean_link(\n                    urllib_parse.urljoin(self.base_url, href)\n                )\n\n                # Determine if this link is internal. If that distinction\n                #   doesn't make sense in this context, then we don't make\n                #   any distinction.\n                internal = None\n                if self.api_version and self.api_version \u003e= 2:\n                    # Only api_versions \u003e= 2 have a distinction between\n                    #   external and internal links\n                    internal = bool(\n                        anchor.get(\"rel\") and\n                        \"internal\" in anchor.get(\"rel\").split()\n                    )\n\n                yield Link(url, self, internal=internal)\n\n    def rel_links(self, rels=('homepage', 'download')):\n        \"\"\"Yields all links with the given relations\"\"\"\n        rels = set(rels)\n\n        for anchor in self.parsed.findall(\".//a\"):\n            if anchor.get(\"rel\") and anchor.get(\"href\"):\n                found_rels = set(anchor.get(\"rel\").split())\n                # Determine the intersection between what rels were found and\n                #   what rels were being looked for\n                if found_rels \u0026 rels:\n                    href = anchor.get(\"href\")\n                    url = self.clean_link(\n                        urllib_parse.urljoin(self.base_url, href)\n                    )\n                    yield Link(url, self, trusted=False)\n\n    _clean_re = re.compile(r'[^a-z0-9$\u0026+,/:;=?@.#%_\\\\|-]', re.I)\n\n    def clean_link(self, url):\n        \"\"\"Makes sure a link is fully encoded.  That is, if a ' ' shows up in\n        the link, it will be rewritten to %20 (while not over-quoting\n        % or other characters).\"\"\"\n        return self._clean_re.sub(\n            lambda match: '%%%2x' % ord(match.group(0)), url)\n\n\nclass Link(object):\n\n    def __init__(self, url, comes_from=None, internal=None, trusted=None):\n\n        # url can be a UNC windows share\n        if url != Inf and url.startswith('\\\\\\\\'):\n            url = path_to_url(url)\n\n        self.url = url\n        self.comes_from = comes_from\n        self.internal = internal\n        self.trusted = trusted\n\n    def __str__(self):\n        if self.comes_from:\n            return '%s (from %s)' % (self.url, self.comes_from)\n        else:\n            return str(self.url)\n\n    def __repr__(self):\n        return '\u003cLink %s\u003e' % self\n\n    def __eq__(self, other):\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url == other.url\n\n    def __ne__(self, other):\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url != other.url\n\n    def __lt__(self, other):\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url \u003c other.url\n\n    def __le__(self, other):\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url \u003c= other.url\n\n    def __gt__(self, other):\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url \u003e other.url\n\n    def __ge__(self, other):\n        if not isinstance(other, Link):\n            return NotImplemented\n        return self.url \u003e= other.url\n\n    def __hash__(self):\n        return hash(self.url)\n\n    @property\n    def filename(self):\n        _, netloc, path, _, _ = urllib_parse.urlsplit(self.url)\n        name = posixpath.basename(path.rstrip('/')) or netloc\n        name = urllib_parse.unquote(name)\n        assert name, ('URL %r produced no filename' % self.url)\n        return name\n\n    @property\n    def scheme(self):\n        return urllib_parse.urlsplit(self.url)[0]\n\n    @property\n    def netloc(self):\n        return urllib_parse.urlsplit(self.url)[1]\n\n    @property\n    def path(self):\n        return urllib_parse.unquote(urllib_parse.urlsplit(self.url)[2])\n\n    def splitext(self):\n        return splitext(posixpath.basename(self.path.rstrip('/')))\n\n    @property\n    def ext(self):\n        return self.splitext()[1]\n\n    @property\n    def url_without_fragment(self):\n        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(self.url)\n        return urllib_parse.urlunsplit((scheme, netloc, path, query, None))\n\n    _egg_fragment_re = re.compile(r'#egg=([^\u0026]*)')\n\n    @property\n    def egg_fragment(self):\n        match = self._egg_fragment_re.search(self.url)\n        if not match:\n            return None\n        return match.group(1)\n\n    _hash_re = re.compile(\n        r'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)'\n    )\n\n    @property\n    def hash(self):\n        match = self._hash_re.search(self.url)\n        if match:\n            return match.group(2)\n        return None\n\n    @property\n    def hash_name(self):\n        match = self._hash_re.search(self.url)\n        if match:\n            return match.group(1)\n        return None\n\n    @property\n    def show_url(self):\n        return posixpath.basename(self.url.split('#', 1)[0].split('?', 1)[0])\n\n    @property\n    def verifiable(self):\n        \"\"\"\n        Returns True if this link can be verified after download, False if it\n        cannot, and None if we cannot determine.\n        \"\"\"\n        trusted = self.trusted or getattr(self.comes_from, \"trusted\", None)\n        if trusted is not None and trusted:\n            # This link came from a trusted source. It *may* be verifiable but\n            #   first we need to see if this page is operating under the new\n            #   API version.\n            try:\n                api_version = getattr(self.comes_from, \"api_version\", None)\n                api_version = int(api_version)\n            except (ValueError, TypeError):\n                api_version = None\n\n            if api_version is None or api_version \u003c= 1:\n                # This link is either trusted, or it came from a trusted,\n                #   however it is not operating under the API version 2 so\n                #   we can't make any claims about if it's safe or not\n                return\n\n            if self.hash:\n                # This link came from a trusted source and it has a hash, so we\n                #   can consider it safe.\n                return True\n            else:\n                # This link came from a trusted source, using the new API\n                #   version, and it does not have a hash. It is NOT verifiable\n                return False\n        elif trusted is not None:\n            # This link came from an untrusted source and we cannot trust it\n            return False\n\n    @property\n    def is_wheel(self):\n        return self.ext == wheel_ext\n\n    @property\n    def is_artifact(self):\n        \"\"\"\n        Determines if this points to an actual artifact (e.g. a tarball) or if\n        it points to an \"abstract\" thing like a path or a VCS location.\n        \"\"\"\n        from pip.vcs import vcs\n\n        if self.scheme in vcs.all_schemes:\n            return False\n\n        return True\n\n\n# An object to represent the \"link\" for the installed version of a requirement.\n# Using Inf as the url makes it sort higher.\nINSTALLED_VERSION = Link(Inf)\n\n\nFormatControl = namedtuple('FormatControl', 'no_binary only_binary')\n\"\"\"This object has two fields, no_binary and only_binary.\n\nIf a field is falsy, it isn't set. If it is {':all:'}, it should match all\npackages except those listed in the other field. Only one field can be set\nto {':all:'} at a time. The rest of the time exact package name matches\nare listed, with any given package only showing up in one field at a time.\n\"\"\"\n\n\ndef fmt_ctl_handle_mutual_exclude(value, target, other):\n    new = value.split(',')\n    while ':all:' in new:\n        other.clear()\n        target.clear()\n        target.add(':all:')\n        del new[:new.index(':all:') + 1]\n        if ':none:' not in new:\n            # Without a none, we want to discard everything as :all: covers it\n            return\n    for name in new:\n        if name == ':none:':\n            target.clear()\n            continue\n        name = pkg_resources.safe_name(name).lower()\n        other.discard(name)\n        target.add(name)\n\n\ndef fmt_ctl_formats(fmt_ctl, canonical_name):\n    result = set([\"binary\", \"source\"])\n    if canonical_name in fmt_ctl.only_binary:\n        result.discard('source')\n    elif canonical_name in fmt_ctl.no_binary:\n        result.discard('binary')\n    elif ':all:' in fmt_ctl.only_binary:\n        result.discard('source')\n    elif ':all:' in fmt_ctl.no_binary:\n        result.discard('binary')\n    return frozenset(result)\n\n\ndef fmt_ctl_no_binary(fmt_ctl):\n    fmt_ctl_handle_mutual_exclude(\n        ':all:', fmt_ctl.no_binary, fmt_ctl.only_binary)\n\n\ndef fmt_ctl_no_use_wheel(fmt_ctl):\n    fmt_ctl_no_binary(fmt_ctl)\n    warnings.warn(\n        '--no-use-wheel is deprecated and will be removed in the future. '\n        ' Please use --no-binary :all: instead.', DeprecationWarning,\n        stacklevel=2)\n\n\nSearch = namedtuple('Search', 'supplied canonical formats')\n\"\"\"Capture key aspects of a search.\n\n:attribute supplied: The user supplied package.\n:attribute canonical: The canonical package name.\n:attribute formats: The formats allowed for this package. Should be a set\n    with 'binary' or 'source' or both in it.\n\"\"\"\n"}
